[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This section explores the application of unsupervised learning techniques on financial and macroeconomic datasets. The objective is to reveal hidden patterns and relationships using methods such as PCA and K-Means clustering.\n\n\n\nUse PCA to reduce the dimensionality of high-dimensional datasets while preserving key information.\n\nApply K-Means clustering to identify distinct patterns or groupings within the data.\n\nInvestigate the relationship between financial market behaviors and macroeconomic indicators.\n\nDetect abnormal behaviors, such as extreme volatility, and analyze their technical and macroeconomic causes.\n\n\n\n\n\n\n\n\nCombined and cleaned datasets (S&P 500, Crude Oil, Gold, Nasdaq, and Dow Jones).\n\nHandled missing values using forward-fill and backward-fill techniques.\n\nRemoved outliers based on a 3-standard deviation threshold.\n\nStandardized data using StandardScaler.\n\n\n\n\n\nPrincipal Component Analysis (PCA) was used to reduce dimensionality to 2 components.\n\nExplained variance: PC1 (56.9%), PC2 (23.4%).\n\nKey Outputs: - Visualized PCA scatter plot to explore cluster tendencies.\n\n\nPCA is a dimensionality reduction technique used to transform high-dimensional data into a low-dimensional space while retaining as much information as possible about the variance of the data. It works as follows:\n\nNormalize the data to ensure that each feature has a mean of zero and a standard deviation of one. Calculate the covariance matrix to capture the relationship between features.\nPerform a feature decomposition of the covariance matrix to find the principal components (i.e., the directions with the highest variance).\nProject the data onto these principal components to achieve dimensionality reduction.\n\nThe end result is a set of uncorrelated principal components that are ordered by the magnitude of the captured variance, thus simplifying the structure of the dataset while preserving its main patterns and information.\n\n\n\n\n\nApplied K-Means Clustering:\n\nOptimal clusters determined using Elbow Method (k=3).\n\nEvaluated clusters using the Silhouette Score (0.346).\n\nAnalyzed feature distributions by cluster.\n\n\nInvestigated macroeconomic indicators’ relationships with market behavior using correlation analysis and Granger causality tests.\n\n\n\nK-Means is an iterative clustering algorithm designed to partition a dataset into k distinct groups. The process involves:\n1. Initialization: Randomly assigning k cluster centroids.\n2. Assignment Step: Assigning each data point to the closest centroid based on a distance metric (e.g., Euclidean distance).\n3. Update Step: Recomputing centroids as the mean position of all points in a cluster.\n4. Repeating the assignment and update steps until centroids stabilize (convergence).\nThe algorithm minimizes within-cluster variance while maximizing between-cluster variance. Key hyperparameters include the number of clusters (k) and the initialization method (e.g., k-means++ for better starting centroids)."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The goal of this section is to perform Exploratory Data Analysis (EDA) on a dataset of financial and economic indicators. It help us understand the distributional properties of the variables. This is an important preparation for subsequent modeling.\n\n\nThe datasets used in this analysis include: - Crude Oil Prices - Dow Jones Index - Gold Prices - Government Spending - Macro Series (GDP, CPI, unemployment, and other indicators)\nThese datasets were preprocessed into clean CSV files for further exploration. The key objectives are:\n\nAnalyzing the distributional characteristics of numerical variables;\nExamining correlations between variables;\nExploring the relationship between key features and target variables.\n\n\n\n\n\n\n\n\nHandled missing values using forward-fill and backward-fill methods.\nRemoved outliers exceeding a 3-standard deviation threshold.\nStandardized numerical features using StandardScaler to ensure uniform scaling.\n\n\n\n\n\nUnivariate Analysis:\n\nUsing histograms and density plots to analyze the distribution of individual variables (e.g., Open, Close, Volume, RSI, MACD).\n\nBivariate Analysis:\n\nUsing box plots and density plots to compare the distribution of variables among target categories\n\nCorrelation Analysis:\n\nIdentifying relationships between numerical features using correlation matrices and heat maps."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "This document outlines the data cleaning process for a financial market analysis project. The goal is to convert raw financial and economic data into a clean, structured format suitable for analysis. The process includes processing market data, macroeconomic indicators, personal income statistics and government spending information.\n\n\n\nThe cleaning process involves several key steps and methods:\n\nData Organization\n\nRaw data is stored in data/raw-data\nProcessed data is saved to data/processed-data\nEach dataset type has specific cleaning requirements\n\nKey Processing Steps\n\nMissing value imputation\nDate standardization\nTechnical indicator calculation\nFeature engineering\nData validation and quality checks\n\n\n\n\n\n\n\n\nRequired Variables: GDP, CPI, Unemployment, FedFundsRate, M2, Umscent\nProcessing Steps:\n\nFill missing values using annual means\nCalculate YoY and MoM growth rates\nGenerate lag variables for key indicators\nFilter data from year 2000 onwards\n\n\n\n\n\n\nRequired Calculations:\n\nMoving averages (50-day, 200-day)\nRSI (14-day period)\nMACD (12-day, 26-day EMAs)\nVolatility metrics\nVolume indicators\nPrice momentum indicators\n\n\n\n\n\n\nKey Metrics:\n\nPersonal income\nDisposable income\nSaving rate\nConsumption expenditures\nPersonal taxes\n\n\n\n\n\n\nComponents:\n\nFederal spending\nState and local spending\nTotal government expenditures\n\n\n\n\n\n\n\n\n\nMissing Values\n\nUse forward fill for price data\nApply annual means for economic indicators\nZero-fill for volume data\nDefault values for technical indicators (e.g., RSI = 50)\n\nDate Range\n\nStart date: 2000-11-01\nEnd date: 2024-11-30\nDaily frequency for market data\nMonthly/quarterly frequency for economic data\n\nTechnical Requirements\n\nUse pandas for data manipulation\nImplement proper error handling\nInclude data validation checks\nDocument all cleaning steps\n\n\n\n\n\n\nSetup\n\nDefine output folders\nImport required libraries\nSet up error logging\n\nFunctions\n\nImplement cleaning functions for each data type\nCreate technical indicator calculations\nBuild data validation checks\n\nOutput\n\nSave cleaned data in CSV format\nGenerate processing logs\nInclude data quality reports\n\n\n\n\n\n\n\n\n\nData Completeness\n\nVerify all required columns exist\nCheck date continuity\nValidate data ranges\n\nData Consistency\n\nCheck for outliers\nVerify calculations\nEnsure proper data types\n\nTechnical Indicator Validation\n\nConfirm indicator calculations\nVerify lag variables\nValidate moving averages\n\n\n\n\n\n\n\n\ndata/\n├── raw-data/\n│   ├── macro_series_raw_collection.csv\n│   ├── t20100_raw_data.csv\n│   ├── t10105_raw_data.csv\n│   └── [asset]_raw_a.csv\n└── processed-data/\n    ├── macro_series_cleaned.csv\n    ├── personal_info_cleaned.csv\n    ├── Government_Spending_Breakdown.csv\n    └── [asset]_cleaned.csv\n\n\n\n\nAll dates in datetime format\nNo missing values\nProper column naming\nConsistent data types\nTechnical indicators calculated\nTarget variables generated\n\n\n\n\n\nThe cleaning process may need to be revisited as:\n\nNew data becomes available\nAnalysis requirements change\nData quality issues are discovered\nNew features are needed"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "This section is to predict financial outcomes and market behavior. The primary focus is on predicting: 1. The future direction of the Dow Jones Index (binary classification). 2. The 5-day future return of the Dow Jones Index (regression).\n\n\n\n\n\n\nNormalization: Features were standardized using StandardScaler to ensure consistent scaling.\nFeature Selection: Selected key financial indicators (e.g., Daily_Return_sp500, Volatility_sp500, MA_50_sp500) and lagged features (e.g., Lag1_Return, Lag2_Return).\nHandling Missing Data: Missing values were imputed using column means or removed when necessary.\nTrain-Test Split: Data was split into 80% training and 20% testing subsets.\n\n\n\n\n\n\n\n\nAlgorithm: Logistic Regression\nObjective: Predict the direction of the Dow Jones Index (Target_dow_jones).\nEvaluation Metrics: Accuracy, Precision, Recall, F1 Score, and Confusion Matrix.\n\n\n\n\n\nAlgorithms: Linear Regression, Random Forest, XGBoost, and LightGBM.\nObjective: Predict the 5-day future return (Future_Return_5D_dow_jones).\nEvaluation Metrics: Mean Squared Error (MSE) and R² Score.\n\n\n\n\n\n\n\nCross-Validation: 5-fold cross-validation to ensure robust model evaluation.\nParameter Tuning: Hyperparameters for Random Forest, XGBoost, and LightGBM were optimized to improve performance."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming:",
    "text": "Brainstorming:\n\nProject Workflow Design:\nAssisted in clarifying and refining project objectives, aligning data collection, EDA, and machine learning processes.\n\nResearch Questions:\nHelped focus on key research questions, such as the relationship between macroeconomic indicators and asset returns."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nSummarizing Results:\nSimplified complex technical findings into clear explanations for non-technical readers.\n\nImproving Flow and Clarity:\nSuggested improvements to sentence structure and paragraph flow to ensure the report was logical and easy to follow.\n\nConsistent Terminology:\nEnsured that technical terms (e.g., “PCA”, “K-means”, “correlation”) were used consistently across the report to avoid confusion for readers.\nFormatting Enhancements:\nRecommended improvements in formatting, such as bullet points, subheadings, and visual aids, to enhance the overall presentation and readability.\nProofreading and Editing:\nReviewed grammar, clarity, and consistency to ensure a polished and professional report.\nMinor edits were made to ensure the tone was appropriate for a non-technical audience."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode Documentation:\nAdded detailed comments to explain the logic behind EDA and unsupervised learning code. This made the code more accessible to non-technical readers and easier to follow.\nModel Selection Assistance:\nProvided guidance on selecting appropriate models for different problems, such as recommending Logistic regression for predicting the Future Direction of the Dow Jones Index.\nDebugging:\n\nAssisted in resolving minor errors during data collection and data cleaning, such as issues with missing values, inconsistent formats, or runtime errors.\n\nProvided suggestions for debugging in the EDA phase, including fixing visualizations and ensuring clean outputs.\n\n\nCode Readability and Organization:\nImproved the structure and readability of scripts by recommending modular code (e.g., breaking long processes into smaller functions).\n\nInterpretation of Unsupervised Code:\nClarified the purpose and logic of K-Means clustering and PCA code, making the results easier to interpret for a general audience. Propose methods to merge dataset to facilitate unsupervised- learning later on."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "The primary goal of this project is to develop a framework for collecting, integrating, and preprocessing economic and financial data from multiple authoritative sources. By creating a systematic approach to data aggregation, we aim to enable advanced economic research, predictive analysis, and data-driven decision-making.\n\n\n\nThe complexity of economic analysis is fundamentally constrained by consistency, and integration. Existing approaches to economic data collection often suffer from:\n\nFragmented data sources\nInconsistent reporting frequencies\nLimited cross-source compatibility\n\n\n\n\nOur specific objectives are focused on creating a framework for economic data collection, emphasizing comprehensive aggregation, data quality, research potential, and methodological transparency:\n\n\n\nCollect diverse economic indicators from multiple authoritative sources\nIntegrate data from key economic and financial platforms:\n\nFederal Reserve Economic Data (FRED)\nBureau of Economic Analysis (BEA)\nYahoo Finance\n\nGather a wide range of economic and market performance metrics, including:\n\nMacroeconomic indicators (GDP, CPI, Unemployment)\nMarket indices (S&P 500, Nasdaq, Dow Jones)\nCommodity prices (Gold, Crude Oil)\nTreasury yield data\n\n\n\n\n\n\nImplement data processing techniques to ensure data reliability\nAddress challenges of data integration across different sources\nDevelop methods to:\n\nHandle missing or invalid data points\nConvert and standardize data types\nMerge datasets from multiple sources\n\nEnsure clean, consistent data preparation for further analysis\n\n\n\n\n\nPrepare a structured, accessible dataset for economic research\nSave raw collected data in easily manageable CSV format\nCreate a foundation for potential future analyses, including:\n\nEconomic trend identification\nMarket performance studies\nMacroeconomic indicator exploration\n\nProvide a flexible dataset that supports various research approaches\n\n\n\n\n\nDocument the entire data collection process comprehensively\nProvide clear, reproducible code for data retrieval\nExplain the rationale behind data source selection\nEnsure other researchers can understand and potentially replicate the data collection methodology\n\n\n\n\n\nBridge the gap between diverse sources and meaningful economic analysis by transforming raw economic data into valuable research resources. We lay the foundation for subsequent analysis by collecting data, managing data quality, research potential, and monitoring economic dynamics.\n\n\n\n\nComprehensive CSV datasets covering multiple economic indicators\nWell-documented data collection methodology\nClean, standardized economic and market performance data"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "The data collecting part focuses on collecting comprehensive financial and economic data to analyze market performance and economic indicators. Our goal is to collect data from three main sources: - Federal Reserve Economic Data (FRED) for macroeconomic indicators - Bureau of Economic Analysis (BEA) for national accounts data - Yahoo Finance for market performance data\nThese sources will help us understand the relationship between economic factors and market behavior.\n\n\n\nWe use three primary methods for data collection:\n\nFRED API Collection:\n\nFetches macroeconomic indicators (GDP, CPI, Unemployment, etc.)\nRetrieves Treasury yield data\nCollects monetary policy indicators\n\nBEA API Integration:\n\nGathers National Income and Product Accounts (NIPA) data\nCollects personal income and consumption metrics\nObtains government spending information\n\nYahoo Finance Data Retrieval:\n\nDownloads market index data (S&P 500, Nasdaq, Dow Jones)\nCollects commodity prices (Gold, Crude Oil)\nProcesses daily market metrics\n\n\n\n\n\n\n\nPrimary focus on tabular data (CSV): - Time series financial data - Economic indicators - Market performance metrics\n\n\n\n\nRegression Targets:\n\nAsset price returns\nEconomic growth rates\nInflation metrics\n\nBinary Classification Targets:\n\nMarket direction (up/down)\nEconomic state (expansion/contraction)\nYield curve status (normal/inverted)\n\nMulticlass Classification Targets:\n\nMarket conditions (bull/bear/sideways)\nEconomic phases (growth/stagnation/recession)\nTrading volume levels (high/medium/low)\n\n\n\n\n\n\n\n\n\nFRED API:\n\nAPI Key required\nMultiple series collection\nJSON response processing\n\nBEA API:\n\nUser authentication\nTable-specific requests\nAnnual data retrieval\n\nYahoo Finance:\n\nNo authentication required\nDaily data download\nMultiple asset handling\n\n\n\n\n\n\nSave raw data to data/raw-data/:\ndata/raw-data/\n├── macro_series_raw_collection.csv\n├── t20100_raw_data.csv\n├── t10105_raw_data.csv\n└── [asset]_raw_a.csv\nMaintain data integrity\nPreserve original format\n\n\n\n\n\n\nCode Documentation:\n\nInclude all API request code\nDocument parameter choices\nExplain data processing steps\n\nData Source Information:\n\nFRED API: https://fred.stlouisfed.org/docs/api/fred/\nBEA API: https://apps.bea.gov/api/\nYahoo Finance API documentation\n\nData Collection Methods:\n\nAPI interaction processes\nData download procedures\nError handling approaches\n\nData Structure Documentation:\n\nColumn descriptions\nData types\nTime periods\nUpdate frequencies\n\n\n\n\n\n\nSetup:\n\nConfigure API keys\nCreate storage directories\nImport required libraries\n\nData Collection:\n\nFetch macroeconomic data\nDownload market data\nRetrieve national accounts data\n\nData Verification:\n\nCheck data completeness\nVerify date ranges\nValidate data types\n\nDocumentation:\n\nRecord collection process\nDocument data structure\nNote any issues or limitations\n\n\n\n\n\nEnsure:\n- All API calls are properly authenticated\n- Data is saved in its original form\n- Collection process is reproducible\n- Documentation is complete and clear"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "This document outlines the data cleaning process for a financial market analysis project. The goal is to convert raw financial and economic data into a clean, structured format suitable for analysis. The process includes processing market data, macroeconomic indicators, personal income statistics and government spending information.\n\n\n\nThe cleaning process involves several key steps and methods:\n\nData Organization\n\nRaw data is stored in data/raw-data\nProcessed data is saved to data/processed-data\nEach dataset type has specific cleaning requirements\n\nKey Processing Steps\n\nMissing value imputation\nDate standardization\nTechnical indicator calculation\nFeature engineering\nData validation and quality checks\n\n\n\n\n\n\n\n\nRequired Variables: GDP, CPI, Unemployment, FedFundsRate, M2, Umscent\nProcessing Steps:\n\nFill missing values using annual means\nCalculate YoY and MoM growth rates\nGenerate lag variables for key indicators\nFilter data from year 2000 onwards\n\n\n\n\n\n\nRequired Calculations:\n\nMoving averages (50-day, 200-day)\nRSI (14-day period)\nMACD (12-day, 26-day EMAs)\nVolatility metrics\nVolume indicators\nPrice momentum indicators\n\n\n\n\n\n\nKey Metrics:\n\nPersonal income\nDisposable income\nSaving rate\nConsumption expenditures\nPersonal taxes\n\n\n\n\n\n\nComponents:\n\nFederal spending\nState and local spending\nTotal government expenditures\n\n\n\n\n\n\n\n\n\nMissing Values\n\nUse forward fill for price data\nApply annual means for economic indicators\nZero-fill for volume data\nDefault values for technical indicators (e.g., RSI = 50)\n\nDate Range\n\nStart date: 2000-11-01\nEnd date: 2024-11-30\nDaily frequency for market data\nMonthly/quarterly frequency for economic data\n\nTechnical Requirements\n\nUse pandas for data manipulation\nImplement proper error handling\nInclude data validation checks\nDocument all cleaning steps\n\n\n\n\n\n\nSetup\n\nDefine output folders\nImport required libraries\nSet up error logging\n\nFunctions\n\nImplement cleaning functions for each data type\nCreate technical indicator calculations\nBuild data validation checks\n\nOutput\n\nSave cleaned data in CSV format\nGenerate processing logs\nInclude data quality reports\n\n\n\n\n\n\n\n\n\nData Completeness\n\nVerify all required columns exist\nCheck date continuity\nValidate data ranges\n\nData Consistency\n\nCheck for outliers\nVerify calculations\nEnsure proper data types\n\nTechnical Indicator Validation\n\nConfirm indicator calculations\nVerify lag variables\nValidate moving averages\n\n\n\n\n\n\n\n\ndata/\n├── raw-data/\n│   ├── macro_series_raw_collection.csv\n│   ├── t20100_raw_data.csv\n│   ├── t10105_raw_data.csv\n│   └── [asset]_raw_a.csv\n└── processed-data/\n    ├── macro_series_cleaned.csv\n    ├── personal_info_cleaned.csv\n    ├── Government_Spending_Breakdown.csv\n    └── [asset]_cleaned.csv\n\n\n\n\nAll dates in datetime format\nNo missing values\nProper column naming\nConsistent data types\nTechnical indicators calculated\nTarget variables generated\n\n\n\n\n\nThe cleaning process may need to be revisited as:\n\nNew data becomes available\nAnalysis requirements change\nData quality issues are discovered\nNew features are needed"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "Hello! My name is Shengmian Wang. I am a first-year student in the Data Science and Analytics(DSAN) masters program at Georgetown University. I completed my undergraduate studies in Taiwan with a major of Information Management.\n\n\n\nWith two years of full-time experience as a data analyst and school learning, I have developed a robust skill set in transforming complex data into actionable intelligence. My professional background has equipped me with:\nData Analysis & Visualization\n\nLeveraged Python (Pandas, NumPy) and R to analyze large-scale datasets, including geospatial data and user reviews, ensuring high data quality and accuracy\nCreated interactive dashboards using Power BI and SPSS that significantly improved decision-making efficiency and delivered actionable insights to diverse clients\nConducted comprehensive exploratory data analysis (EDA) using SQL and BigQuery, driving substantial increases in user engagement and identifying key business patterns\nGenerated extensive analytical reports using SPSS and Excel, translating complex data into clear insights for stakeholders\n\nMachine Learning & Statistical Analysis\n\nImplemented various machine learning models (Decision Tree, K-means, KNN) achieving high accuracy in predicting consumer behavior patterns\nApplied advanced statistical methods including Chi-Square tests, Logistic Regression, and Bayes’ Theorem to analyze adoption patterns and consumer behavior\nDeveloped NLP pipelines utilizing BioBERT embeddings and cosine similarity for sentiment analysis and classification\nOptimized model performance through feature engineering and parameter tuning, significantly improving data accuracy\n\nData Collection & Processing\n\nAutomated data collection processes using Selenium, successfully gathering extensive data entries and user reviews from multiple sources\nDeveloped a VBA-based format check tool that substantially increased data validation efficiency\nCreated efficient data processing pipelines to handle various data formats (CSV, JSON, API responses)\nOptimized SQL queries for Google Analytics transitions, enhancing implementation efficiency\n\n\n\n\n\nLinkedIn - Professional Profile\nGitHub - Project Repository\nPortfolio - Project Showcase\nEmail: wsm19990820@gmail.com\nPhone: (571) 276-6276\n\n\n\n\n\n\n\nHello! My name is Hailing Liao. I am also a first-year student in the Data Science and Analytics(DSAN) masters program at Georgetown University. I completed my undergraduate studies in Zhongnan University of Economics and Law, majored in Information management and Information system.\n\n\n\nDuring my undergraduate study, I found myself a strong interest in data analytics through relevant coursework called “Commercial Data Analysis”. I developed a strong interest in business analytics and data visualization, which led me to work on several related projects. This experience deepened my interest in data science. So I chose my graduate study at Georgetown University. In the DSAN program, I hope to build solid programming skills and apply them to my future work. Although I feel like my current programming abilities need a lot of work, I’m eager to learn and make the most of this program. After graduation, my goal is to join a leading firm as a data analyst.\n\n\n\n\nI haven’t worked full time, and after my undergrad graduation gap for a year to prepare for various exams and materials for grad school. I previously had a two-month stint as a business analytics intern at Johnson & Johnson. While there, I - using data analysis tools, including Tableau and Python - analyzed sales data from over 10 retail stores. I then conducted detailed interviews and analyzed customer traffic to develop tailored promotional strategies for contact lens brands, contributing to increased sales conversions.\n\n\n\nLinkedIn - Professional Profile\nGitHub - Project Repository\nPortfolio - Project Showcase\nEmail: haileyyy.liao@gmail.com\nPhone: (202) 975-9385"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Landing page",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Landing page",
    "section": "What to Include on This Page",
    "text": "What to Include on This Page\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nCreate a Landing Page for Your Project\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Landing page",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Final Report\n\n\nMarket pattern analysis and portfolio optimization through machine learning methods\nIn today’s complex financial markets, understanding the interactions between different market assets, macroeconomic indicators and technical factors is important for effective investment decisions. To address this problem, our project leverages data science techniques to analyze market behavior patterns, identify relationships between different asset classes, and develop portfolio optimization strategies.\nAs global markets become increasingly more volatile and interconnected, investors have greater challenges and opportunities. Traditional market analysis methods often struggle to capture the complex dynamics between various market assets and their responses to economic changes. In addition, market conditions change rapidly, requiring more sophisticated methods to detect abnormal market behavior and predict potential market trends.\nThis project addresses these issues through a comprehensive analytical framework that combines machine learning, statistical modeling and technical analysis. Through the analysis of many aspects of financial markets, we aim to provide theoretical insights into market behavior and provide practical tools for market analysis and portfolio management. In addition, we also hope that the results of this research can be useful for investors, analysts and financial institutions to effectively analyze the market environment and optimize investment strategies.\n\n\nObjective\nThis research aims to enhance market analysis and investment decision-making through the following objectives:\n1. Market Pattern Analysis and Prediction\n\nExamine the predictability of short-term market returns\nEvaluate the effectiveness of various technical indicators\nDevelop machine learning models to capture market patterns\n\n2. Macroeconomic Impact Assessment\n\nInvestigate the complex relationship between macroeconomic indicators and market performance\nAnalyze how different asset classes respond to economic changes\nIdentify key economic drivers for market movements\n\n3. Technical Analysis Framework Development\n\nCombine technical indicators with future earnings analysis\nStudy correlations between technical indicators and market performance\nDevelop reliable market analysis and forecasting frameworks\n\n4. Investment Decision Support\n\nCreate practical tools for market analysis\nDevelop portfolio management strategies\nProvide actionable insights for investment decisions\n\n\n\nData Source\nOur data collection approach uses three main methods to ensure we assemble a comprehensive data set that captures macroeconomic fundamentals and market performance indicators. Through the FRED API, we obtain comprehensive macroeconomic data, including key indicators such as GDP, CPI, unemployment rate, and government bond yields. Next, integration with the BEA API allows us to collect detailed national income as well as personal income, consumption indicators and government spending information. Additionally, we used Yahoo Finance to retrieve market data, which includes major market indexes such as the S&P 500, Nasdaq, and Dow Jones, as well as commodity prices such as gold and crude oil and their daily market indicators.\n\n\nKey Findings\n\nMarket Classification and Characteristics\n\n\nStock market indices (Nasdaq, Dow Jones, S&P 500) demonstrate strong correlations (0.88-0.97), suggesting similar movement patterns\nGold exhibits unique safe-haven characteristics, showing low correlation with other assets\nTechnical indicators vary significantly across different clusters, helping us identify market conditions\n\n\nMacroeconomic Influences\n\n\nGDP significantly impacts stock markets across all time lags (p-values &lt; 0.05), showing the economy’s strong influence on market performance\nConsumer Price Index (CPI) has the most pronounced effect on markets (p-values &lt; 0.0001), particularly on S&P 500 and Dow Jones\nWhile unemployment rates strongly affect stock markets, they have less impact on commodities like oil and gold\nThe Federal Funds Rate shows consistent and significant influence on the Dow Jones Industrial Average\n\n\nVolatility and Market Anomalies\n\n\nStock markets tend to move together during extreme market events, sharing similar volatility patterns\nThe oil market experiences the most extreme price swings, particularly during specific historical events\nGold shows relatively stable behavior, confirming its role as a safe-haven asset\nTechnical indicators prove effective in identifying unusual market behavior\n\n\nPortfolio Optimization Insights\n\n\nRisk-return analysis suggests a substantial oil allocation (approximately 55%) in the portfolio\nNasdaq represents about 35% of the optimal portfolio, capturing growth opportunities\nA modest gold allocation (around 10%) serves as a portfolio hedge\nMinimal S&P 500 allocation due to its high correlation with other stock indices\n\n\nStock Market Short-term Return Predictability\n\n\nXGBoost and LightGBM models are able to capture the general trend of the Dow Jones index’s 5-day future returns to a certain extent\nThe models’ predicted values maintain relatively good consistency with the actual observed values, especially in the middle range of the returns\nThis suggests that short-term stock market returns can be predicted using machine learning models, but the accuracy is not perfect, particularly at the extreme values of the returns\n\n\n\nConclusion\nThe analysis shows that the machine learning models XGBoost and LightGBM show potential for predicting short-term stock market returns, such as the 5-day future return of the Dow Jones Index. These models are able to capture the overall trend in returns, and the predicted values ​​are in good agreement with actual observations, more significantly in the middle range of returns. However, forecast accuracy under extreme returns is not perfect, which illustrates that short-term stock market forecasting still faces challenges and limitations.\nSecondly, regarding the impact of macroeconomic factors, research results show that GDP, consumer price index (CPI), unemployment rate and federal funds interest rate all have an impact on the Dow Jones Index, S&P 500 Index, Nasdaq Index, gold and other markets. Make an impact. Have a greater impact on performance. The study found that GDP has a significant impact on the stock market index at all time intervals, representing a strong correlation between the overall economy and market performance. We found that the S&P 500 Index and the Dow Jones Index are more sensitive to inflationary pressure, and CPI has the most obvious impact on these stock market indexes. While the unemployment rate also had a strong impact on the stock market, it had a smaller impact on commodities such as oil and gold. In addition, the federal funds rate has a large and persistent impact on the Dow Jones Industrial Average, showing the importance of monetary policy to market dynamics.\nThe analysis also explores the ability to identify abnormal market behavior through technical indicators and volatility analysis. The findings indicate that during extreme market events, stock markets exhibit similar volatility patterns and tend to move together. We find that the most volatile price movements occur in the oil market, possibly due to specific historical events such as geopolitical conflicts and supply disruptions: major events such as the Gulf War, the Iraq War or tensions in the Middle East that could disrupt the global economy. Gold has shown relatively stable behavior, confirming its role as a safe-haven asset, and while further analysis is needed to establish specific correlations, our ability to identify these market anomalies through technical indicators suggests. potential relationships between factors.\nRegarding the characteristic patterns of different market assets, we mainly find that the stock market indexes (NASDAQ, Dow Jones, S&P 500 Index) show strong correlations. It can be seen that their trend patterns are similar, while gold shows a unique avoidance Risk characteristics and correlations with other assets are low. These findings can guide the allocation of different asset classes to achieve an ideal risk-return profile, which is of great significance to portfolio optimization.\nOverall, the key findings presented in this report provide insights into the dynamics and predictability of stock market performance, the impact of macroeconomic factors, identification of market anomalies, and implications for portfolio optimization. While short-term stock market returns can be predicted to some extent, there is room for improvement in the accuracy and robustness of these forecasts during periods of heightened market volatility.\n\n\nReference\nhttps://academic.oup.com/rfs/article-abstract/15/3/751/1603456?redirectedFrom=fulltext\n\n\nData Source\nMacroeconomic data: https://www.bea.gov/tools\nNational income: https://fred.stlouisfed.org/\nFinance market data: https://finance.yahoo.com/"
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "Report Content",
    "text": "Report Content\nThese are just examples, you can use any structure that is suitable for your project.\n\nCase-1: Academic-Oriented Projects\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\nCase-2: Business-Oriented Projects\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "Final Tips",
    "text": "Final Tips\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "Navigating the complex world of economic data APIs comes with many challenges. The financial data landscape is characterized by fragmentation, with each source (FRED, BEA, and Yahoo Finance) that offers unique data formats and reporting mechanisms. So we need a sophisticated approach to data standardization and integration.\nOne of the most important difficulties is the inherent time inconsistency of managing economic indicators. Macroeconomic data often arrive at different frequencies—some monthly, others quarterly or annually—creating data alignment challenges. Our solution is to utilize interpolation and aggregation strategies to transform these disparate data streams into coherent, consistent time series.\n\n\n\n\n\n\nIrregular Yield Curve:\n\nUnexpected fluctuations in 10-year Treasury yield data detected\nPotential impact on the interpretation of economic indicators\nRecommend further investigation of anomalous data points\n\n\n\n\n\n\nPerformance Bottleneck:\n\nIdentify potential optimization opportunities in the data retrieval and processing pipeline\nEmphasis on the need for a more efficient API request strategy\n\n\n\n\n\n\n\n\n\nSuccessfully developed a powerful multi-source financial data collection framework\nCreate a flexible modular data processing pipeline\nAbility to effectively process complex, multi-frequency economic data sets\n\n\n\n\n\nShort-Term Improvements:\n\nImplement more complex error handling\nEnhanced data verification mechanism\nOptimize API request strategy\n\nLong-Term Research Directions:\n\nExplore machine learning integration for predictive economic analysis\nDevelop a more comprehensive data standardization framework\nEstablish a common economic data collection and processing toolkit\n\n\n\n\n\nThis step is an important step in creating a more flexible approach to financial and economic data collection. By solving technical challenges and using solutions, we lay the foundation for subsequent economic analysis techniques."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Purpose: Retrieve comprehensive macroeconomic indicators\nSeries Collected:\n\nGross Domestic Product (GDP)\nConsumer Price Index (CPI)\nUnemployment Rate\nFederal Funds Rate\nMoney Supply (M2)\nConsumer Sentiment Index\nReal Estate Price Index\nExports and Imports Data\n\nTechnique:\n\nUtilized requests library for API communication\nConverted raw JSON responses into pandas DataFrames\nStandardized date formats and numeric conversions\nMerged multiple economic series into a single comprehensive dataset\n\n\n\n\n\n\nPurpose: Fetch National Income and Product Accounts (NIPA) data\nTables Retrieved:\n\nT20100: Gross Domestic Product and Personal Consumption\nT10105: National Account Savings and Investment Data\n\nApproach:\n\nCreated a generic fetch_bea_data() function\nConfigured API parameters for annual frequency data\nHandled potential API response variations\nConverted successful responses to CSV for further analysis\n\n\n\n\n\n\nPurpose: Download market performance data for major indices and commodities\nAssets Retrieved:\n\nS&P 500 Index\nNasdaq Composite\nDow Jones Industrial Average\nGold Futures\nCrude Oil Futures\n\nMethod:\n\nImplemented a flexible fetch_data() function\nSet consistent date ranges for historical data\nDownloaded daily price and volume information\nStandardized data columns and saved as CSV files\n\n\n\n\n\n\n\n\n\nUsed os module for robust file path handling\nDynamically created output directories\nImplemented consistent file naming conventions\nEnsured data is saved in a structured, accessible format\n\n\n\n\n\nImplemented error handling and logging\nConverted API responses to consistent pandas DataFrame structures\nPerformed data type conversions (datetime, numeric)\nManaged potential missing or invalid data points\n\n\n\n\n\nReset DataFrame indexes\nRenamed columns for clarity\nPerformed basic data cleaning\nPrepared data for subsequent analysis stages\n\n\n\n\n\n\nCreated reusable functions for data fetching\nSeparated concerns between data collection, processing, and storage\nUsed configuration dictionaries for flexible asset and series selection\nImplemented error-tolerant code with try-except blocks\n\n\n\n\n\nUtilized environment-specific API keys\nConfigured date ranges and data retrieval parameters\nImplemented interval and frequency specifications for precise data collection\n\n\n\n\n\nSaved raw collected data as CSV files\nMaintained a consistent output folder structure\nEnabled easy data inspection and further processing\n\n\n\n\n\nProgramming Language: Python 3.12\nKey Libraries:\n\nrequests for API communication\npandas for data manipulation\nyfinance for financial data retrieval\nos for file system operations\n\nData Sources:\n\nFRED API\nBEA API\nYahoo Finance"
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Introduction",
    "section": "",
    "text": "The goal of this section is to perform Exploratory Data Analysis (EDA) on a dataset of financial and economic indicators. It help us understand the distributional properties of the variables. This is an important preparation for subsequent modeling.\n\n\nThe datasets used in this analysis include: - Crude Oil Prices - Dow Jones Index - Gold Prices - Government Spending - Macro Series (GDP, CPI, unemployment, and other indicators)\nThese datasets were preprocessed into clean CSV files for further exploration. The key objectives are:\n\nAnalyzing the distributional characteristics of numerical variables;\nExamining correlations between variables;\nExploring the relationship between key features and target variables.\n\n\n\n\n\n\n\n\nHandled missing values using forward-fill and backward-fill methods.\nRemoved outliers exceeding a 3-standard deviation threshold.\nStandardized numerical features using StandardScaler to ensure uniform scaling.\n\n\n\n\n\nUnivariate Analysis:\n\nUsing histograms and density plots to analyze the distribution of individual variables (e.g., Open, Close, Volume, RSI, MACD).\n\nBivariate Analysis:\n\nUsing box plots and density plots to compare the distribution of variables among target categories\n\nCorrelation Analysis:\n\nIdentifying relationships between numerical features using correlation matrices and heat maps."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Project Progress Log",
    "section": "",
    "text": "Complete model optimization\nCreate final visualization dashboard\nWrite comprehensive documentation\nImplement automated testing\nAdd cross-validation framework\nCreate user guide\n\n\n\n\n\n\nPortfolio: [Link to portfolio]\nProject Role: Data Infrastructure & Unsupervised Learning\n\n\n\nResearch and identify data sources\nImplement data collection pipeline\nDevelop data cleaning workflow\nPerform unsupervised learning analysis\nCreate visualization components\nMaintain code repository\n\n\n\n\nW: 12/11-12/13\n\nImplemented unsupervised learning components:\n\nCreated PCA dimensionality reduction\nDeveloped K-means clustering\nAnalyzed market behavior patterns\nGenerated clustering visualizations\n\n\nW: 12/04-12/08\n\nDeveloped data cleaning pipeline:\n\nCreated cleaning functions\nImplemented technical indicators\nAdded data validation\nHandled missing values\n\n\nW: 11/27-12/01\n\nSet up data collection infrastructure:\n\nImplemented FRED API integration\nCreated BEA API connection\nSet up Yahoo Finance pipeline\n\n\nW: 11/20-11/24\n\nInitial project research:\n\nIdentified required data sources\nResearched available APIs\nPlanned data collection strategy\n\n\n\n\n\n\nPortfolio: [Link to portfolio]\nProject Role: Analysis & Supervised Learning\n\n\n\nPerform exploratory data analysis\nDevelop supervised learning models\nCreate performance metrics\nGenerate visualization components\nWrite technical documentation\n\n\n\n\nW: 12/11-12/13\n\nImplemented supervised learning models:\n\nCreated logistic regression\nDeveloped Random Forest model\nAdded XGBoost and LightGBM implementations\nGenerated model performance metrics\n\n\nW: 12/04-12/08\n\nPerformed comprehensive EDA:\n\nCreated distribution analysis\nGenerated correlation matrices\nAnalyzed feature relationships\nProduced statistical summaries\nDeveloped visualization components\n\n\nW: 11/27-12/01\n\nInitial analysis planning:\n\nResearched analysis methods\nPlanned EDA approach\nDesigned model framework"
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with GPT\nwrite a technical methods sections for K-means\nwrite a technical methods sections for PCA\n\n… etc"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nT: 10-15-2024\n\nCoordinate with team member to set up weekly meeting time\n\nM: 10-14-2024\n\nDo a first draft of the project landing page"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2",
    "text": "Member-2\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 10-16-2024\n\nAttend first group meeting"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Introduction",
    "section": "",
    "text": "This section explores the application of unsupervised learning techniques on financial and macroeconomic datasets. The objective is to reveal hidden patterns and relationships using methods such as PCA and K-Means clustering.\n\n\n\nUse PCA to reduce the dimensionality of high-dimensional datasets while preserving key information.\n\nApply K-Means clustering to identify distinct patterns or groupings within the data.\n\nInvestigate the relationship between financial market behaviors and macroeconomic indicators.\n\nDetect abnormal behaviors, such as extreme volatility, and analyze their technical and macroeconomic causes.\n\n\n\n\n\n\n\n\nCombined and cleaned datasets (S&P 500, Crude Oil, Gold, Nasdaq, and Dow Jones).\n\nHandled missing values using forward-fill and backward-fill techniques.\n\nRemoved outliers based on a 3-standard deviation threshold.\n\nStandardized data using StandardScaler.\n\n\n\n\n\nPrincipal Component Analysis (PCA) was used to reduce dimensionality to 2 components.\n\nExplained variance: PC1 (56.9%), PC2 (23.4%).\n\nKey Outputs: - Visualized PCA scatter plot to explore cluster tendencies.\n\n\nPCA is a dimensionality reduction technique used to transform high-dimensional data into a low-dimensional space while retaining as much information as possible about the variance of the data. It works as follows:\n\nNormalize the data to ensure that each feature has a mean of zero and a standard deviation of one. Calculate the covariance matrix to capture the relationship between features.\nPerform a feature decomposition of the covariance matrix to find the principal components (i.e., the directions with the highest variance).\nProject the data onto these principal components to achieve dimensionality reduction.\n\nThe end result is a set of uncorrelated principal components that are ordered by the magnitude of the captured variance, thus simplifying the structure of the dataset while preserving its main patterns and information.\n\n\n\n\n\nApplied K-Means Clustering:\n\nOptimal clusters determined using Elbow Method (k=3).\n\nEvaluated clusters using the Silhouette Score (0.346).\n\nAnalyzed feature distributions by cluster.\n\n\nInvestigated macroeconomic indicators’ relationships with market behavior using correlation analysis and Granger causality tests.\n\n\n\nK-Means is an iterative clustering algorithm designed to partition a dataset into k distinct groups. The process involves:\n1. Initialization: Randomly assigning k cluster centroids.\n2. Assignment Step: Assigning each data point to the closest centroid based on a distance metric (e.g., Euclidean distance).\n3. Update Step: Recomputing centroids as the mean position of all points in a cluster.\n4. Repeating the assignment and update steps until centroids stabilize (convergence).\nThe algorithm minimizes within-cluster variance while maximizing between-cluster variance. Key hyperparameters include the number of clusters (k) and the initialization method (e.g., k-means++ for better starting centroids)."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "The data collecting part focuses on collecting comprehensive financial and economic data to analyze market performance and economic indicators. Our goal is to collect data from three main sources: - Federal Reserve Economic Data (FRED) for macroeconomic indicators - Bureau of Economic Analysis (BEA) for national accounts data - Yahoo Finance for market performance data\nThese sources will help us understand the relationship between economic factors and market behavior.\n\n\n\nWe use three primary methods for data collection:\n\nFRED API Collection:\n\nFetches macroeconomic indicators (GDP, CPI, Unemployment, etc.)\nRetrieves Treasury yield data\nCollects monetary policy indicators\n\nBEA API Integration:\n\nGathers National Income and Product Accounts (NIPA) data\nCollects personal income and consumption metrics\nObtains government spending information\n\nYahoo Finance Data Retrieval:\n\nDownloads market index data (S&P 500, Nasdaq, Dow Jones)\nCollects commodity prices (Gold, Crude Oil)\nProcesses daily market metrics\n\n\n\n\n\n\n\nPrimary focus on tabular data (CSV): - Time series financial data - Economic indicators - Market performance metrics\n\n\n\n\nRegression Targets:\n\nAsset price returns\nEconomic growth rates\nInflation metrics\n\nBinary Classification Targets:\n\nMarket direction (up/down)\nEconomic state (expansion/contraction)\nYield curve status (normal/inverted)\n\nMulticlass Classification Targets:\n\nMarket conditions (bull/bear/sideways)\nEconomic phases (growth/stagnation/recession)\nTrading volume levels (high/medium/low)\n\n\n\n\n\n\n\n\n\nFRED API:\n\nAPI Key required\nMultiple series collection\nJSON response processing\n\nBEA API:\n\nUser authentication\nTable-specific requests\nAnnual data retrieval\n\nYahoo Finance:\n\nNo authentication required\nDaily data download\nMultiple asset handling\n\n\n\n\n\n\nSave raw data to data/raw-data/:\ndata/raw-data/\n├── macro_series_raw_collection.csv\n├── t20100_raw_data.csv\n├── t10105_raw_data.csv\n└── [asset]_raw_a.csv\nMaintain data integrity\nPreserve original format\n\n\n\n\n\n\nCode Documentation:\n\nInclude all API request code\nDocument parameter choices\nExplain data processing steps\n\nData Source Information:\n\nFRED API: https://fred.stlouisfed.org/docs/api/fred/\nBEA API: https://apps.bea.gov/api/\nYahoo Finance API documentation\n\nData Collection Methods:\n\nAPI interaction processes\nData download procedures\nError handling approaches\n\nData Structure Documentation:\n\nColumn descriptions\nData types\nTime periods\nUpdate frequencies\n\n\n\n\n\n\nSetup:\n\nConfigure API keys\nCreate storage directories\nImport required libraries\n\nData Collection:\n\nFetch macroeconomic data\nDownload market data\nRetrieve national accounts data\n\nData Verification:\n\nCheck data completeness\nVerify date ranges\nValidate data types\n\nDocumentation:\n\nRecord collection process\nDocument data structure\nNote any issues or limitations\n\n\n\n\n\nEnsure:\n- All API calls are properly authenticated\n- Data is saved in its original form\n- Collection process is reproducible\n- Documentation is complete and clear"
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Step 1: Send a request to Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\nresponse = requests.get(url)\n\n# Step 2: Parse the page content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Step 3: Find the table containing the data (usually the first table for such lists)\ntable = soup.find('table', {'class': 'wikitable'})\n\n# Step 4: Extract data from the table rows\ncountries = []\npopulations = []\n\n# Iterate over the table rows\nfor row in table.find_all('tr')[1:]:  # Skip the header row\n    cells = row.find_all('td')\n    if len(cells) &gt; 1:\n        country = cells[1].text.strip()  # The country name is in the second column\n        population = cells[2].text.strip()  # The population is in the third column\n        countries.append(country)\n        populations.append(population)\n\n# Step 5: Create a DataFrame to store the results\ndata = pd.DataFrame({\n    'Country': countries,\n    'Population': populations\n})\n\n# Display the scraped data\nprint(data)\n\n# Optionally save to CSV\ndata.to_csv('../../data/raw-data/countries_population.csv', index=False)\n\n                                 Country     Population\n0                                  World  8,119,000,000\n1                                  China  1,409,670,000\n2                          1,404,910,000          17.3%\n3                          United States    335,893,238\n4                              Indonesia    281,603,800\n..                                   ...            ...\n235                   Niue (New Zealand)          1,681\n236                Tokelau (New Zealand)          1,647\n237                         Vatican City            764\n238  Cocos (Keeling) Islands (Australia)            593\n239                Pitcairn Islands (UK)             35\n\n[240 rows x 2 columns]"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "This section is to predict financial outcomes and market behavior. The primary focus is on predicting: 1. The future direction of the Dow Jones Index (binary classification). 2. The 5-day future return of the Dow Jones Index (regression).\n\n\n\n\n\n\nNormalization: Features were standardized using StandardScaler to ensure consistent scaling.\nFeature Selection: Selected key financial indicators (e.g., Daily_Return_sp500, Volatility_sp500, MA_50_sp500) and lagged features (e.g., Lag1_Return, Lag2_Return).\nHandling Missing Data: Missing values were imputed using column means or removed when necessary.\nTrain-Test Split: Data was split into 80% training and 20% testing subsets.\n\n\n\n\n\n\n\n\nAlgorithm: Logistic Regression\nObjective: Predict the direction of the Dow Jones Index (Target_dow_jones).\nEvaluation Metrics: Accuracy, Precision, Recall, F1 Score, and Confusion Matrix.\n\n\n\n\n\nAlgorithms: Linear Regression, Random Forest, XGBoost, and LightGBM.\nObjective: Predict the 5-day future return (Future_Return_5D_dow_jones).\nEvaluation Metrics: Mean Squared Error (MSE) and R² Score.\n\n\n\n\n\n\n\nCross-Validation: 5-fold cross-validation to ensure robust model evaluation.\nParameter Tuning: Hyperparameters for Random Forest, XGBoost, and LightGBM were optimized to improve performance."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#import-data",
    "href": "technical-details/data-collection/main.html#import-data",
    "title": "Data Collection",
    "section": "Import data",
    "text": "Import data\nIn the following code, we utilize the FRED API (Federal Reserve Economic Data) for macroeconomic data and BEA API (Bureau of Economic Analysis) for assets data. This combination allows us to gather comprehensive economic indicators and market performance data for our analysis."
  },
  {
    "objectID": "technical-details/data-collection/main.html#set-the-path-to-store-data",
    "href": "technical-details/data-collection/main.html#set-the-path-to-store-data",
    "title": "Data Collection",
    "section": "Set the path to store data",
    "text": "Set the path to store data\n\nimport os  # Import os module for handling file paths\n\n# Define the target folder for saving CSV files\noutput_folder = \"../../data/raw-data\"\n\n# Create the folder if it doesn't exist\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#importing-and-processing-macroeconomic-data-from-fred-api",
    "href": "technical-details/data-collection/main.html#importing-and-processing-macroeconomic-data-from-fred-api",
    "title": "Data Collection",
    "section": "Importing and Processing Macroeconomic Data from FRED API",
    "text": "Importing and Processing Macroeconomic Data from FRED API\n\nimport requests\nimport pandas as pd\n\n# API Key\nAPI_KEY = \"01e2e0f764ac9522003f01e4458beabe\"\n\n# FRED API URL\nbase_url = \"https://api.stlouisfed.org/fred/series/observations\"\n\n# Define the series IDs to fetch\nseries_ids = {\n    \"GDP\": \"GDP\",\n    \"CPI\": \"CPIAUCSL\",\n    \"Unemployment\": \"UNRATE\",\n    \"FedFundsRate\": \"FEDFUNDS\",\n    \"M2\": \"M2SL\",\n    \"Umscent\": \"UMCSENT\",\n    \"real_estate\": \"CSUSHPINSA\",\n    \"Exports\": \"EXPGS\",\n    \"Imports\": \"IMPGS\"\n}\n\n# Create an empty list to store dataframes\ndata_frames = []\n\nfor name, series_id in series_ids.items():\n    # Construct the API request parameters\n    params = {\n        \"series_id\": series_id,\n        \"api_key\": API_KEY,\n        \"file_type\": \"json\"\n    }\n    # Send the API request\n    response = requests.get(base_url, params=params)\n    json_data = response.json()\n\n    # Extract the observations data\n    observations = json_data[\"observations\"]\n    df = pd.DataFrame(observations)\n    \n    # Process the dataframe\n    df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")  # Convert invalid values to NaN\n    df[\"date\"] = pd.to_datetime(df[\"date\"])  # Ensure the date column is in datetime format\n    df = df.rename(columns={\"value\": name})  # Rename the value column to the series name\n    df = df[[\"date\", name]]  # Keep only the date and series value columns\n    data_frames.append(df)\n\n# Merge all the dataframes into a single dataframe\nmerged_data = data_frames[0]\nfor df in data_frames[1:]:\n    merged_data = pd.merge(merged_data, df, on=\"date\", how=\"outer\")\n\n# Save the raw collected data for inspection or further processing\nraw_data_file = os.path.join(output_folder, \"macro_series_raw_collection.csv\")\nmerged_data.to_csv(raw_data_file, index=False)\nprint(f\"Raw collected data saved to '{raw_data_file}'.\")\n\nData for macro_series saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/macro_series_raw.csv'.\n          date        GDP    CPI  Unemployment  FedFundsRate      M2  Umscent  \\\n640 2000-01-01  10002.179  169.3           4.0          5.45  4667.6    112.0   \n641 2000-02-01  10250.952  170.0           4.1          5.73  4680.9    111.3   \n642 2000-03-01  10250.952  171.0           4.0          5.85  4711.7    107.1   \n643 2000-04-01  10247.720  170.9           3.8          6.02  4767.8    109.2   \n644 2000-05-01  10250.952  171.2           4.0          6.27  4755.7    110.7   \n645 2000-06-01  10250.952  172.2           4.0          6.53  4773.6    106.4   \n646 2000-07-01  10318.165  172.7           4.0          6.54  4791.3    108.3   \n647 2000-08-01  10250.952  172.7           4.1          6.50  4819.5    107.3   \n648 2000-09-01  10250.952  173.6           3.9          6.52  4855.3    106.8   \n649 2000-10-01  10435.744  173.9           3.9          6.51  4871.4    105.8   \n650 2000-11-01  10250.952  174.2           3.9          6.51  4882.8    107.6   \n651 2000-12-01  10250.952  174.6           3.9          6.40  4927.7     98.4   \n652 2001-01-01  10470.231  175.6           4.2          5.98  4978.4     94.7   \n653 2001-02-01  10581.929  176.0           4.2          5.49  5017.1     90.6   \n654 2001-03-01  10581.929  176.1           4.3          5.31  5074.9     91.5   \n655 2001-04-01  10599.000  176.4           4.4          4.80  5139.2     88.4   \n656 2001-05-01  10581.929  177.3           4.3          4.21  5137.3     92.0   \n657 2001-06-01  10581.929  177.7           4.5          3.97  5180.3     92.6   \n658 2001-07-01  10598.020  177.4           4.6          3.77  5210.2     92.4   \n659 2001-08-01  10581.929  177.4           4.9          3.65  5243.9     91.5   \n\n     real_estate     Exports   Imports  year   GDP_YoY   GDP_MoM   CPI_YoY  \\\n640      100.000  1052.90400  1409.487  2000  0.062741  0.038522  0.027930   \n641      100.571  1096.11075  1477.184  2000  0.064351  0.024872  0.032180   \n642      101.466  1096.11075  1477.184  2000  0.064351  0.000000  0.037621   \n643      102.541  1093.36000  1455.860  2000  0.075739 -0.000315  0.030139   \n644      103.702  1096.11075  1477.184  2000  0.064351  0.000315  0.031325   \n645      104.856  1096.11075  1477.184  2000  0.064351  0.000000  0.037349   \n646      105.722  1125.00200  1518.869  2000  0.065197  0.006557  0.035993   \n647      106.522  1096.11075  1477.184  2000  0.064351 -0.006514  0.033513   \n648      107.136  1096.11075  1477.184  2000  0.064351  0.000000  0.034565   \n649      107.729  1113.17700  1524.520  2000  0.054098  0.018027  0.034503   \n650      108.292  1096.11075  1477.184  2000  0.064351 -0.017708  0.034442   \n651      108.792  1096.11075  1477.184  2000  0.064351  0.000000  0.034360   \n652      109.215  1096.81200  1499.464  2001  0.046795  0.021391  0.037212   \n653      109.643  1026.81175  1403.559  2001  0.032287  0.010668  0.035294   \n654      110.395  1026.81175  1403.559  2001  0.032287  0.000000  0.029825   \n655      111.248  1058.01300  1422.028  2001  0.034279  0.001613  0.032183   \n656      112.203  1026.81175  1403.559  2001  0.032287 -0.001611  0.035631   \n657      113.273  1026.81175  1403.559  2001  0.032287  0.000000  0.031940   \n658      114.227   998.90200  1369.536  2001  0.027123  0.001521  0.027215   \n659      114.989  1026.81175  1403.559  2001  0.032287 -0.001518  0.027215   \n\n        GDP_Lag1  CPI_Lag1  \n640   9631.17175     168.8  \n641  10002.17900     169.3  \n642  10250.95200     170.0  \n643  10250.95200     171.0  \n644  10247.72000     170.9  \n645  10250.95200     171.2  \n646  10250.95200     172.2  \n647  10318.16500     172.7  \n648  10250.95200     172.7  \n649  10250.95200     173.6  \n650  10435.74400     173.9  \n651  10250.95200     174.2  \n652  10250.95200     174.6  \n653  10470.23100     175.6  \n654  10581.92900     176.0  \n655  10581.92900     176.1  \n656  10599.00000     176.4  \n657  10581.92900     177.3  \n658  10581.92900     177.7  \n659  10598.02000     177.4  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 299 entries, 640 to 938\nData columns (total 16 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   date          299 non-null    datetime64[ns]\n 1   GDP           299 non-null    float64       \n 2   CPI           299 non-null    float64       \n 3   Unemployment  299 non-null    float64       \n 4   FedFundsRate  299 non-null    float64       \n 5   M2            298 non-null    float64       \n 6   Umscent       299 non-null    float64       \n 7   real_estate   297 non-null    float64       \n 8   Exports       299 non-null    float64       \n 9   Imports       299 non-null    float64       \n 10  year          299 non-null    int32         \n 11  GDP_YoY       299 non-null    float64       \n 12  GDP_MoM       299 non-null    float64       \n 13  CPI_YoY       299 non-null    float64       \n 14  GDP_Lag1      299 non-null    float64       \n 15  CPI_Lag1      299 non-null    float64       \ndtypes: datetime64[ns](1), float64(14), int32(1)\nmemory usage: 38.5 KB\nNone"
  },
  {
    "objectID": "technical-details/data-collection/main.html#fetching-10-year-treasury-yield-data-from-fred-api",
    "href": "technical-details/data-collection/main.html#fetching-10-year-treasury-yield-data-from-fred-api",
    "title": "Data Collection",
    "section": "Fetching 10-Year Treasury Yield Data from FRED API",
    "text": "Fetching 10-Year Treasury Yield Data from FRED API\n\n# Define request parameters\nparams = {\n    \"series_id\": \"DGS10\",   # 10-Year Treasury Yield\n    \"api_key\": API_KEY,\n    \"file_type\": \"json\",\n    \"observation_start\": \"2000-01-01\",\n    \"observation_end\": \"2024-12-31\",\n    \"frequency\": \"d\",  # Monthly frequency\n    \"aggregation_method\": \"eop\",  # Use period end value\n    \"units\": \"pch\",  # Percentage change\n}\n\n# Send the request\nresponse = requests.get(base_url, params=params)\n\n# Check if the request is successful\nif response.status_code == 200:\n    print(\"Data request successful!\")\n    data = response.json()\n    \n    # Extract data and convert to DataFrame\n    records = []\n    for item in data['observations']:\n        records.append({\n            \"date\": item['date'],\n            \"10_year_yield\": float(item['value']) if item['value'] != \".\" else None\n        })\n    df = pd.DataFrame(records)\n    \n    # Process data\n    df['date'] = pd.to_datetime(df['date'])\n    print(df.head())\nelse:\n    print(\"Data request failed!\", response.status_code, response.text)\n\n# Save the result to a CSV file\nfile_name = os.path.join(output_folder, f\"10_year_treasury.csv\")\ndf.to_csv(file_name, index=False)\nprint(f\"Data for 10_year_treasury saved to '{file_name}'.\")\n\nData request successful!\n        date  10_year_yield\n0 2000-01-03        2.01550\n1 2000-01-04       -1.36778\n2 2000-01-05        2.00308\n3 2000-01-06       -0.75529\n4 2000-01-07       -0.76104\nData for 10_year_treasury saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/10_year_treasury.csv'."
  },
  {
    "objectID": "technical-details/data-collection/main.html#fetching-national-income-and-product-accounts-nipa-data-from-bea-api",
    "href": "technical-details/data-collection/main.html#fetching-national-income-and-product-accounts-nipa-data-from-bea-api",
    "title": "Data Collection",
    "section": "Fetching National Income and Product Accounts (NIPA) Data from BEA API",
    "text": "Fetching National Income and Product Accounts (NIPA) Data from BEA API\n\nThese data tables include:\nT20100: Gross Domestic Product and Personal Consumption\nT10105: National Account Savings and Investment Data\n\nimport requests\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\n# BEA API basic configuration\nBEA_API_KEY = \"0539E64B-28C5-43F2-8885-D19E3D784EEE\"  # Replace with your API key\nbase_url = \"https://apps.bea.gov/api/data/\"\n\ndef fetch_bea_data(table_name):\n    params = {\n        \"UserID\": BEA_API_KEY,\n        \"method\": \"GetData\",\n        \"datasetname\": \"NIPA\",\n        \"Frequency\": \"A\",  # Annual data\n        \"TableName\": table_name, \n        \"Year\": \"X\",  # Retrieve all years\n        \"ResultFormat\": \"JSON\"\n    }\n    response = requests.get(base_url, params=params)\n    data = response.json()\n    if \"Results\" in data[\"BEAAPI\"] and \"Data\" in data[\"BEAAPI\"][\"Results\"]:\n        return pd.DataFrame(data[\"BEAAPI\"][\"Results\"][\"Data\"])\n    else:\n        print(f\"Failed to fetch data for table {table_name}, error message:\", data)\n        return pd.DataFrame()  # Return an empty DataFrame if no data is retrieved\n\n\n\n# Fetch T20100 data\nt20100_data = fetch_bea_data(\"T20100\")\nt10105_data = fetch_bea_data(\"T10105\")\n\nfile_name = os.path.join(output_folder, f\"t20100_raw_data.csv\")\nt20100_data.to_csv(file_name, index=False)\nprint(f\"Data for t20100_raw_data saved to '{file_name}'.\")\n\nfile_name = os.path.join(output_folder, f\"t10105_raw_data.csv\")\nt10105_data.to_csv(file_name, index=False)\nprint(f\"Data for t10105_raw_data saved to '{file_name}'.\")\n\nData for t20100_raw_data saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/t20100_raw_data.csv'.\nData for t10105_raw_data saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/t10105_raw_data.csv'."
  },
  {
    "objectID": "technical-details/data-collection/main.html#downloading-market-data-from-yahoo-finance",
    "href": "technical-details/data-collection/main.html#downloading-market-data-from-yahoo-finance",
    "title": "Data Collection",
    "section": "Downloading Market Data from Yahoo Finance",
    "text": "Downloading Market Data from Yahoo Finance\n\nThis code fetches daily market data for major indices and commodities:\nS&P 500 (^GSPC)\nNasdaq (^IXIC)\nDow Jones (^DJI)\nGold Futures (GC=F)\nCrude Oil Futures (CL=F)\n\nimport yfinance as yf\nimport pandas as pd\nimport os\n\ndef fetch_data(ticker, start_date, end_date):\n    \"\"\"\n    Fetch asset data from Yahoo Finance.\n    \n    Args:\n        ticker (str): Asset ticker symbol (e.g., \"^GSPC\").\n        start_date (str): Start date (format \"YYYY-MM-DD\").\n        end_date (str): End date (format \"YYYY-MM-DD\").\n\n    Returns:\n        pd.DataFrame: DataFrame containing basic asset data (Date, Open, High, Low, Close, Volume).\n    \"\"\"\n    print(f\"Downloading data for {ticker}...\")\n    try:\n        # Download data from Yahoo Finance\n        data = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n        if data.empty:\n            print(f\"No data found for {ticker}.\")\n            return None\n\n        # Retain relevant columns and reset the index\n        data.reset_index(inplace=True)\n        data = data[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n        return data\n    except Exception as e:\n        print(f\"Error fetching data for {ticker}: {e}\")\n        return None\n\n\nif __name__ == \"__main__\":\n    # Define the list of assets\n    assets = {\n        \"^GSPC\": \"S&P_500\",\n        \"^IXIC\": \"Nasdaq\",\n        \"^DJI\": \"Dow_Jones\",\n        \"GC=F\": \"Gold\",\n        \"CL=F\": \"Crude_Oil\",\n    }\n\n    # Define the date range\n    start_date = \"2000-11-01\"\n    end_date = \"2024-11-30\"\n\n\n    # Iterate through the assets and download data\n    for ticker, name in assets.items():\n        data = fetch_data(ticker, start_date, end_date)\n        # Ensure column names are single-level (remove multi-level index)\n        if isinstance(data.columns, pd.MultiIndex):\n            data.columns = data.columns.droplevel(1)\n        if data is not None:\n            # Save the raw data to a CSV file\n            \n            file_path = os.path.join(output_folder, f\"{name}_raw_a.csv\")\n            data.to_csv(file_path, index=False)\n            print(f\"Raw data for {name} saved to '{file_path}'.\")\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\nDownloading data for ^GSPC...\nRaw data for S&P_500 saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/S&P_500_raw_a.csv'.\nDownloading data for ^IXIC...\n\n\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\nRaw data for Nasdaq saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/Nasdaq_raw_a.csv'.\nDownloading data for ^DJI...\nRaw data for Dow_Jones saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/Dow_Jones_raw_a.csv'.\nDownloading data for GC=F...\nRaw data for Gold saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/Gold_raw_a.csv'.\nDownloading data for CL=F...\n\n\n[*********************100%***********************]  1 of 1 completed\n\n\nRaw data for Crude_Oil saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/raw-data/Crude_Oil_raw_a.csv'."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#set-the-path-to-store-data",
    "href": "technical-details/data-cleaning/main.html#set-the-path-to-store-data",
    "title": "Data Cleaning",
    "section": "Set the path to store data",
    "text": "Set the path to store data\n\nimport os  # Import os module for handling file paths\n\n# Define the target folder for saving CSV files\noutput_folder_cleaned = \"../../data/processed-data\"\nraw_data_folder = \"../../data/raw-data\"\n\n# Create the folder if it doesn't exist\nif not os.path.exists(output_folder_cleaned):\n    os.makedirs(output_folder_cleaned)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#processing-and-cleaning-macroeconomic-data",
    "href": "technical-details/data-cleaning/main.html#processing-and-cleaning-macroeconomic-data",
    "title": "Data Cleaning",
    "section": "Processing and Cleaning Macroeconomic Data",
    "text": "Processing and Cleaning Macroeconomic Data\n\nimport pandas as pd\nimport numpy as np\nimport os\n\n# Load the raw data file\nraw_data_file = os.path.join(raw_data_folder, \"macro_series_raw_collection.csv\")\n\ntry:\n    # Read the CSV file\n    merged_data = pd.read_csv(raw_data_file)\n    merged_data[\"date\"] = pd.to_datetime(merged_data[\"date\"])  # Ensure the date column is in datetime format\nexcept FileNotFoundError:\n    print(f\"Error: File '{raw_data_file}' not found.\")\n    exit()\nexcept Exception as e:\n    print(f\"Error loading file: {e}\")\n    exit()\n\n# Extract the year as a new column\nmerged_data[\"year\"] = merged_data[\"date\"].dt.year\n\n# Check if the required columns exist\nrequired_columns = [\"GDP\", \"Umscent\", \"Exports\", \"Imports\", \"CPI\"]\nmissing_columns = [col for col in required_columns if col not in merged_data.columns]\nif missing_columns:\n    print(f\"Error: Missing required columns: {missing_columns}\")\n    exit()\n\n# ======= Define Function =======\n\ndef fill_missing_with_annual_mean(data, column):\n    \"\"\"\n    Fill missing values: Calculate the annual mean for each year and use it to fill missing values.\n    \"\"\"\n    annual_mean = data.groupby(\"year\")[column].transform(\"mean\")\n    data[column] = data[column].fillna(annual_mean)\n\n# ======= Fill Missing Values =======\n\nfill_missing_with_annual_mean(merged_data, \"GDP\")\nfill_missing_with_annual_mean(merged_data, \"Umscent\")\nfill_missing_with_annual_mean(merged_data, \"Exports\")\nfill_missing_with_annual_mean(merged_data, \"Imports\")\n\n# ======= Calculate Growth Rates =======\n\n# Calculate Year-over-Year (YoY) and Month-over-Month (MoM) growth rates for GDP\nmerged_data[\"GDP_YoY\"] = merged_data[\"GDP\"].pct_change(periods=12)  # YoY GDP growth\nmerged_data[\"GDP_MoM\"] = merged_data[\"GDP\"].pct_change(periods=1)   # MoM GDP growth\n\n# Calculate the YoY growth rate for CPI\nmerged_data[\"CPI_YoY\"] = merged_data[\"CPI\"].pct_change(periods=12)\n\n# ======= Add Lag Variables =======\n\nmerged_data[\"GDP_Lag1\"] = merged_data[\"GDP\"].shift(1)  # Previous month's GDP\nmerged_data[\"CPI_Lag1\"] = merged_data[\"CPI\"].shift(1)  # Previous month's CPI\n\n# ======= Filter Data =======\n# Filter the data: Keep only data from the year 2000 and later\nfiltered_data = merged_data[merged_data[\"year\"] &gt;= 2000]\n\n# ======= Save Cleaned Data =======\noutput_file = os.path.join(output_folder_cleaned, \"macro_series_cleaned.csv\")\nfiltered_data.to_csv(output_file, index=False)\n\n# ======= Print Data Summary =======\n\nprint(f\"Cleaned data saved to '{output_file}'.\")\nprint(filtered_data.head(20))\nprint(filtered_data.info())\n\nCleaned data saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/macro_series_cleaned.csv'.\n          date        GDP    CPI  Unemployment  FedFundsRate      M2  Umscent  \\\n640 2000-01-01  10002.179  169.3           4.0          5.45  4667.6    112.0   \n641 2000-02-01  10250.952  170.0           4.1          5.73  4680.9    111.3   \n642 2000-03-01  10250.952  171.0           4.0          5.85  4711.7    107.1   \n643 2000-04-01  10247.720  170.9           3.8          6.02  4767.8    109.2   \n644 2000-05-01  10250.952  171.2           4.0          6.27  4755.7    110.7   \n645 2000-06-01  10250.952  172.2           4.0          6.53  4773.6    106.4   \n646 2000-07-01  10318.165  172.7           4.0          6.54  4791.3    108.3   \n647 2000-08-01  10250.952  172.7           4.1          6.50  4819.5    107.3   \n648 2000-09-01  10250.952  173.6           3.9          6.52  4855.3    106.8   \n649 2000-10-01  10435.744  173.9           3.9          6.51  4871.4    105.8   \n650 2000-11-01  10250.952  174.2           3.9          6.51  4882.8    107.6   \n651 2000-12-01  10250.952  174.6           3.9          6.40  4927.7     98.4   \n652 2001-01-01  10470.231  175.6           4.2          5.98  4978.4     94.7   \n653 2001-02-01  10581.929  176.0           4.2          5.49  5017.1     90.6   \n654 2001-03-01  10581.929  176.1           4.3          5.31  5074.9     91.5   \n655 2001-04-01  10599.000  176.4           4.4          4.80  5139.2     88.4   \n656 2001-05-01  10581.929  177.3           4.3          4.21  5137.3     92.0   \n657 2001-06-01  10581.929  177.7           4.5          3.97  5180.3     92.6   \n658 2001-07-01  10598.020  177.4           4.6          3.77  5210.2     92.4   \n659 2001-08-01  10581.929  177.4           4.9          3.65  5243.9     91.5   \n\n     real_estate     Exports   Imports  year   GDP_YoY   GDP_MoM   CPI_YoY  \\\n640      100.000  1052.90400  1409.487  2000  0.062741  0.038522  0.027930   \n641      100.571  1096.11075  1477.184  2000  0.064351  0.024872  0.032180   \n642      101.466  1096.11075  1477.184  2000  0.064351  0.000000  0.037621   \n643      102.541  1093.36000  1455.860  2000  0.075739 -0.000315  0.030139   \n644      103.702  1096.11075  1477.184  2000  0.064351  0.000315  0.031325   \n645      104.856  1096.11075  1477.184  2000  0.064351  0.000000  0.037349   \n646      105.722  1125.00200  1518.869  2000  0.065197  0.006557  0.035993   \n647      106.522  1096.11075  1477.184  2000  0.064351 -0.006514  0.033513   \n648      107.136  1096.11075  1477.184  2000  0.064351  0.000000  0.034565   \n649      107.729  1113.17700  1524.520  2000  0.054098  0.018027  0.034503   \n650      108.292  1096.11075  1477.184  2000  0.064351 -0.017708  0.034442   \n651      108.792  1096.11075  1477.184  2000  0.064351  0.000000  0.034360   \n652      109.215  1096.81200  1499.464  2001  0.046795  0.021391  0.037212   \n653      109.643  1026.81175  1403.559  2001  0.032287  0.010668  0.035294   \n654      110.395  1026.81175  1403.559  2001  0.032287  0.000000  0.029825   \n655      111.248  1058.01300  1422.028  2001  0.034279  0.001613  0.032183   \n656      112.203  1026.81175  1403.559  2001  0.032287 -0.001611  0.035631   \n657      113.273  1026.81175  1403.559  2001  0.032287  0.000000  0.031940   \n658      114.227   998.90200  1369.536  2001  0.027123  0.001521  0.027215   \n659      114.989  1026.81175  1403.559  2001  0.032287 -0.001518  0.027215   \n\n        GDP_Lag1  CPI_Lag1  \n640   9631.17175     168.8  \n641  10002.17900     169.3  \n642  10250.95200     170.0  \n643  10250.95200     171.0  \n644  10247.72000     170.9  \n645  10250.95200     171.2  \n646  10250.95200     172.2  \n647  10318.16500     172.7  \n648  10250.95200     172.7  \n649  10250.95200     173.6  \n650  10435.74400     173.9  \n651  10250.95200     174.2  \n652  10250.95200     174.6  \n653  10470.23100     175.6  \n654  10581.92900     176.0  \n655  10581.92900     176.1  \n656  10599.00000     176.4  \n657  10581.92900     177.3  \n658  10581.92900     177.7  \n659  10598.02000     177.4  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 299 entries, 640 to 938\nData columns (total 16 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   date          299 non-null    datetime64[ns]\n 1   GDP           299 non-null    float64       \n 2   CPI           299 non-null    float64       \n 3   Unemployment  299 non-null    float64       \n 4   FedFundsRate  299 non-null    float64       \n 5   M2            298 non-null    float64       \n 6   Umscent       299 non-null    float64       \n 7   real_estate   297 non-null    float64       \n 8   Exports       299 non-null    float64       \n 9   Imports       299 non-null    float64       \n 10  year          299 non-null    int32         \n 11  GDP_YoY       299 non-null    float64       \n 12  GDP_MoM       299 non-null    float64       \n 13  CPI_YoY       299 non-null    float64       \n 14  GDP_Lag1      299 non-null    float64       \n 15  CPI_Lag1      299 non-null    float64       \ndtypes: datetime64[ns](1), float64(14), int32(1)\nmemory usage: 38.5 KB\nNone"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#processing-personal-income-and-saving-data-from-bea-table-20100",
    "href": "technical-details/data-cleaning/main.html#processing-personal-income-and-saving-data-from-bea-table-20100",
    "title": "Data Cleaning",
    "section": "Processing Personal Income and Saving Data from BEA Table 20100",
    "text": "Processing Personal Income and Saving Data from BEA Table 20100\n\nraw_data_file = os.path.join(raw_data_folder, \"t20100_raw_data.csv\")\nt20100_data = pd.read_csv(raw_data_file)\n\n# Filter key data\ndef clean_t20100_data(df):\n    # Define the keywords for the fields we need\n    keywords = [\n        \"Personal income\",\n        \"Equals: Disposable personal income\",\n        \"Equals: Personal saving\",\n        \"Personal saving as a percentage of disposable personal income\",\n        \"Personal consumption expenditures\",\n        \"Less: Personal current taxes\"\n    ]\n    # Filter the data we need\n    filtered_df = df[df[\"LineDescription\"].str.contains(\"|\".join(keywords), case=False, na=False)]\n    # Keep only the required fields\n    filtered_df = filtered_df[[\"TimePeriod\", \"LineDescription\", \"DataValue\"]]\n    # Convert DataValue to numeric type\n    \n    filtered_df[\"DataValue\"] = filtered_df[\"DataValue\"].astype(str)  # Ensure it's a string\n    filtered_df[\"DataValue\"] = filtered_df[\"DataValue\"].str.replace(\",\", \"\", regex=True).str.strip()  # Remove commas and spaces\n\n    # Convert DataValue to numeric type\n    filtered_df[\"DataValue\"] = pd.to_numeric(filtered_df[\"DataValue\"], errors=\"coerce\")\n\n    return filtered_df\n\nfiltered_t20100 = clean_t20100_data(t20100_data)\n\n# Print the filtered data\nprint(\"Filtered T20100 data:\")\nprint(filtered_t20100.head())\n\n\n# Pivot the data to wide format\nt20100_wide = filtered_t20100.pivot(index=\"TimePeriod\", columns=\"LineDescription\", values=\"DataValue\").reset_index()\n\n# Rename columns (for easier merging later)\nt20100_wide.rename(columns={\n    \"Personal income\": \"Personal_Income (Millions of $)\",\n    \"Equals: Disposable personal income\": \"Disposable_Income (Millions of $)\",\n    \"Equals: Personal saving\": \"Personal_Saving (Millions of $)\",\n    \"Personal saving as a percentage of disposable personal income\": \"Saving_Rate (%)\",\n    \"Personal consumption expenditures\": \"Consumption_Expenditures (Millions of $)\",\n    \"Less: Personal current taxes\": \"Personal_Taxes (Millions of $)\"\n}, inplace=True)\n\n# Print the reshaped data\nprint(\"Reshaped T20100 data:\")\nprint(t20100_wide.tail())\n\nfiltered_data = t20100_wide[t20100_wide[\"TimePeriod\"] &gt;= 2000]\n\n\n\nfile_name = os.path.join(output_folder_cleaned, f\"personal_info_cleaned.csv\")\nfiltered_data.to_csv(file_name, index=False)\nprint(f\"Data for personal_info_cleaned saved to '{file_name}'.\")\n\nFiltered T20100 data:\n   TimePeriod  LineDescription  DataValue\n0        1929  Personal income    85289.0\n1        1930  Personal income    76533.0\n2        1931  Personal income    65668.0\n3        1932  Personal income    50294.0\n4        1933  Personal income    47234.0\nReshaped T20100 data:\nLineDescription  TimePeriod  \\\n90                     2019   \n91                     2020   \n92                     2021   \n93                     2022   \n94                     2023   \n\nLineDescription  Disposable personal income, chained (2017) dollars  \\\n90                                                             3.1    \n91                                                             6.3    \n92                                                             3.4    \n93                                                            -5.5    \n94                                                             5.1    \n\nLineDescription  Disposable personal income, current dollars  \\\n90                                                       4.6   \n91                                                       7.5   \n92                                                       7.7   \n93                                                       0.7   \n94                                                       9.0   \n\nLineDescription  Disposable_Income (Millions of $)  \\\n90                                      16164498.0   \n91                                      17374805.0   \n92                                      18714400.0   \n93                                      18844013.0   \n94                                      20546800.0   \n\nLineDescription  Personal_Saving (Millions of $)  \\\n90                                     1178158.0   \n91                                     2659010.0   \n92                                     2095700.0   \n93                                      566096.0   \n94                                      967225.0   \n\nLineDescription  Personal_Taxes (Millions of $)  \\\n90                                    2198663.0   \n91                                    2245277.0   \n92                                    2705098.0   \n93                                    3244907.0   \n94                                    2855735.0   \n\nLineDescription  Consumption_Expenditures (Millions of $)  \\\n90                                             14437543.0   \n91                                             14225657.0   \n92                                             16113945.0   \n93                                             17690841.0   \n94                                             18822769.0   \n\nLineDescription  Personal_Income (Millions of $)  \\\n90                                    18363160.0   \n91                                    19620082.0   \n92                                    21419498.0   \n93                                    22088920.0   \n94                                    23402535.0   \n\nLineDescription  Personal income excluding current transfer receipts, Billions of chained (2017) dollars  \\\n90                                                      14700259.0                                         \n91                                                      14708618.0                                         \n92                                                      15385380.0                                         \n93                                                      15459172.0                                         \n94                                                      15880418.0                                         \n\nLineDescription  Personal income receipts on assets  Saving_Rate (%)  \n90                                        2950022.0              7.3  \n91                                        2912422.0             15.3  \n92                                        3180726.0             11.2  \n93                                        3474032.0              3.0  \n94                                        3822888.0              4.7  \nData for personal_info_cleaned saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/personal_info_cleaned.csv'."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#processing-government-spending-data-from-bea-table-10105",
    "href": "technical-details/data-cleaning/main.html#processing-government-spending-data-from-bea-table-10105",
    "title": "Data Cleaning",
    "section": "Processing Government Spending Data from BEA Table 10105",
    "text": "Processing Government Spending Data from BEA Table 10105\n\nraw_data_file = os.path.join(raw_data_folder, \"t10105_raw_data.csv\")\nt10105_data = pd.read_csv(raw_data_file)\n\ndef clean_t10105_data(df):\n    # Define the keywords for the required fields\n    keywords = [\n       \"Government consumption expenditures and gross investment\",\n       \"Federal\",\n       \"State and local\"\n    ]\n    # Filter the required data\n    filtered_df = df[df[\"LineDescription\"].str.contains(\"|\".join(keywords), case=False, na=False)]\n    # Keep only the required columns\n    filtered_df = filtered_df[[\"TimePeriod\", \"LineDescription\", \"DataValue\"]]\n    # Remove commas from the DataValue column (to prepare for numeric conversion)\n    filtered_df[\"DataValue\"] = filtered_df[\"DataValue\"].str.replace(\",\", \"\", regex=True)\n    # Convert the cleaned DataValue column to numeric type\n    filtered_df[\"DataValue\"] = pd.to_numeric(filtered_df[\"DataValue\"], errors=\"coerce\")\n    return filtered_df\n\n# Fetch T10105 data\nfiltered_t10105 = clean_t10105_data(t10105_data)\n\n# Print the filtered data\nprint(\"Filtered T10105 data:\")\nprint(filtered_t10105.head())\n\n# Reshape the data to wide format\nt10105_wide = filtered_t10105.pivot(index=\"TimePeriod\", columns=\"LineDescription\", values=\"DataValue\").reset_index()\n\n# Rename columns (for easier merging or analysis)\nt10105_wide.rename(columns={\n    \"Government consumption expenditures and gross investment\": \"Government consumption expenditures and gross investment\",\n    \n}, inplace=True)\n\n# Convert the data to billions of dollars by dividing by 1000\n# Add \"(Millions of $)\" to column names for better clarity\nt10105_wide.rename(columns=lambda x: f\"{x} (Millions of $)\" if x != \"TimePeriod\" else x, inplace=True)\n\n# Print the reshaped data\nprint(\"Reshaped T10105 data:\")\nprint(t10105_wide.tail())\n\n# Save the data as a CSV file\nfile_name = os.path.join(output_folder_cleaned, f\"Government_Spending_Breakdown.csv\")\nt10105_wide.to_csv(file_name, index=False)\nprint(f\"Data for Government_Spending_Breakdown saved to '{file_name}'.\")\n\nFiltered T10105 data:\n      TimePeriod                                    LineDescription  DataValue\n1995        1929  Government consumption expenditures and gross ...       9622\n1996        1930  Government consumption expenditures and gross ...      10273\n1997        1931  Government consumption expenditures and gross ...      10169\n1998        1932  Government consumption expenditures and gross ...       8946\n1999        1933  Government consumption expenditures and gross ...       8875\nReshaped T10105 data:\nLineDescription  TimePeriod  Federal (Millions of $)  \\\n90                     2019                  1419500   \n91                     2020                  1523016   \n92                     2021                  1603184   \n93                     2022                  1641036   \n94                     2023                  1762627   \n\nLineDescription  Government consumption expenditures and gross investment (Millions of $)  \\\n90                                                         3785995                          \n91                                                         3999638                          \n92                                                         4203482                          \n93                                                         4453765                          \n94                                                         4710522                          \n\nLineDescription  State and local (Millions of $)  \n90                                       2366496  \n91                                       2476621  \n92                                       2600299  \n93                                       2812730  \n94                                       2947896  \nData for Government_Spending_Breakdown saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/Government_Spending_Breakdown.csv'."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#processing-market-data-and-calculating-technical-indicators",
    "href": "technical-details/data-cleaning/main.html#processing-market-data-and-calculating-technical-indicators",
    "title": "Data Cleaning",
    "section": "Processing Market Data and Calculating Technical Indicators",
    "text": "Processing Market Data and Calculating Technical Indicators\n\n\ndef calculate_rsi(data, window=14):\n    \"\"\"\n    Calculate RSI (Relative Strength Index) indicator.\n    \"\"\"\n    delta = data['Close'].diff(1)\n    gain = (delta.where(delta &gt; 0, 0)).rolling(window=window).mean()\n    loss = (-delta.where(delta &lt; 0, 0)).rolling(window=window).mean()\n    rs = gain / loss\n    rsi = 100 - (100 / (1 + rs))\n    return rsi\n\ndef clean_and_process_data(data, date_range):\n    \"\"\"\n    Cleans and processes raw financial data, including calculating technical indicators,\n    filling missing values, and generating target variables.\n    \"\"\"\n    try:\n        # Ensure the 'Date' column is in datetime format and set it as the index\n        data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n        data = data.set_index(\"Date\")\n\n        # === Calculate technical indicators ===\n        data[\"MA_50\"] = data[\"Close\"].rolling(window=50).mean()  # 50-day moving average\n        data[\"MA_200\"] = data[\"Close\"].rolling(window=200).mean()  # 200-day moving average\n        data[\"Daily_Return\"] = data[\"Close\"].pct_change()  # Daily return\n        data[\"Volatility\"] = data[\"Daily_Return\"].rolling(window=30).std()  # 30-day volatility\n        data[\"RSI\"] = calculate_rsi(data, window=14)  # RSI indicator\n        data[\"EMA_12\"] = data[\"Close\"].ewm(span=12, adjust=False).mean()  # 12-day EMA\n        data[\"EMA_26\"] = data[\"Close\"].ewm(span=26, adjust=False).mean()  # 26-day EMA\n        data[\"MACD\"] = data[\"EMA_12\"] - data[\"EMA_26\"]  # MACD line\n        data[\"Signal_Line\"] = data[\"MACD\"].ewm(span=9, adjust=False).mean()  # Signal line\n        data[\"High_Low\"] = data[\"High\"] - data[\"Low\"]  # High-Low range\n        data[\"High_Close\"] = abs(data[\"High\"] - data[\"Close\"].shift(1))  # High-Close range\n        data[\"Low_Close\"] = abs(data[\"Low\"] - data[\"Close\"].shift(1))  # Low-Close range\n        data[\"True_Range\"] = data[[\"High_Low\", \"High_Close\", \"Low_Close\"]].max(axis=1)  # True Range\n        data[\"ATR\"] = data[\"True_Range\"].rolling(window=14).mean()  # Average True Range (ATR)\n        data[\"OBV\"] = (np.sign(data[\"Close\"].diff()) * data[\"Volume\"]).fillna(0).cumsum()  # On-Balance Volume (OBV)\n\n        # Add lagged technical indicators\n        for lag in [1, 2, 3]:\n            data[f\"RSI_Lag{lag}\"] = data[\"RSI\"].shift(lag)\n            data[f\"MACD_Lag{lag}\"] = data[\"MACD\"].shift(lag)\n            data[f\"Close_Lag{lag}\"] = data[\"Close\"].shift(lag)\n            data[f\"Volume_Lag{lag}\"] = data[\"Volume\"].shift(lag)\n\n        # === Align data with the specified date range ===\n        data = data.reindex(date_range)\n\n        # === Drop rows with insufficient data for technical indicators ===\n        # Drop rows where MA_200 or other long-window indicators are NaN\n        data.dropna(subset=[\"MA_200\", \"EMA_12\", \"EMA_26\"], inplace=True)\n\n        # === Fill missing values ===\n        # Fill price data\n        data[\"Close\"] = data[\"Close\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        data[\"Open\"] = data[\"Open\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        data[\"High\"] = data[\"High\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        data[\"Low\"] = data[\"Low\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n\n        # Fill volume data\n        data[\"Volume\"] = data[\"Volume\"].fillna(0)\n\n        # Fill technical indicators\n        data[\"MA_50\"] = data[\"MA_50\"].fillna(method=\"ffill\")\n        data[\"MA_200\"] = data[\"MA_200\"].fillna(method=\"ffill\")\n        data[\"Daily_Return\"] = data[\"Daily_Return\"].fillna(0)  # Default daily return to 0\n        data[\"Volatility\"] = data[\"Volatility\"].fillna(0)  # Default volatility to 0\n        data[\"RSI\"] = data[\"RSI\"].fillna(50)  # Default RSI to neutral value of 50\n        data[\"MACD\"] = data[\"MACD\"].fillna(0)\n        data[\"Signal_Line\"] = data[\"Signal_Line\"].fillna(0)\n        data[\"ATR\"] = data[\"ATR\"].fillna(0)\n        data[\"High_Low\"] = data[\"High_Low\"].fillna(0)\n        data[\"High_Close\"] = data[\"High_Close\"].fillna(0)\n        data[\"Low_Close\"] = data[\"Low_Close\"].fillna(0)\n        data[\"True_Range\"] = data[\"True_Range\"].fillna(0)\n\n        # Fill lagged indicators\n        for lag in [1, 2, 3]:\n            data[f\"RSI_Lag{lag}\"] = data[f\"RSI_Lag{lag}\"].fillna(50)\n            data[f\"MACD_Lag{lag}\"] = data[f\"MACD_Lag{lag}\"].fillna(0)\n            data[f\"Close_Lag{lag}\"] = data[f\"Close_Lag{lag}\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n            data[f\"Volume_Lag{lag}\"] = data[f\"Volume_Lag{lag}\"].fillna(0)\n\n        # === Generate target variables ===\n        data[\"Target\"] = np.where(data[\"Close\"].shift(-5) &gt; data[\"Close\"], 1, 0)  # Binary target\n        data[\"Future_Return_5D\"] = data[\"Close\"].shift(-5) / data[\"Close\"] - 1  # 5-day future return\n\n        # Fill target variables\n        data[\"Target\"] = data[\"Target\"].fillna(-1)  # Mark missing targets as -1\n        data[\"Future_Return_5D\"] = data[\"Future_Return_5D\"].fillna(0)\n\n        # Reset index\n        data.reset_index(inplace=True)\n\n        # Check for any remaining missing values\n        if data.isnull().values.any():\n            print(\"Warning: There are still missing values after processing:\")\n            print(data.isnull().sum())\n\n        return data\n    except Exception as e:\n        print(f\"Error cleaning data: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    # Define date range\n    start_date = \"2000-11-01\"\n    end_date = \"2024-11-30\"\n    date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n\n    # Process raw files in the folder\n    for file_name in os.listdir(raw_data_folder):\n        if file_name.endswith(\"_raw_a.csv\"):\n            asset_name = file_name.replace(\"_raw_a.csv\", \"\")\n            raw_file_path = os.path.join(raw_data_folder, file_name)\n            raw_data = pd.read_csv(raw_file_path)\n\n            # Clean and process the data\n            cleaned_data = clean_and_process_data(raw_data, date_range)\n            if cleaned_data is not None:\n                # Save cleaned data\n                processed_file_path = os.path.join(output_folder_cleaned, f\"{asset_name}_cleaned.csv\")\n                cleaned_data.to_csv(processed_file_path, index=False)\n                print(f\"Cleaned data for {asset_name} saved to '{processed_file_path}'.\")\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:59: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Close\"] = data[\"Close\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:60: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Open\"] = data[\"Open\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"High\"] = data[\"High\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:62: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Low\"] = data[\"Low\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:68: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_50\"] = data[\"MA_50\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:69: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_200\"] = data[\"MA_200\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[f\"Close_Lag{lag}\"] = data[f\"Close_Lag{lag}\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n\n\nCleaned data for Gold saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/Gold_cleaned.csv'.\n\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:59: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Close\"] = data[\"Close\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:60: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Open\"] = data[\"Open\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"High\"] = data[\"High\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:62: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Low\"] = data[\"Low\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:68: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_50\"] = data[\"MA_50\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:69: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_200\"] = data[\"MA_200\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[f\"Close_Lag{lag}\"] = data[f\"Close_Lag{lag}\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n\n\nCleaned data for Nasdaq saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/Nasdaq_cleaned.csv'.\n\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:59: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Close\"] = data[\"Close\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:60: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Open\"] = data[\"Open\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"High\"] = data[\"High\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:62: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Low\"] = data[\"Low\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:68: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_50\"] = data[\"MA_50\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:69: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_200\"] = data[\"MA_200\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[f\"Close_Lag{lag}\"] = data[f\"Close_Lag{lag}\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n\n\nCleaned data for Crude_Oil saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/Crude_Oil_cleaned.csv'.\n\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:59: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Close\"] = data[\"Close\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:60: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Open\"] = data[\"Open\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"High\"] = data[\"High\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:62: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Low\"] = data[\"Low\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:68: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_50\"] = data[\"MA_50\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:69: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_200\"] = data[\"MA_200\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[f\"Close_Lag{lag}\"] = data[f\"Close_Lag{lag}\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n\n\nCleaned data for S&P_500 saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/S&P_500_cleaned.csv'.\n\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:59: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Close\"] = data[\"Close\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:60: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Open\"] = data[\"Open\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"High\"] = data[\"High\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:62: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"Low\"] = data[\"Low\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:68: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_50\"] = data[\"MA_50\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:69: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[\"MA_200\"] = data[\"MA_200\"].fillna(method=\"ffill\")\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78435/3783961959.py:85: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  data[f\"Close_Lag{lag}\"] = data[f\"Close_Lag{lag}\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n\n\nCleaned data for Dow_Jones saved to '/Users/qqmian/Desktop/GU_5000/Stock_Market_Performance/data/processed-data/Dow_Jones_cleaned.csv'."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/instructions.html#introduction-and-motivation",
    "title": "Instructions",
    "section": "",
    "text": "This document outlines the data cleaning process for a financial market analysis project. The goal is to convert raw financial and economic data into a clean, structured format suitable for analysis. The process includes processing market data, macroeconomic indicators, personal income statistics and government spending information."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#overview-of-methods",
    "href": "technical-details/data-cleaning/instructions.html#overview-of-methods",
    "title": "Instructions",
    "section": "",
    "text": "The cleaning process involves several key steps and methods:\n\nData Organization\n\nRaw data is stored in data/raw-data\nProcessed data is saved to data/processed-data\nEach dataset type has specific cleaning requirements\n\nKey Processing Steps\n\nMissing value imputation\nDate standardization\nTechnical indicator calculation\nFeature engineering\nData validation and quality checks"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#implementation-steps",
    "href": "technical-details/data-cleaning/instructions.html#implementation-steps",
    "title": "Data Cleaning Process",
    "section": "",
    "text": "Loading and validating raw data from FRED API\nHandling missing values using annual means\nComputing YoY and MoM growth rates\nOutput: macro_series_cleaned.csv\n\n\n\n\n\nExtracting income and saving metrics\nStandardizing monetary units\nOutput: personal_info_cleaned.csv\n\n\n\n\n\nProcessing federal and state data\nStandardizing measurements\nOutput: Government_Spending_Breakdown.csv\n\n\n\n\nTechnical indicators calculated: - Multiple moving averages - RSI and MACD indicators - Volume and volatility metrics - Prediction target variables"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#results-and-technical-implications",
    "href": "technical-details/data-cleaning/instructions.html#results-and-technical-implications",
    "title": "Data Cleaning Process",
    "section": "",
    "text": "Successfully handled missing values across all datasets\nStandardized date ranges from 2000 onwards\nNormalized monetary units for consistency\n\n\n\n\n\nMarket data: Daily frequency with technical indicators\nEconomic data: Monthly frequency with growth rates\nGovernment data: Annual frequency with standardized units\n\n\n\n\n\nForward-looking bias prevention in indicator calculations\nConsistent handling of missing values\nCareful alignment of different time frequencies\n\nThe cleaned datasets provide a robust foundation for subsequent market analysis and modeling work, ensuring data quality and consistency across all sources."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#data-types-and-processing-requirements",
    "href": "technical-details/data-cleaning/instructions.html#data-types-and-processing-requirements",
    "title": "Instructions",
    "section": "",
    "text": "Required Variables: GDP, CPI, Unemployment, FedFundsRate, M2, Umscent\nProcessing Steps:\n\nFill missing values using annual means\nCalculate YoY and MoM growth rates\nGenerate lag variables for key indicators\nFilter data from year 2000 onwards\n\n\n\n\n\n\nRequired Calculations:\n\nMoving averages (50-day, 200-day)\nRSI (14-day period)\nMACD (12-day, 26-day EMAs)\nVolatility metrics\nVolume indicators\nPrice momentum indicators\n\n\n\n\n\n\nKey Metrics:\n\nPersonal income\nDisposable income\nSaving rate\nConsumption expenditures\nPersonal taxes\n\n\n\n\n\n\nComponents:\n\nFederal spending\nState and local spending\nTotal government expenditures"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#implementation-requirements",
    "href": "technical-details/data-cleaning/instructions.html#implementation-requirements",
    "title": "Instructions",
    "section": "",
    "text": "Missing Values\n\nUse forward fill for price data\nApply annual means for economic indicators\nZero-fill for volume data\nDefault values for technical indicators (e.g., RSI = 50)\n\nDate Range\n\nStart date: 2000-11-01\nEnd date: 2024-11-30\nDaily frequency for market data\nMonthly/quarterly frequency for economic data\n\nTechnical Requirements\n\nUse pandas for data manipulation\nImplement proper error handling\nInclude data validation checks\nDocument all cleaning steps\n\n\n\n\n\n\nSetup\n\nDefine output folders\nImport required libraries\nSet up error logging\n\nFunctions\n\nImplement cleaning functions for each data type\nCreate technical indicator calculations\nBuild data validation checks\n\nOutput\n\nSave cleaned data in CSV format\nGenerate processing logs\nInclude data quality reports"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#validation-and-quality-checks",
    "href": "technical-details/data-cleaning/instructions.html#validation-and-quality-checks",
    "title": "Instructions",
    "section": "",
    "text": "Data Completeness\n\nVerify all required columns exist\nCheck date continuity\nValidate data ranges\n\nData Consistency\n\nCheck for outliers\nVerify calculations\nEnsure proper data types\n\nTechnical Indicator Validation\n\nConfirm indicator calculations\nVerify lag variables\nValidate moving averages"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#output-specifications",
    "href": "technical-details/data-cleaning/instructions.html#output-specifications",
    "title": "Instructions",
    "section": "",
    "text": "data/\n├── raw-data/\n│   ├── macro_series_raw_collection.csv\n│   ├── t20100_raw_data.csv\n│   ├── t10105_raw_data.csv\n│   └── [asset]_raw_a.csv\n└── processed-data/\n    ├── macro_series_cleaned.csv\n    ├── personal_info_cleaned.csv\n    ├── Government_Spending_Breakdown.csv\n    └── [asset]_cleaned.csv\n\n\n\n\nAll dates in datetime format\nNo missing values\nProper column naming\nConsistent data types\nTechnical indicators calculated\nTarget variables generated"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#iterative-process-notes",
    "href": "technical-details/data-cleaning/instructions.html#iterative-process-notes",
    "title": "Instructions",
    "section": "",
    "text": "The cleaning process may need to be revisited as:\n\nNew data becomes available\nAnalysis requirements change\nData quality issues are discovered\nNew features are needed"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#introduction-and-motivation",
    "href": "technical-details/data-collection/instructions.html#introduction-and-motivation",
    "title": "Instructions",
    "section": "",
    "text": "The data collecting part focuses on collecting comprehensive financial and economic data to analyze market performance and economic indicators. Our goal is to collect data from three main sources: - Federal Reserve Economic Data (FRED) for macroeconomic indicators - Bureau of Economic Analysis (BEA) for national accounts data - Yahoo Finance for market performance data\nThese sources will help us understand the relationship between economic factors and market behavior."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#overview-of-methods",
    "href": "technical-details/data-collection/instructions.html#overview-of-methods",
    "title": "Instructions",
    "section": "",
    "text": "We use three primary methods for data collection:\n\nFRED API Collection:\n\nFetches macroeconomic indicators (GDP, CPI, Unemployment, etc.)\nRetrieves Treasury yield data\nCollects monetary policy indicators\n\nBEA API Integration:\n\nGathers National Income and Product Accounts (NIPA) data\nCollects personal income and consumption metrics\nObtains government spending information\n\nYahoo Finance Data Retrieval:\n\nDownloads market index data (S&P 500, Nasdaq, Dow Jones)\nCollects commodity prices (Gold, Crude Oil)\nProcesses daily market metrics"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#data-requirements",
    "href": "technical-details/data-collection/instructions.html#data-requirements",
    "title": "Instructions",
    "section": "",
    "text": "Primary focus on tabular data (CSV): - Time series financial data - Economic indicators - Market performance metrics\n\n\n\n\nRegression Targets:\n\nAsset price returns\nEconomic growth rates\nInflation metrics\n\nBinary Classification Targets:\n\nMarket direction (up/down)\nEconomic state (expansion/contraction)\nYield curve status (normal/inverted)\n\nMulticlass Classification Targets:\n\nMarket conditions (bull/bear/sideways)\nEconomic phases (growth/stagnation/recession)\nTrading volume levels (high/medium/low)"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#implementation-requirements",
    "href": "technical-details/data-collection/instructions.html#implementation-requirements",
    "title": "Instructions",
    "section": "",
    "text": "FRED API:\n\nAPI Key required\nMultiple series collection\nJSON response processing\n\nBEA API:\n\nUser authentication\nTable-specific requests\nAnnual data retrieval\n\nYahoo Finance:\n\nNo authentication required\nDaily data download\nMultiple asset handling\n\n\n\n\n\n\nSave raw data to data/raw-data/:\ndata/raw-data/\n├── macro_series_raw_collection.csv\n├── t20100_raw_data.csv\n├── t10105_raw_data.csv\n└── [asset]_raw_a.csv\nMaintain data integrity\nPreserve original format"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#documentation-requirements",
    "href": "technical-details/data-collection/instructions.html#documentation-requirements",
    "title": "Instructions",
    "section": "",
    "text": "Code Documentation:\n\nInclude all API request code\nDocument parameter choices\nExplain data processing steps\n\nData Source Information:\n\nFRED API: https://fred.stlouisfed.org/docs/api/fred/\nBEA API: https://apps.bea.gov/api/\nYahoo Finance API documentation\n\nData Collection Methods:\n\nAPI interaction processes\nData download procedures\nError handling approaches\n\nData Structure Documentation:\n\nColumn descriptions\nData types\nTime periods\nUpdate frequencies"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#workflow-description",
    "href": "technical-details/data-collection/instructions.html#workflow-description",
    "title": "Instructions",
    "section": "",
    "text": "Setup:\n\nConfigure API keys\nCreate storage directories\nImport required libraries\n\nData Collection:\n\nFetch macroeconomic data\nDownload market data\nRetrieve national accounts data\n\nData Verification:\n\nCheck data completeness\nVerify date ranges\nValidate data types\n\nDocumentation:\n\nRecord collection process\nDocument data structure\nNote any issues or limitations"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#quality-assurance",
    "href": "technical-details/data-collection/instructions.html#quality-assurance",
    "title": "Instructions",
    "section": "",
    "text": "Ensure:\n- All API calls are properly authenticated\n- Data is saved in its original form\n- Collection process is reproducible\n- Documentation is complete and clear"
  },
  {
    "objectID": "technical-details/data-collection/main.html#introduction-and-motivation",
    "href": "technical-details/data-collection/main.html#introduction-and-motivation",
    "title": "Data Collection",
    "section": "",
    "text": "The data collecting part focuses on collecting comprehensive financial and economic data to analyze market performance and economic indicators. Our goal is to collect data from three main sources: - Federal Reserve Economic Data (FRED) for macroeconomic indicators - Bureau of Economic Analysis (BEA) for national accounts data - Yahoo Finance for market performance data\nThese sources will help us understand the relationship between economic factors and market behavior."
  },
  {
    "objectID": "technical-details/data-collection/main.html#overview-of-methods",
    "href": "technical-details/data-collection/main.html#overview-of-methods",
    "title": "Data Collection",
    "section": "",
    "text": "We use three primary methods for data collection:\n\nFRED API Collection:\n\nFetches macroeconomic indicators (GDP, CPI, Unemployment, etc.)\nRetrieves Treasury yield data\nCollects monetary policy indicators\n\nBEA API Integration:\n\nGathers National Income and Product Accounts (NIPA) data\nCollects personal income and consumption metrics\nObtains government spending information\n\nYahoo Finance Data Retrieval:\n\nDownloads market index data (S&P 500, Nasdaq, Dow Jones)\nCollects commodity prices (Gold, Crude Oil)\nProcesses daily market metrics"
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-requirements",
    "href": "technical-details/data-collection/main.html#data-requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Primary focus on tabular data (CSV): - Time series financial data - Economic indicators - Market performance metrics\n\n\n\n\nRegression Targets:\n\nAsset price returns\nEconomic growth rates\nInflation metrics\n\nBinary Classification Targets:\n\nMarket direction (up/down)\nEconomic state (expansion/contraction)\nYield curve status (normal/inverted)\n\nMulticlass Classification Targets:\n\nMarket conditions (bull/bear/sideways)\nEconomic phases (growth/stagnation/recession)\nTrading volume levels (high/medium/low)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#implementation-requirements",
    "href": "technical-details/data-collection/main.html#implementation-requirements",
    "title": "Data Collection",
    "section": "",
    "text": "FRED API:\n\nAPI Key required\nMultiple series collection\nJSON response processing\n\nBEA API:\n\nUser authentication\nTable-specific requests\nAnnual data retrieval\n\nYahoo Finance:\n\nNo authentication required\nDaily data download\nMultiple asset handling\n\n\n\n\n\n\nSave raw data to data/raw-data/:\ndata/raw-data/\n├── macro_series_raw_collection.csv\n├── t20100_raw_data.csv\n├── t10105_raw_data.csv\n└── [asset]_raw_a.csv\nMaintain data integrity\nPreserve original format"
  },
  {
    "objectID": "technical-details/data-collection/main.html#documentation-requirements",
    "href": "technical-details/data-collection/main.html#documentation-requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Code Documentation:\n\nInclude all API request code\nDocument parameter choices\nExplain data processing steps\n\nData Source Information:\n\nFRED API: https://fred.stlouisfed.org/docs/api/fred/\nBEA API: https://apps.bea.gov/api/\nYahoo Finance API documentation\n\nData Collection Methods:\n\nAPI interaction processes\nData download procedures\nError handling approaches\n\nData Structure Documentation:\n\nColumn descriptions\nData types\nTime periods\nUpdate frequencies"
  },
  {
    "objectID": "technical-details/data-collection/main.html#workflow-description",
    "href": "technical-details/data-collection/main.html#workflow-description",
    "title": "Data Collection",
    "section": "",
    "text": "Setup:\n\nConfigure API keys\nCreate storage directories\nImport required libraries\n\nData Collection:\n\nFetch macroeconomic data\nDownload market data\nRetrieve national accounts data\n\nData Verification:\n\nCheck data completeness\nVerify date ranges\nValidate data types\n\nDocumentation:\n\nRecord collection process\nDocument data structure\nNote any issues or limitations"
  },
  {
    "objectID": "technical-details/data-collection/main.html#quality-assurance",
    "href": "technical-details/data-collection/main.html#quality-assurance",
    "title": "Data Collection",
    "section": "",
    "text": "Ensure:\n- All API calls are properly authenticated\n- Data is saved in its original form\n- Collection process is reproducible\n- Documentation is complete and clear"
  },
  {
    "objectID": "technical-details/progress-log.html#to-do-list",
    "href": "technical-details/progress-log.html#to-do-list",
    "title": "Project Progress Log",
    "section": "",
    "text": "Complete model optimization\nCreate final visualization dashboard\nWrite comprehensive documentation\nImplement automated testing\nAdd cross-validation framework\nCreate user guide"
  },
  {
    "objectID": "technical-details/progress-log.html#project-members",
    "href": "technical-details/progress-log.html#project-members",
    "title": "Project Progress Log",
    "section": "",
    "text": "Portfolio: [Link to portfolio]\nProject Role: Data Infrastructure & Unsupervised Learning\n\n\n\nResearch and identify data sources\nImplement data collection pipeline\nDevelop data cleaning workflow\nPerform unsupervised learning analysis\nCreate visualization components\nMaintain code repository\n\n\n\n\nW: 12/11-12/13\n\nImplemented unsupervised learning components:\n\nCreated PCA dimensionality reduction\nDeveloped K-means clustering\nAnalyzed market behavior patterns\nGenerated clustering visualizations\n\n\nW: 12/04-12/08\n\nDeveloped data cleaning pipeline:\n\nCreated cleaning functions\nImplemented technical indicators\nAdded data validation\nHandled missing values\n\n\nW: 11/27-12/01\n\nSet up data collection infrastructure:\n\nImplemented FRED API integration\nCreated BEA API connection\nSet up Yahoo Finance pipeline\n\n\nW: 11/20-11/24\n\nInitial project research:\n\nIdentified required data sources\nResearched available APIs\nPlanned data collection strategy\n\n\n\n\n\n\nPortfolio: [Link to portfolio]\nProject Role: Analysis & Supervised Learning\n\n\n\nPerform exploratory data analysis\nDevelop supervised learning models\nCreate performance metrics\nGenerate visualization components\nWrite technical documentation\n\n\n\n\nW: 12/11-12/13\n\nImplemented supervised learning models:\n\nCreated logistic regression\nDeveloped Random Forest model\nAdded XGBoost and LightGBM implementations\nGenerated model performance metrics\n\n\nW: 12/04-12/08\n\nPerformed comprehensive EDA:\n\nCreated distribution analysis\nGenerated correlation matrices\nAnalyzed feature relationships\nProduced statistical summaries\nDeveloped visualization components\n\n\nW: 11/27-12/01\n\nInitial analysis planning:\n\nResearched analysis methods\nPlanned EDA approach\nDesigned model framework"
  },
  {
    "objectID": "technical-details/progress-log.html#technical-implementation-progress",
    "href": "technical-details/progress-log.html#technical-implementation-progress",
    "title": "Project Progress Log",
    "section": "",
    "text": "Research data sources\nSet up API integrations\nCreate data pipeline\nImplement data cleaning\nAdd technical indicators\nComplete unsupervised learning\n\n\n\n\n\nComplete EDA implementation\nCreate basic ML models\nGenerate performance metrics\nAdd visualization components\nImplement cross-validation\nCreate ensemble methods"
  },
  {
    "objectID": "technical-details/progress-log.html#notes-and-observations",
    "href": "technical-details/progress-log.html#notes-and-observations",
    "title": "Project Progress Log",
    "section": "",
    "text": "APIs require careful rate limit management\nTechnical indicators improve model performance\nPCA effectively reduced dimensionality\nK-means revealed distinct market groups\nModels show promising initial results\nNeed to implement automated testing"
  },
  {
    "objectID": "technical-details/progress-log.html#next-steps",
    "href": "technical-details/progress-log.html#next-steps",
    "title": "Project Progress Log",
    "section": "",
    "text": "Complete cross-validation framework\nOptimize model parameters\nCreate final visualization dashboard\nWrite comprehensive documentation\nImplement automated testing\nAdd ensemble methods"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#introduction-and-motivation",
    "href": "technical-details/unsupervised-learning/instructions.html#introduction-and-motivation",
    "title": "Introduction",
    "section": "",
    "text": "This section explores the application of unsupervised learning techniques on financial and macroeconomic datasets. The objective is to reveal hidden patterns and relationships using methods such as PCA and K-Means clustering.\n\n\n\nUse PCA to reduce the dimensionality of high-dimensional datasets while preserving key information.\n\nApply K-Means clustering to identify distinct patterns or groupings within the data.\n\nInvestigate the relationship between financial market behaviors and macroeconomic indicators.\n\nDetect abnormal behaviors, such as extreme volatility, and analyze their technical and macroeconomic causes."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#overview-of-methods",
    "href": "technical-details/unsupervised-learning/instructions.html#overview-of-methods",
    "title": "Introduction",
    "section": "",
    "text": "Combined and cleaned datasets (S&P 500, Crude Oil, Gold, Nasdaq, and Dow Jones).\n\nHandled missing values using forward-fill and backward-fill techniques.\n\nRemoved outliers based on a 3-standard deviation threshold.\n\nStandardized data using StandardScaler.\n\n\n\n\n\nPrincipal Component Analysis (PCA) was used to reduce dimensionality to 2 components.\n\nExplained variance: PC1 (56.9%), PC2 (23.4%).\n\nKey Outputs: - Visualized PCA scatter plot to explore cluster tendencies.\n\n\nPCA is a dimensionality reduction technique used to transform high-dimensional data into a low-dimensional space while retaining as much information as possible about the variance of the data. It works as follows:\n\nNormalize the data to ensure that each feature has a mean of zero and a standard deviation of one. Calculate the covariance matrix to capture the relationship between features.\nPerform a feature decomposition of the covariance matrix to find the principal components (i.e., the directions with the highest variance).\nProject the data onto these principal components to achieve dimensionality reduction.\n\nThe end result is a set of uncorrelated principal components that are ordered by the magnitude of the captured variance, thus simplifying the structure of the dataset while preserving its main patterns and information.\n\n\n\n\n\nApplied K-Means Clustering:\n\nOptimal clusters determined using Elbow Method (k=3).\n\nEvaluated clusters using the Silhouette Score (0.346).\n\nAnalyzed feature distributions by cluster.\n\n\nInvestigated macroeconomic indicators’ relationships with market behavior using correlation analysis and Granger causality tests.\n\n\n\nK-Means is an iterative clustering algorithm designed to partition a dataset into k distinct groups. The process involves:\n1. Initialization: Randomly assigning k cluster centroids.\n2. Assignment Step: Assigning each data point to the closest centroid based on a distance metric (e.g., Euclidean distance).\n3. Update Step: Recomputing centroids as the mean position of all points in a cluster.\n4. Repeating the assignment and update steps until centroids stabilize (convergence).\nThe algorithm minimizes within-cluster variance while maximizing between-cluster variance. Key hyperparameters include the number of clusters (k) and the initialization method (e.g., k-means++ for better starting centroids)."
  },
  {
    "objectID": "technical-details/eda/summary.html",
    "href": "technical-details/eda/summary.html",
    "title": "Summary",
    "section": "",
    "text": "By analyzing the histograms and density plots, we find that the distributions of Volume and Volatility show a clear right-skewed trend, which indicates that there are some extreme values or outliers in the data. In addition, the distribution of price characteristics (e.g. Open, High, Low, Close) is closer to normal distribution, showing a balanced and symmetrical characteristic.\n\n\n\n\nThere is a strong positive correlation between the price characteristics (Open, High, Low, Close).\nMA_50 and MA_200 are highly correlated, as expected.\n\n\n\n\nFuture_Return_5D: Target=1 (price increase) is associated with higher returns compared to Target=0 (price decrease). RSI: Smaller differences between target categories, but Target=1 has a slight aggregation in the 50-60 range. MACD and Signal_Line: both have similar distributions between target categories and do not show significant differences.\n\n\n\n\nThis exploratory data analysis revealed critical insights into the dataset:\n\nUnivariate Analysis identified the distribution characteristics of all numerical features.\n\nCorrelation Analysis highlighted strong relationships among price features and moving averages.\n\nBivariate Analysis demonstrated differences in key variables (e.g., Future_Return_5D) across the target variable.\n\nThese findings provide a solid foundation for further predictive modeling and feature selection."
  },
  {
    "objectID": "technical-details/eda/summary.html#summary-and-interpretation-of-results",
    "href": "technical-details/eda/summary.html#summary-and-interpretation-of-results",
    "title": "Summary",
    "section": "",
    "text": "By analyzing the histograms and density plots, we find that the distributions of Volume and Volatility show a clear right-skewed trend, which indicates that there are some extreme values or outliers in the data. In addition, the distribution of price characteristics (e.g. Open, High, Low, Close) is closer to normal distribution, showing a balanced and symmetrical characteristic.\n\n\n\n\nThere is a strong positive correlation between the price characteristics (Open, High, Low, Close).\nMA_50 and MA_200 are highly correlated, as expected.\n\n\n\n\nFuture_Return_5D: Target=1 (price increase) is associated with higher returns compared to Target=0 (price decrease). RSI: Smaller differences between target categories, but Target=1 has a slight aggregation in the 50-60 range. MACD and Signal_Line: both have similar distributions between target categories and do not show significant differences."
  },
  {
    "objectID": "technical-details/eda/summary.html#conclusion",
    "href": "technical-details/eda/summary.html#conclusion",
    "title": "Summary",
    "section": "",
    "text": "This exploratory data analysis revealed critical insights into the dataset:\n\nUnivariate Analysis identified the distribution characteristics of all numerical features.\n\nCorrelation Analysis highlighted strong relationships among price features and moving averages.\n\nBivariate Analysis demonstrated differences in key variables (e.g., Future_Return_5D) across the target variable.\n\nThese findings provide a solid foundation for further predictive modeling and feature selection."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/unsupervised-learning/main.html#introduction-and-motivation",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This section explores the application of unsupervised learning techniques on financial and macroeconomic datasets. The objective is to reveal hidden patterns and relationships using methods such as PCA and K-Means clustering.\n\n\n\nUse PCA to reduce the dimensionality of high-dimensional datasets while preserving key information.\n\nApply K-Means clustering to identify distinct patterns or groupings within the data.\n\nInvestigate the relationship between financial market behaviors and macroeconomic indicators.\n\nDetect abnormal behaviors, such as extreme volatility, and analyze their technical and macroeconomic causes."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#overview-of-methods",
    "href": "technical-details/unsupervised-learning/main.html#overview-of-methods",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Combined and cleaned datasets (S&P 500, Crude Oil, Gold, Nasdaq, and Dow Jones).\n\nHandled missing values using forward-fill and backward-fill techniques.\n\nRemoved outliers based on a 3-standard deviation threshold.\n\nStandardized data using StandardScaler.\n\n\n\n\n\nPrincipal Component Analysis (PCA) was used to reduce dimensionality to 2 components.\n\nExplained variance: PC1 (56.9%), PC2 (23.4%).\n\nKey Outputs: - Visualized PCA scatter plot to explore cluster tendencies.\n\n\nPCA is a dimensionality reduction technique used to transform high-dimensional data into a low-dimensional space while retaining as much information as possible about the variance of the data. It works as follows:\n\nNormalize the data to ensure that each feature has a mean of zero and a standard deviation of one. Calculate the covariance matrix to capture the relationship between features.\nPerform a feature decomposition of the covariance matrix to find the principal components (i.e., the directions with the highest variance).\nProject the data onto these principal components to achieve dimensionality reduction.\n\nThe end result is a set of uncorrelated principal components that are ordered by the magnitude of the captured variance, thus simplifying the structure of the dataset while preserving its main patterns and information.\n\n\n\n\n\nApplied K-Means Clustering:\n\nOptimal clusters determined using Elbow Method (k=3).\n\nEvaluated clusters using the Silhouette Score (0.346).\n\nAnalyzed feature distributions by cluster.\n\n\nInvestigated macroeconomic indicators’ relationships with market behavior using correlation analysis and Granger causality tests.\n\n\n\nK-Means is an iterative clustering algorithm designed to partition a dataset into k distinct groups. The process involves:\n1. Initialization: Randomly assigning k cluster centroids.\n2. Assignment Step: Assigning each data point to the closest centroid based on a distance metric (e.g., Euclidean distance).\n3. Update Step: Recomputing centroids as the mean position of all points in a cluster.\n4. Repeating the assignment and update steps until centroids stabilize (convergence).\nThe algorithm minimizes within-cluster variance while maximizing between-cluster variance. Key hyperparameters include the number of clusters (k) and the initialization method (e.g., k-means++ for better starting centroids)."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#summary-of-interpretation-of-results",
    "href": "technical-details/unsupervised-learning/main.html#summary-of-interpretation-of-results",
    "title": "Unsupervised Learning",
    "section": "Summary of Interpretation of Results",
    "text": "Summary of Interpretation of Results\n\n1. Dimensionality Reduction\n\nPrincipal Component Analysis (PCA) successfully reduced the dataset’s dimensionality to two principal components, retaining over 80% of the variance.\nThe first principal component (PC1) captured 56.9% of the variance, while the second component (PC2) captured 23.4%.\nPCA revealed natural groupings in the data, which were later analyzed using clustering techniques.\n\n\n\n2. Clustering Analysis\n\nK-Means Clustering identified three distinct clusters in the dataset:\n\nOptimal cluster count (k=3) was determined using the Elbow Method.\nThe clustering structure was validated with a Silhouette Score of 0.346, indicating moderate separation between clusters.\nEach cluster exhibited unique characteristics based on financial metrics like returns, volatility, and technical indicators.\n\nCluster Interpretations:\n\nCluster 1: High volatility, indicating risky assets.\nCluster 2: Low volatility and consistent returns, aligning with stable investments.\nCluster 3: Intermediate behavior with mixed risk profiles.\n\n\n\n\n3. Correlation and Macro Linkages\n\nCorrelation Analysis revealed strong linear relationships among price-related features (e.g., Open, Close, MA_50, MA_200).\nGranger Causality Tests highlighted significant causality between macroeconomic indicators like GDP, CPI, and market returns.\n\nFor example, GDP growth exhibited strong causality with S&P 500 returns, suggesting its importance in market performance analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#conclusion",
    "href": "technical-details/unsupervised-learning/main.html#conclusion",
    "title": "Unsupervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThe unsupervised-learning revealed critical patterns and relationships within financial datasets, providing insights into market dynamics and their macroeconomic drivers.\n\nKey Takeaways:\n\nPCA effectively reduced the dataset’s complexity while preserving critical variance, making it suitable for clustering and visualization.\nK-Means Clustering identified natural groupings, distinguishing risky and stable investments."
  },
  {
    "objectID": "technical-details/eda/main.html#introduction-and-motivation",
    "href": "technical-details/eda/main.html#introduction-and-motivation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The goal of this section is to perform Exploratory Data Analysis (EDA) on a dataset of financial and economic indicators. It help us understand the distributional properties of the variables. This is an important preparation for subsequent modeling.\n\n\nThe datasets used in this analysis include: - Crude Oil Prices - Dow Jones Index - Gold Prices - Government Spending - Macro Series (GDP, CPI, unemployment, and other indicators)\nThese datasets were preprocessed into clean CSV files for further exploration. The key objectives are:\n\nAnalyzing the distributional characteristics of numerical variables;\nExamining correlations between variables;\nExploring the relationship between key features and target variables."
  },
  {
    "objectID": "technical-details/eda/main.html#overview-of-methods",
    "href": "technical-details/eda/main.html#overview-of-methods",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Handled missing values using forward-fill and backward-fill methods.\nRemoved outliers exceeding a 3-standard deviation threshold.\nStandardized numerical features using StandardScaler to ensure uniform scaling.\n\n\n\n\n\nUnivariate Analysis:\n\nUsing histograms and density plots to analyze the distribution of individual variables (e.g., Open, Close, Volume, RSI, MACD).\n\nBivariate Analysis:\n\nUsing box plots and density plots to compare the distribution of variables among target categories\n\nCorrelation Analysis:\n\nIdentifying relationships between numerical features using correlation matrices and heat maps."
  },
  {
    "objectID": "technical-details/eda/main.html#summary-and-interpretation-of-results",
    "href": "technical-details/eda/main.html#summary-and-interpretation-of-results",
    "title": "Exploratory Data Analysis",
    "section": "Summary and Interpretation of Results",
    "text": "Summary and Interpretation of Results\n\nUnivariate Analysis\nBy analyzing the histograms and density plots, we find that the distributions of Volume and Volatility show a clear right-skewed trend, which indicates that there are some extreme values or outliers in the data. In addition, the distribution of price characteristics (e.g. Open, High, Low, Close) is closer to normal distribution, showing a balanced and symmetrical characteristic.\n\n\nCorrelation Analysis\n\nThere is a strong positive correlation between the price characteristics (Open, High, Low, Close).\nMA_50 and MA_200 are highly correlated, as expected.\n\n\n\nBivariate Analysis\nFuture_Return_5D: Target=1 (price increase) is associated with higher returns compared to Target=0 (price decrease). RSI: Smaller differences between target categories, but Target=1 has a slight aggregation in the 50-60 range. MACD and Signal_Line: both have similar distributions between target categories and do not show significant differences."
  },
  {
    "objectID": "technical-details/eda/main.html#conclusion",
    "href": "technical-details/eda/main.html#conclusion",
    "title": "Exploratory Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis exploratory data analysis revealed critical insights into the dataset:\n\nUnivariate Analysis identified the distribution characteristics of all numerical features.\n\nCorrelation Analysis highlighted strong relationships among price features and moving averages.\n\nBivariate Analysis demonstrated differences in key variables (e.g., Future_Return_5D) across the target variable.\n\nThese findings provide a solid foundation for further predictive modeling and feature selection."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "href": "technical-details/data-cleaning/main.html#introduction-and-motivation",
    "title": "Data Cleaning",
    "section": "",
    "text": "This document outlines the data cleaning process for a financial market analysis project. The goal is to convert raw financial and economic data into a clean, structured format suitable for analysis. The process includes processing market data, macroeconomic indicators, personal income statistics and government spending information."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#overview-of-methods",
    "href": "technical-details/data-cleaning/main.html#overview-of-methods",
    "title": "Data Cleaning",
    "section": "",
    "text": "The cleaning process involves several key steps and methods:\n\nData Organization\n\nRaw data is stored in data/raw-data\nProcessed data is saved to data/processed-data\nEach dataset type has specific cleaning requirements\n\nKey Processing Steps\n\nMissing value imputation\nDate standardization\nTechnical indicator calculation\nFeature engineering\nData validation and quality checks"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#data-types-and-processing-requirements",
    "href": "technical-details/data-cleaning/main.html#data-types-and-processing-requirements",
    "title": "Data Cleaning",
    "section": "",
    "text": "Required Variables: GDP, CPI, Unemployment, FedFundsRate, M2, Umscent\nProcessing Steps:\n\nFill missing values using annual means\nCalculate YoY and MoM growth rates\nGenerate lag variables for key indicators\nFilter data from year 2000 onwards\n\n\n\n\n\n\nRequired Calculations:\n\nMoving averages (50-day, 200-day)\nRSI (14-day period)\nMACD (12-day, 26-day EMAs)\nVolatility metrics\nVolume indicators\nPrice momentum indicators\n\n\n\n\n\n\nKey Metrics:\n\nPersonal income\nDisposable income\nSaving rate\nConsumption expenditures\nPersonal taxes\n\n\n\n\n\n\nComponents:\n\nFederal spending\nState and local spending\nTotal government expenditures"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#implementation-requirements",
    "href": "technical-details/data-cleaning/main.html#implementation-requirements",
    "title": "Data Cleaning",
    "section": "",
    "text": "Missing Values\n\nUse forward fill for price data\nApply annual means for economic indicators\nZero-fill for volume data\nDefault values for technical indicators (e.g., RSI = 50)\n\nDate Range\n\nStart date: 2000-11-01\nEnd date: 2024-11-30\nDaily frequency for market data\nMonthly/quarterly frequency for economic data\n\nTechnical Requirements\n\nUse pandas for data manipulation\nImplement proper error handling\nInclude data validation checks\nDocument all cleaning steps\n\n\n\n\n\n\nSetup\n\nDefine output folders\nImport required libraries\nSet up error logging\n\nFunctions\n\nImplement cleaning functions for each data type\nCreate technical indicator calculations\nBuild data validation checks\n\nOutput\n\nSave cleaned data in CSV format\nGenerate processing logs\nInclude data quality reports"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#validation-and-quality-checks",
    "href": "technical-details/data-cleaning/main.html#validation-and-quality-checks",
    "title": "Data Cleaning",
    "section": "",
    "text": "Data Completeness\n\nVerify all required columns exist\nCheck date continuity\nValidate data ranges\n\nData Consistency\n\nCheck for outliers\nVerify calculations\nEnsure proper data types\n\nTechnical Indicator Validation\n\nConfirm indicator calculations\nVerify lag variables\nValidate moving averages"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#output-specifications",
    "href": "technical-details/data-cleaning/main.html#output-specifications",
    "title": "Data Cleaning",
    "section": "",
    "text": "data/\n├── raw-data/\n│   ├── macro_series_raw_collection.csv\n│   ├── t20100_raw_data.csv\n│   ├── t10105_raw_data.csv\n│   └── [asset]_raw_a.csv\n└── processed-data/\n    ├── macro_series_cleaned.csv\n    ├── personal_info_cleaned.csv\n    ├── Government_Spending_Breakdown.csv\n    └── [asset]_cleaned.csv\n\n\n\n\nAll dates in datetime format\nNo missing values\nProper column naming\nConsistent data types\nTechnical indicators calculated\nTarget variables generated"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#iterative-process-notes",
    "href": "technical-details/data-cleaning/main.html#iterative-process-notes",
    "title": "Data Cleaning",
    "section": "",
    "text": "The cleaning process may need to be revisited as:\n\nNew data becomes available\nAnalysis requirements change\nData quality issues are discovered\nNew features are needed"
  },
  {
    "objectID": "technical-details/eda/instructions.html#introduction-and-motivation",
    "href": "technical-details/eda/instructions.html#introduction-and-motivation",
    "title": "Introduction",
    "section": "",
    "text": "The goal of this section is to perform Exploratory Data Analysis (EDA) on a dataset of financial and economic indicators. It help us understand the distributional properties of the variables. This is an important preparation for subsequent modeling.\n\n\nThe datasets used in this analysis include: - Crude Oil Prices - Dow Jones Index - Gold Prices - Government Spending - Macro Series (GDP, CPI, unemployment, and other indicators)\nThese datasets were preprocessed into clean CSV files for further exploration. The key objectives are:\n\nAnalyzing the distributional characteristics of numerical variables;\nExamining correlations between variables;\nExploring the relationship between key features and target variables."
  },
  {
    "objectID": "technical-details/eda/instructions.html#overview-of-methods",
    "href": "technical-details/eda/instructions.html#overview-of-methods",
    "title": "Introduction",
    "section": "",
    "text": "Handled missing values using forward-fill and backward-fill methods.\nRemoved outliers exceeding a 3-standard deviation threshold.\nStandardized numerical features using StandardScaler to ensure uniform scaling.\n\n\n\n\n\nUnivariate Analysis:\n\nUsing histograms and density plots to analyze the distribution of individual variables (e.g., Open, Close, Volume, RSI, MACD).\n\nBivariate Analysis:\n\nUsing box plots and density plots to compare the distribution of variables among target categories\n\nCorrelation Analysis:\n\nIdentifying relationships between numerical features using correlation matrices and heat maps."
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html",
    "href": "technical-details/unsupervised-learning/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Principal Component Analysis (PCA) successfully reduced the dataset’s dimensionality to two principal components, retaining over 80% of the variance.\nThe first principal component (PC1) captured 56.9% of the variance, while the second component (PC2) captured 23.4%.\nPCA revealed natural groupings in the data, which were later analyzed using clustering techniques.\n\n\n\n\n\nK-Means Clustering identified three distinct clusters in the dataset:\n\nOptimal cluster count (k=3) was determined using the Elbow Method.\nThe clustering structure was validated with a Silhouette Score of 0.346, indicating moderate separation between clusters.\nEach cluster exhibited unique characteristics based on financial metrics like returns, volatility, and technical indicators.\n\nCluster Interpretations:\n\nCluster 1: High volatility, indicating risky assets.\nCluster 2: Low volatility and consistent returns, aligning with stable investments.\nCluster 3: Intermediate behavior with mixed risk profiles.\n\n\n\n\n\n\nCorrelation Analysis revealed strong linear relationships among price-related features (e.g., Open, Close, MA_50, MA_200).\nGranger Causality Tests highlighted significant causality between macroeconomic indicators like GDP, CPI, and market returns.\n\nFor example, GDP growth exhibited strong causality with S&P 500 returns, suggesting its importance in market performance analysis.\n\n\n\n\n\n\nThe unsupervised-learning revealed critical patterns and relationships within financial datasets, providing insights into market dynamics and their macroeconomic drivers.\n\n\n\nPCA effectively reduced the dataset’s complexity while preserving critical variance, making it suitable for clustering and visualization.\nK-Means Clustering identified natural groupings, distinguishing risky and stable investments."
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html#summary-of-interpretation-of-results",
    "href": "technical-details/unsupervised-learning/summary.html#summary-of-interpretation-of-results",
    "title": "Summary",
    "section": "",
    "text": "Principal Component Analysis (PCA) successfully reduced the dataset’s dimensionality to two principal components, retaining over 80% of the variance.\nThe first principal component (PC1) captured 56.9% of the variance, while the second component (PC2) captured 23.4%.\nPCA revealed natural groupings in the data, which were later analyzed using clustering techniques.\n\n\n\n\n\nK-Means Clustering identified three distinct clusters in the dataset:\n\nOptimal cluster count (k=3) was determined using the Elbow Method.\nThe clustering structure was validated with a Silhouette Score of 0.346, indicating moderate separation between clusters.\nEach cluster exhibited unique characteristics based on financial metrics like returns, volatility, and technical indicators.\n\nCluster Interpretations:\n\nCluster 1: High volatility, indicating risky assets.\nCluster 2: Low volatility and consistent returns, aligning with stable investments.\nCluster 3: Intermediate behavior with mixed risk profiles.\n\n\n\n\n\n\nCorrelation Analysis revealed strong linear relationships among price-related features (e.g., Open, Close, MA_50, MA_200).\nGranger Causality Tests highlighted significant causality between macroeconomic indicators like GDP, CPI, and market returns.\n\nFor example, GDP growth exhibited strong causality with S&P 500 returns, suggesting its importance in market performance analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/summary.html#conclusion",
    "href": "technical-details/unsupervised-learning/summary.html#conclusion",
    "title": "Summary",
    "section": "",
    "text": "The unsupervised-learning revealed critical patterns and relationships within financial datasets, providing insights into market dynamics and their macroeconomic drivers.\n\n\n\nPCA effectively reduced the dataset’s complexity while preserving critical variance, making it suitable for clustering and visualization.\nK-Means Clustering identified natural groupings, distinguishing risky and stable investments."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#predicting-the-future-direction-of-the-dow-jones-index",
    "href": "technical-details/supervised-learning/main.html#predicting-the-future-direction-of-the-dow-jones-index",
    "title": "Supervised Learning",
    "section": "Predicting the Future Direction of the Dow Jones Index",
    "text": "Predicting the Future Direction of the Dow Jones Index\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.metrics import silhouette_score\n\n# Load the macro_data dataset\nmacro_data = pd.read_csv(\"../../data/processed-data/macro_data.csv\")\n\n# Display the first few rows to understand the structure\nprint(macro_data.head())\n\n# Check the distribution of the target variable\ntarget_counts = macro_data['Target_dow_jones'].value_counts()\nprint(target_counts)\n\n# Plot the distribution of the target variable\nplt.figure(figsize=(6, 4))\ntarget_counts.plot(kind='bar', color=['skyblue', 'orange'])\nplt.title('Distribution of Target_dow_jones')\nplt.xlabel('Target Value')\nplt.ylabel('Frequency')\nplt.show()\n\n       date_x       GDP       CPI  Unemployment  FedFundsRate        M2  \\\n0  2000-01-01 -1.419333 -1.594756     -0.867414      1.754717 -1.259868   \n1  2000-02-01 -1.371109 -1.576251     -0.816379      1.893549 -1.257353   \n2  2000-03-01 -1.371109 -1.549815     -0.867414      1.953048 -1.251527   \n3  2000-04-01 -1.371735 -1.552459     -0.969482      2.037339 -1.240917   \n4  2000-05-01 -1.371109 -1.544528     -0.867414      2.161296 -1.243205   \n\n    Umscent  real_estate   Exports   Imports  ...  MACD_Lag2_dow_jones  \\\n0  2.199732    -1.446673 -1.458048 -1.614811  ...           -65.554617   \n1  2.146820    -1.436692 -1.391110 -1.523412  ...           -65.554617   \n2  1.829348    -1.421047 -1.391110 -1.523412  ...           -65.554617   \n3  1.988084    -1.402256 -1.395372 -1.552202  ...           -65.554617   \n4  2.101467    -1.381961 -1.391110 -1.523412  ...           -65.554617   \n\n   Close_Lag2_dow_jones  Volume_Lag2_dow_jones  RSI_Lag3_dow_jones  \\\n0          10090.900391            160980000.0           46.925369   \n1          10090.900391            160980000.0           46.925369   \n2          10090.900391            160980000.0           46.925369   \n3          10090.900391            160980000.0           46.925369   \n4          10090.900391            160980000.0           46.925369   \n\n   MACD_Lag3_dow_jones  Close_Lag3_dow_jones Volume_Lag3_dow_jones  \\\n0           -50.963179          10222.030273           172820000.0   \n1           -50.963179          10222.030273           172820000.0   \n2           -50.963179          10222.030273           172820000.0   \n3           -50.963179          10222.030273           172820000.0   \n4           -50.963179          10222.030273           172820000.0   \n\n  Target_dow_jones  Future_Return_5D_dow_jones      date_y  \n0              0.0                   -0.034598  2001-08-31  \n1              0.0                   -0.034598  2001-08-31  \n2              0.0                   -0.034598  2001-08-31  \n3              0.0                   -0.034598  2001-08-31  \n4              0.0                   -0.034598  2001-08-31  \n\n[5 rows x 189 columns]\nTarget_dow_jones\n1.0    172\n0.0    127\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Drop non-numeric columns \nX = macro_data.drop(columns=['Target_dow_jones', 'date_x', 'date_y'], errors='ignore')\ny = macro_data['Target_dow_jones']\n\n# Ensure all remaining columns are numeric\nX = X.select_dtypes(include=['float64', 'int64'])\n\n# Fill missing values with the mean of each column\nX = X.fillna(X.mean())\n\n# Split data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Standardize features using StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Output the shape of the datasets\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Testing set shape: {X_test.shape}\")\n\nTraining set shape: (239, 184)\nTesting set shape: (60, 184)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport os\n\n# Initialize the logistic regression model\nmodel = LogisticRegression(max_iter=1000)  # Increase max_iter to ensure convergence\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.2f}\")\n\n# Display classification report\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n\n# Display confusion matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Save predictions into a DataFrame\nresults = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n\n# Define output directory\noutput_dir = \"../../data/visualized-data\"\nos.makedirs(output_dir, exist_ok=True)\n\nModel Accuracy: 0.95\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.96      0.92      0.94        25\n         1.0       0.94      0.97      0.96        35\n\n    accuracy                           0.95        60\n   macro avg       0.95      0.95      0.95        60\nweighted avg       0.95      0.95      0.95        60\n\nConfusion Matrix:\n[[23  2]\n [ 1 34]]\n\n\n\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Compute confusion matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualize confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['0.0', '1.0'], yticklabels=['0.0', '1.0'])\nplt.title(\"Confusion Matrix Heatmap\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve, auc\n\n# Compute probabilities and ROC curve\ny_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1.0\nfpr, tpr, thresholds = roc_curve(y_test, y_proba)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Diagonal line\nplt.title(\"Receiver Operating Characteristic (ROC) Curve\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#predicting-5-day-future-returns-of-the-dow-jones-index",
    "href": "technical-details/supervised-learning/main.html#predicting-5-day-future-returns-of-the-dow-jones-index",
    "title": "Supervised Learning",
    "section": "Predicting 5-Day Future Returns of the Dow Jones Index",
    "text": "Predicting 5-Day Future Returns of the Dow Jones Index\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the merged_assets dataset\ndata = pd.read_csv(\"../../data/processed-data/merged_assets.csv\")\n\n# Feature selection: Choose relevant features for prediction\nselected_features = ['Daily_Return_sp500', 'Volatility_sp500', 'MA_50_sp500', 'MA_200_sp500']\ntarget = 'Future_Return_5D_dow_jones'\n\n# Drop rows with missing values in selected features or target\ndata = data.dropna(subset=selected_features + [target])\n\n# Split data into features (X) and target (y)\nX = data[selected_features]\ny = data[target]\n\n# Split data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Initialize Linear Regression and Random Forest models\nlr_model = LinearRegression()\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train Linear Regression model\nlr_model.fit(X_train, y_train)\ny_pred_lr = lr_model.predict(X_test)\n\n# Train Random Forest model\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)\n\n# Evaluate both models\nmse_lr = mean_squared_error(y_test, y_pred_lr)\nr2_lr = r2_score(y_test, y_pred_lr)\n\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\n\n# Print results\nprint(\"Linear Regression:\")\nprint(f\"Mean Squared Error (MSE): {mse_lr:.4f}\")\nprint(f\"R^2 Score: {r2_lr:.4f}\\n\")\n\nprint(\"Random Forest Regression:\")\nprint(f\"Mean Squared Error (MSE): {mse_rf:.4f}\")\nprint(f\"R^2 Score: {r2_rf:.4f}\")\n\nLinear Regression:\nMean Squared Error (MSE): 0.0006\nR^2 Score: -0.0070\n\nRandom Forest Regression:\nMean Squared Error (MSE): 0.0004\nR^2 Score: 0.3629\n\n\n\nimport matplotlib.pyplot as plt\n\n# Scatter plot for Linear Regression\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_lr, alpha=0.7, label=\"Linear Regression\")\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title(\"Linear Regression: Actual vs Predicted\")\nplt.xlabel(\"Actual Future_Return_5D_dow_jones\")\nplt.ylabel(\"Predicted Future_Return_5D_dow_jones\")\nplt.legend()\nplt.show()\n\n# Scatter plot for Random Forest Regression\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_rf, alpha=0.7, label=\"Random Forest\")\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title(\"Random Forest: Actual vs Predicted\")\nplt.xlabel(\"Actual Future_Return_5D_dow_jones\")\nplt.ylabel(\"Predicted Future_Return_5D_dow_jones\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Initialize XGBoost and LightGBM models\nxgb_model = XGBRegressor(n_estimators=100, random_state=42)\nlgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n\n# Train XGBoost\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Train LightGBM\nlgbm_model.fit(X_train, y_train)\ny_pred_lgbm = lgbm_model.predict(X_test)\n\n# Evaluate XGBoost\nmse_xgb = mean_squared_error(y_test, y_pred_xgb)\nr2_xgb = r2_score(y_test, y_pred_xgb)\n\n# Evaluate LightGBM\nmse_lgbm = mean_squared_error(y_test, y_pred_lgbm)\nr2_lgbm = r2_score(y_test, y_pred_lgbm)\n\n# Print results\nprint(\"XGBoost Regression:\")\nprint(f\"Mean Squared Error (MSE): {mse_xgb:.4f}\")\nprint(f\"R^2 Score: {r2_xgb:.4f}\\n\")\n\nprint(\"LightGBM Regression:\")\nprint(f\"Mean Squared Error (MSE): {mse_lgbm:.4f}\")\nprint(f\"R^2 Score: {r2_lgbm:.4f}\")\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1020\n[LightGBM] [Info] Number of data points in the train set: 4670, number of used features: 4\n[LightGBM] [Info] Start training from score 0.001494\nXGBoost Regression:\nMean Squared Error (MSE): 0.0004\nR^2 Score: 0.2709\n\nLightGBM Regression:\nMean Squared Error (MSE): 0.0004\nR^2 Score: 0.2653\n\n\n\n# Scatter plot for XGBoost\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_xgb, alpha=0.7, label=\"XGBoost\")\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title(\"XGBoost: Actual vs Predicted\")\nplt.xlabel(\"Actual Future_Return_5D_dow_jones\")\nplt.ylabel(\"Predicted Future_Return_5D_dow_jones\")\nplt.legend()\nplt.show()\n\n# Scatter plot for LightGBM\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_lgbm, alpha=0.7, label=\"LightGBM\")\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title(\"LightGBM: Actual vs Predicted\")\nplt.xlabel(\"Actual Future_Return_5D_dow_jones\")\nplt.ylabel(\"Predicted Future_Return_5D_dow_jones\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Load dataset\ndata = pd.read_csv(\"../../data/processed-data/merged_assets.csv\")\n\n# Add lag features\ndata['Lag1_Return'] = data['Future_Return_5D_dow_jones'].shift(1)\ndata['Lag2_Return'] = data['Future_Return_5D_dow_jones'].shift(2)\n\n# Add rolling mean and standard deviation\ndata['Rolling_Mean_5'] = data['Future_Return_5D_dow_jones'].rolling(window=5).mean()\ndata['Rolling_Std_5'] = data['Future_Return_5D_dow_jones'].rolling(window=5).std()\n\n# Drop rows with NaN values caused by lag and rolling calculations\ndata = data.dropna()\n\n# Display the first rows with new features\nprint(data[['Future_Return_5D_dow_jones', 'Lag1_Return', 'Lag2_Return', 'Rolling_Mean_5', 'Rolling_Std_5']].head())\n\n   Future_Return_5D_dow_jones  Lag1_Return  Lag2_Return  Rolling_Mean_5  \\\n4                   -0.045420    -0.030264    -0.018099       -0.016608   \n5                   -0.037069    -0.045420    -0.030264       -0.025229   \n6                   -0.018466    -0.037069    -0.045420       -0.029863   \n7                   -0.024781    -0.018466    -0.037069       -0.031200   \n8                   -0.031627    -0.024781    -0.018466       -0.031473   \n\n   Rolling_Std_5  \n4       0.022281  \n5       0.019495  \n6       0.011857  \n7       0.010499  \n8       0.010486  \n\n\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Select features (including new lag and rolling features) and target\nfeatures = ['Daily_Return_sp500', 'Volatility_sp500', 'Lag1_Return', 'Lag2_Return', 'Rolling_Mean_5', 'Rolling_Std_5']\nX = data[features]\ny = data['Future_Return_5D_dow_jones']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize XGBoost and LightGBM with optimized parameters\nxgb_model = XGBRegressor(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=42)\nlgbm_model = LGBMRegressor(learning_rate=0.1, max_depth=5, n_estimators=300, random_state=42)\n\n# Train XGBoost model\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Train LightGBM model\nlgbm_model.fit(X_train, y_train)\ny_pred_lgbm = lgbm_model.predict(X_test)\n\n# Evaluate XGBoost\nmse_xgb = mean_squared_error(y_test, y_pred_xgb)\nr2_xgb = r2_score(y_test, y_pred_xgb)\n\n# Evaluate LightGBM\nmse_lgbm = mean_squared_error(y_test, y_pred_lgbm)\nr2_lgbm = r2_score(y_test, y_pred_lgbm)\n\n# Print results\nprint(\"XGBoost Regression:\")\nprint(f\"Mean Squared Error (MSE): {mse_xgb:.4f}\")\nprint(f\"R^2 Score: {r2_xgb:.4f}\\n\")\n\nprint(\"LightGBM Regression:\")\nprint(f\"Mean Squared Error (MSE): {mse_lgbm:.4f}\")\nprint(f\"R^2 Score: {r2_lgbm:.4f}\")\n\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1530\n[LightGBM] [Info] Number of data points in the train set: 4667, number of used features: 6\n[LightGBM] [Info] Start training from score 0.001702\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nXGBoost Regression:\nMean Squared Error (MSE): 0.0001\nR^2 Score: 0.8061\n\nLightGBM Regression:\nMean Squared Error (MSE): 0.0001\nR^2 Score: 0.7820\n\n\n\nimport matplotlib.pyplot as plt\n\n# Plot for XGBoost\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_xgb, alpha=0.7, label=\"XGBoost\")\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title(\"XGBoost: Actual vs Predicted\")\nplt.xlabel(\"Actual Future_Return_5D_dow_jones\")\nplt.ylabel(\"Predicted Future_Return_5D_dow_jones\")\nplt.legend()\nplt.show()\n\n# Plot for LightGBM\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_lgbm, alpha=0.7, label=\"LightGBM\")\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\nplt.title(\"LightGBM: Actual vs Predicted\")\nplt.xlabel(\"Actual Future_Return_5D_dow_jones\")\nplt.ylabel(\"Predicted Future_Return_5D_dow_jones\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Get the latest data\nlatest_data = data.iloc[-1]\n\n# Create the feature vector\nlatest_features = [\n    latest_data['Daily_Return_sp500'],\n    latest_data['Volatility_sp500'],\n    latest_data['Lag1_Return'],\n    latest_data['Lag2_Return'],\n    latest_data['Rolling_Mean_5'],\n    latest_data['Rolling_Std_5']\n]\n\n# Use the model to make the prediction\nfuture_price = lgbm_model.predict([latest_features])\n\n# Analyze the prediction\nif future_price[0] &gt; data['Close_dow_jones'].iloc[-1]:\n    print('Predicted future price will increase')\nelse:\n    print('Predicted future price will decrease')\n\nPredicted future price will decrease"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#data-collection-strategies",
    "href": "technical-details/data-collection/methods.html#data-collection-strategies",
    "title": "Methods",
    "section": "",
    "text": "Purpose: Retrieve comprehensive macroeconomic indicators\nSeries Collected:\n\nGross Domestic Product (GDP)\nConsumer Price Index (CPI)\nUnemployment Rate\nFederal Funds Rate\nMoney Supply (M2)\nConsumer Sentiment Index\nReal Estate Price Index\nExports and Imports Data\n\nTechnique:\n\nUtilized requests library for API communication\nConverted raw JSON responses into pandas DataFrames\nStandardized date formats and numeric conversions\nMerged multiple economic series into a single comprehensive dataset\n\n\n\n\n\n\nPurpose: Fetch National Income and Product Accounts (NIPA) data\nTables Retrieved:\n\nT20100: Gross Domestic Product and Personal Consumption\nT10105: National Account Savings and Investment Data\n\nApproach:\n\nCreated a generic fetch_bea_data() function\nConfigured API parameters for annual frequency data\nHandled potential API response variations\nConverted successful responses to CSV for further analysis\n\n\n\n\n\n\nPurpose: Download market performance data for major indices and commodities\nAssets Retrieved:\n\nS&P 500 Index\nNasdaq Composite\nDow Jones Industrial Average\nGold Futures\nCrude Oil Futures\n\nMethod:\n\nImplemented a flexible fetch_data() function\nSet consistent date ranges for historical data\nDownloaded daily price and volume information\nStandardized data columns and saved as CSV files"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#data-processing-techniques",
    "href": "technical-details/data-collection/methods.html#data-processing-techniques",
    "title": "Methods",
    "section": "",
    "text": "Used os module for robust file path handling\nDynamically created output directories\nImplemented consistent file naming conventions\nEnsured data is saved in a structured, accessible format\n\n\n\n\n\nImplemented error handling and logging\nConverted API responses to consistent pandas DataFrame structures\nPerformed data type conversions (datetime, numeric)\nManaged potential missing or invalid data points\n\n\n\n\n\nReset DataFrame indexes\nRenamed columns for clarity\nPerformed basic data cleaning\nPrepared data for subsequent analysis stages"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#modular-design-principles",
    "href": "technical-details/data-collection/methods.html#modular-design-principles",
    "title": "Methods",
    "section": "",
    "text": "Created reusable functions for data fetching\nSeparated concerns between data collection, processing, and storage\nUsed configuration dictionaries for flexible asset and series selection\nImplemented error-tolerant code with try-except blocks"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#security-and-configuration",
    "href": "technical-details/data-collection/methods.html#security-and-configuration",
    "title": "Methods",
    "section": "",
    "text": "Utilized environment-specific API keys\nConfigured date ranges and data retrieval parameters\nImplemented interval and frequency specifications for precise data collection"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#output-and-persistence",
    "href": "technical-details/data-collection/methods.html#output-and-persistence",
    "title": "Methods",
    "section": "",
    "text": "Saved raw collected data as CSV files\nMaintained a consistent output folder structure\nEnabled easy data inspection and further processing"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#technological-stack",
    "href": "technical-details/data-collection/methods.html#technological-stack",
    "title": "Methods",
    "section": "",
    "text": "Programming Language: Python 3.12\nKey Libraries:\n\nrequests for API communication\npandas for data manipulation\nyfinance for financial data retrieval\nos for file system operations\n\nData Sources:\n\nFRED API\nBEA API\nYahoo Finance"
  },
  {
    "objectID": "technical-details/data-collection/methods.html#conclusion",
    "href": "technical-details/data-collection/methods.html#conclusion",
    "title": "Methods",
    "section": "",
    "text": "This methods section provides a comprehensive overview of the technical approaches used in data collection, emphasizing the systematic and robust methodology employed to gather economic and market performance data."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#technical-challenges-and-limitations",
    "href": "technical-details/data-collection/closing.html#technical-challenges-and-limitations",
    "title": "Summary",
    "section": "",
    "text": "Navigating the complex world of economic data APIs comes with many challenges. The financial data landscape is characterized by fragmentation, with each source (FRED, BEA, and Yahoo Finance) that offers unique data formats and reporting mechanisms. So we need a sophisticated approach to data standardization and integration.\nOne of the most important difficulties is the inherent time inconsistency of managing economic indicators. Macroeconomic data often arrive at different frequencies—some monthly, others quarterly or annually—creating data alignment challenges. Our solution is to utilize interpolation and aggregation strategies to transform these disparate data streams into coherent, consistent time series."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#unintended-consequences-and-technical-impact",
    "href": "technical-details/data-collection/closing.html#unintended-consequences-and-technical-impact",
    "title": "Summary",
    "section": "",
    "text": "Irregular Yield Curve:\n\nUnexpected fluctuations in 10-year Treasury yield data detected\nPotential impact on the interpretation of economic indicators\nRecommend further investigation of anomalous data points\n\n\n\n\n\n\nPerformance Bottleneck:\n\nIdentify potential optimization opportunities in the data retrieval and processing pipeline\nEmphasis on the need for a more efficient API request strategy"
  },
  {
    "objectID": "technical-details/data-collection/closing.html#future-work-areas",
    "href": "technical-details/data-collection/closing.html#future-work-areas",
    "title": "DSAN-5000: Project",
    "section": "Future Work Areas",
    "text": "Future Work Areas\n\n1. Technical Improvements\n\nEnhanced Data Integration:\n\nDevelop more complex data alignment algorithms\nImplement advanced interpolation techniques for cross-frequency data\nCreate a more powerful profile verification framework\n\n\n\n\n2. Computational Optimization\n\nPerformance Enhancement:\n\nDiscuss parallel processing technology for API data retrieval\nImplement caching mechanism to reduce redundant API calls\nDevelop more memory-saving data processing methods\n\n\n\n\n3. Advanced Analysis Technology\n\nMachine Learning Integration:\n\nUse collected economic indicators to explore forecasting models\nDevelop more sophisticated feature engineering methods\nImplement time series forecasting models"
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarking-and-comparative-analysis",
    "href": "technical-details/data-collection/closing.html#benchmarking-and-comparative-analysis",
    "title": "DSAN-5000: Project",
    "section": "Benchmarking and Comparative Analysis",
    "text": "Benchmarking and Comparative Analysis\n\nMethod Evaluation\n\nData Collection Efficiency:\n\nCompare current methods with existing methods of collecting financial information\nDemonstrates exceptional flexibility and comprehensive data retrieval\nIdentify the unique advantages of integrating multi-source data\n\n\n\n\nPerformance Indicators\n\nAPI Interaction Efficiency:\n\nAnalyze the success rate of API requests\nAssess the completeness of data from different sources\nCalculate processing time and resource utilization"
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-suggestions",
    "href": "technical-details/data-collection/closing.html#conclusion-and-suggestions",
    "title": "Summary",
    "section": "",
    "text": "Successfully developed a powerful multi-source financial data collection framework\nCreate a flexible modular data processing pipeline\nAbility to effectively process complex, multi-frequency economic data sets\n\n\n\n\n\nShort-Term Improvements:\n\nImplement more complex error handling\nEnhanced data verification mechanism\nOptimize API request strategy\n\nLong-Term Research Directions:\n\nExplore machine learning integration for predictive economic analysis\nDevelop a more comprehensive data standardization framework\nEstablish a common economic data collection and processing toolkit\n\n\n\n\n\nThis step is an important step in creating a more flexible approach to financial and economic data collection. By solving technical challenges and using solutions, we lay the foundation for subsequent economic analysis techniques."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#acknowledgments",
    "href": "technical-details/data-collection/closing.html#acknowledgments",
    "title": "DSAN-5000: Project",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nSpecial thanks to the API providers (FRED, BEA, Yahoo Finance) for making their data accessible, and to the open-source community for providing powerful data processing tools."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#goals",
    "href": "technical-details/data-collection/overview.html#goals",
    "title": "Overview",
    "section": "",
    "text": "The primary goal of this project is to develop a framework for collecting, integrating, and preprocessing economic and financial data from multiple authoritative sources. By creating a systematic approach to data aggregation, we aim to enable advanced economic research, predictive analysis, and data-driven decision-making."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#motivation",
    "href": "technical-details/data-collection/overview.html#motivation",
    "title": "Overview",
    "section": "",
    "text": "The complexity of economic analysis is fundamentally constrained by consistency, and integration. Existing approaches to economic data collection often suffer from:\n\nFragmented data sources\nInconsistent reporting frequencies\nLimited cross-source compatibility"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#objectives",
    "href": "technical-details/data-collection/overview.html#objectives",
    "title": "Overview",
    "section": "",
    "text": "Our specific objectives are focused on creating a framework for economic data collection, emphasizing comprehensive aggregation, data quality, research potential, and methodological transparency:\n\n\n\nCollect diverse economic indicators from multiple authoritative sources\nIntegrate data from key economic and financial platforms:\n\nFederal Reserve Economic Data (FRED)\nBureau of Economic Analysis (BEA)\nYahoo Finance\n\nGather a wide range of economic and market performance metrics, including:\n\nMacroeconomic indicators (GDP, CPI, Unemployment)\nMarket indices (S&P 500, Nasdaq, Dow Jones)\nCommodity prices (Gold, Crude Oil)\nTreasury yield data\n\n\n\n\n\n\nImplement data processing techniques to ensure data reliability\nAddress challenges of data integration across different sources\nDevelop methods to:\n\nHandle missing or invalid data points\nConvert and standardize data types\nMerge datasets from multiple sources\n\nEnsure clean, consistent data preparation for further analysis\n\n\n\n\n\nPrepare a structured, accessible dataset for economic research\nSave raw collected data in easily manageable CSV format\nCreate a foundation for potential future analyses, including:\n\nEconomic trend identification\nMarket performance studies\nMacroeconomic indicator exploration\n\nProvide a flexible dataset that supports various research approaches\n\n\n\n\n\nDocument the entire data collection process comprehensively\nProvide clear, reproducible code for data retrieval\nExplain the rationale behind data source selection\nEnsure other researchers can understand and potentially replicate the data collection methodology"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#comprehensive-data-aggregation",
    "href": "technical-details/data-collection/overview.html#comprehensive-data-aggregation",
    "title": "Overview",
    "section": "",
    "text": "Collect diverse economic indicators from multiple authoritative sources\nIntegrate data from key economic and financial platforms:\n\nFederal Reserve Economic Data (FRED)\nBureau of Economic Analysis (BEA)\nYahoo Finance\n\nGather a wide range of economic and market performance metrics, including:\n\nMacroeconomic indicators (GDP, CPI, Unemployment)\nMarket indices (S&P 500, Nasdaq, Dow Jones)\nCommodity prices (Gold, Crude Oil)\nTreasury yield data"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#data-quality-and-consistency",
    "href": "technical-details/data-collection/overview.html#data-quality-and-consistency",
    "title": "Overview",
    "section": "",
    "text": "Implement data processing techniques to ensure data reliability\nAddress challenges of data integration across different sources\nDevelop methods to:\n\nHandle missing or invalid data points\nConvert and standardize data types\nMerge datasets from multiple sources\n\nEnsure clean, consistent data preparation for further analysis"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#research-enablement",
    "href": "technical-details/data-collection/overview.html#research-enablement",
    "title": "Overview",
    "section": "",
    "text": "Prepare a structured, accessible dataset for economic research\nSave raw collected data in easily manageable CSV format\nCreate a foundation for potential future analyses, including:\n\nEconomic trend identification\nMarket performance studies\nMacroeconomic indicator exploration\n\nProvide a flexible dataset that supports various research approaches"
  },
  {
    "objectID": "technical-details/data-collection/overview.html#methodological-transparency",
    "href": "technical-details/data-collection/overview.html#methodological-transparency",
    "title": "Overview",
    "section": "",
    "text": "Document the entire data collection process comprehensively\nProvide clear, reproducible code for data retrieval\nExplain the rationale behind data source selection\nEnsure other researchers can understand and potentially replicate the data collection methodology"
  },
  {
    "objectID": "technical-details/supervised-learning/summary.html",
    "href": "technical-details/supervised-learning/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Model Performance:\n\nLogistic Regression achieved an accuracy of 95%.\nConfusion Matrix highlighted balanced performance across both classes:\n\nPrecision: 96% (for Target=0) and 94% (for Target=1).\nRecall: 92% (for Target=0) and 97% (for Target=1).\n\n\n\n\n\n\n\nLinear Regression:\n\nMSE: 0.0006\nR² Score: -0.007\n\nRandom Forest:\n\nMSE: 0.0004\nR² Score: 0.362\n\nXGBoost:\n\nMSE: 0.0001\nR² Score: 0.801\n\nLightGBM:\n\nMSE: 0.0002\nR² Score: 0.750\n\nKey Insight: XGBoost outperformed other models in regression, achieving the highest R² Score of 0.801, indicating better predictive power.\n\n\n\n\n\n\nThis supervised learning analysis successfully demonstrated the use of classification and regression techniques to predict market outcomes. Using XGboost, we predicted that the 5-Day Future Returns of the Dow Jones Index would decrease. Nonlinear models such as XGBoost and LightGBM are able to perform better when facing complex market dynamics.\n\nBinary Classification:\n\nLogistic Regression showed high accuracy and balanced performance, making it a reliable model for predicting price direction.\n\nRegression:\n\nXGBoost and LightGBM proved effective for regression tasks, with XGBoost providing the best results in terms of accuracy and interpretability.\n\n\n\n\n\nFor Investors: Accurate predictions of market movements and returns can aid in portfolio management and decision-making.\nFor Researchers: Insights gained from this analysis can guide the development of more advanced predictive models, including ensemble approaches."
  },
  {
    "objectID": "technical-details/supervised-learning/summary.html#summary-of-findings",
    "href": "technical-details/supervised-learning/summary.html#summary-of-findings",
    "title": "Summary",
    "section": "",
    "text": "Model Performance:\n\nLogistic Regression achieved an accuracy of 95%.\nConfusion Matrix highlighted balanced performance across both classes:\n\nPrecision: 96% (for Target=0) and 94% (for Target=1).\nRecall: 92% (for Target=0) and 97% (for Target=1).\n\n\n\n\n\n\n\nLinear Regression:\n\nMSE: 0.0006\nR² Score: -0.007\n\nRandom Forest:\n\nMSE: 0.0004\nR² Score: 0.362\n\nXGBoost:\n\nMSE: 0.0001\nR² Score: 0.801\n\nLightGBM:\n\nMSE: 0.0002\nR² Score: 0.750\n\nKey Insight: XGBoost outperformed other models in regression, achieving the highest R² Score of 0.801, indicating better predictive power."
  },
  {
    "objectID": "technical-details/supervised-learning/summary.html#conclusion",
    "href": "technical-details/supervised-learning/summary.html#conclusion",
    "title": "Summary",
    "section": "",
    "text": "This supervised learning analysis successfully demonstrated the use of classification and regression techniques to predict market outcomes. Using XGboost, we predicted that the 5-Day Future Returns of the Dow Jones Index would decrease. Nonlinear models such as XGBoost and LightGBM are able to perform better when facing complex market dynamics.\n\nBinary Classification:\n\nLogistic Regression showed high accuracy and balanced performance, making it a reliable model for predicting price direction.\n\nRegression:\n\nXGBoost and LightGBM proved effective for regression tasks, with XGBoost providing the best results in terms of accuracy and interpretability.\n\n\n\n\n\nFor Investors: Accurate predictions of market movements and returns can aid in portfolio management and decision-making.\nFor Researchers: Insights gained from this analysis can guide the development of more advanced predictive models, including ensemble approaches."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/main.html#introduction-and-motivation",
    "title": "Supervised Learning",
    "section": "",
    "text": "This section is to predict financial outcomes and market behavior. The primary focus is on predicting: 1. The future direction of the Dow Jones Index (binary classification). 2. The 5-day future return of the Dow Jones Index (regression)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#overview-of-methods",
    "href": "technical-details/supervised-learning/main.html#overview-of-methods",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization: Features were standardized using StandardScaler to ensure consistent scaling.\nFeature Selection: Selected key financial indicators (e.g., Daily_Return_sp500, Volatility_sp500, MA_50_sp500) and lagged features (e.g., Lag1_Return, Lag2_Return).\nHandling Missing Data: Missing values were imputed using column means or removed when necessary.\nTrain-Test Split: Data was split into 80% training and 20% testing subsets.\n\n\n\n\n\n\n\n\nAlgorithm: Logistic Regression\nObjective: Predict the direction of the Dow Jones Index (Target_dow_jones).\nEvaluation Metrics: Accuracy, Precision, Recall, F1 Score, and Confusion Matrix.\n\n\n\n\n\nAlgorithms: Linear Regression, Random Forest, XGBoost, and LightGBM.\nObjective: Predict the 5-day future return (Future_Return_5D_dow_jones).\nEvaluation Metrics: Mean Squared Error (MSE) and R² Score.\n\n\n\n\n\n\n\nCross-Validation: 5-fold cross-validation to ensure robust model evaluation.\nParameter Tuning: Hyperparameters for Random Forest, XGBoost, and LightGBM were optimized to improve performance."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#summary-of-findings",
    "href": "technical-details/supervised-learning/main.html#summary-of-findings",
    "title": "Supervised Learning",
    "section": "Summary of Findings",
    "text": "Summary of Findings\n\nBinary Classification\n\nModel Performance:\n\nLogistic Regression achieved an accuracy of 95%.\nConfusion Matrix highlighted balanced performance across both classes:\n\nPrecision: 96% (for Target=0) and 94% (for Target=1).\nRecall: 92% (for Target=0) and 97% (for Target=1).\n\n\n\n\n\nRegression\n\nLinear Regression:\n\nMSE: 0.0006\nR² Score: -0.007\n\nRandom Forest:\n\nMSE: 0.0004\nR² Score: 0.362\n\nXGBoost:\n\nMSE: 0.0001\nR² Score: 0.801\n\nLightGBM:\n\nMSE: 0.0002\nR² Score: 0.750\n\nKey Insight: XGBoost outperformed other models in regression, achieving the highest R² Score of 0.801, indicating better predictive power."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#conclusion",
    "href": "technical-details/supervised-learning/main.html#conclusion",
    "title": "Supervised Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThis supervised learning analysis successfully demonstrated the use of classification and regression techniques to predict market outcomes. Using XGboost, we predicted that the 5-Day Future Returns of the Dow Jones Index would decrease. Nonlinear models such as XGBoost and LightGBM are able to perform better when facing complex market dynamics.\n\nBinary Classification:\n\nLogistic Regression showed high accuracy and balanced performance, making it a reliable model for predicting price direction.\n\nRegression:\n\nXGBoost and LightGBM proved effective for regression tasks, with XGBoost providing the best results in terms of accuracy and interpretability.\n\n\n\nImplications:\n\nFor Investors: Accurate predictions of market movements and returns can aid in portfolio management and decision-making.\nFor Researchers: Insights gained from this analysis can guide the development of more advanced predictive models, including ensemble approaches."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#data-cleaning-for-assets",
    "href": "technical-details/unsupervised-learning/main.html#data-cleaning-for-assets",
    "title": "Unsupervised Learning",
    "section": "Data Cleaning for assets",
    "text": "Data Cleaning for assets\n\n# File paths for all datasets\nfile_paths = {\n    \"sp500\": \"../../data/processed-data/S&P_500_cleaned.csv\",\n    \"oil\": \"../../data/processed-data/Crude_Oil_cleaned.csv\",\n    \"gold\": \"../../data/processed-data/Gold_cleaned.csv\",\n    \"nasdaq\": \"../../data/processed-data/Nasdaq_cleaned.csv\",\n    \"dow_jones\": \"../../data/processed-data/Dow_Jones_cleaned.csv\"\n}\n\n# Load all datasets into a dictionary\ndatasets = {name: pd.read_csv(path, parse_dates=[\"index\"]) for name, path in file_paths.items()}\n\n# Initialize merged_data with S&P 500 dataset\nasset_data = datasets[\"sp500\"]\n\n# Merge datasets iteratively with others\nfor name, df in datasets.items():\n    if name != \"sp500\":  # Skip the first dataset (already initialized)\n        asset_data = pd.merge(asset_data, df, on=\"index\", suffixes=(None, f\"_{name}\"))\n\n# Handle missing data by forward-filling and dropping any remaining NaNs\nasset_data = asset_data.fillna(method='ffill').dropna()\n\nsp500_columns = [\n    'Open', 'High', 'Low', 'Close', 'Volume', 'MA_50', 'MA_200', \n    'Daily_Return', 'Volatility', 'RSI', 'EMA_12', 'EMA_26', 'MACD', \n    'Signal_Line', 'High_Low', 'High_Close', 'Low_Close', 'True_Range', \n    'ATR', 'OBV', 'RSI_Lag1', 'MACD_Lag1', 'Close_Lag1', 'Volume_Lag1', \n    'RSI_Lag2', 'MACD_Lag2', 'Close_Lag2', 'Volume_Lag2', 'RSI_Lag3', \n    'MACD_Lag3', 'Close_Lag3', 'Volume_Lag3', 'Target', 'Future_Return_5D'\n]\n\n# Add the prefix \"_sp500\" only to the SP500 columns\nasset_data.rename(columns={col: f\"{col}_sp500\" for col in sp500_columns}, inplace=True)\n\n# Output the first few rows to check the new column names\nasset_data.head()\n# Save the merged dataset\nasset_data.to_csv(\"../../data/processed-data/merged_assets.csv\", index=False)\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78437/2584913618.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  asset_data = asset_data.fillna(method='ffill').dropna()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#merge-macro-data-and-asset-data",
    "href": "technical-details/unsupervised-learning/main.html#merge-macro-data-and-asset-data",
    "title": "Unsupervised Learning",
    "section": "Merge macro data and asset data",
    "text": "Merge macro data and asset data\n\n# Load macroeconomic data (monthly data)\nmacro_data = pd.read_csv(\"../../data/processed-data/macro_series_cleaned.csv\")\nmacro_data.head()\n\n# 1. Ensure the `date` column is in datetime format\nmacro_data['date'] = pd.to_datetime(macro_data['date'])\n\n# Extract year and month from the `date` column\nmacro_data['year_month'] = macro_data['date'].dt.to_period('M').astype(str)  # Format: YYYY-MM\n\n# Check for missing values and fill them\nmacro_data.fillna(method='ffill', inplace=True)  # Forward fill any missing values\n\n# 2. Generate derived features\n# Calculate year-over-year (YoY) and month-over-month (MoM) growth rates\nmacro_data['GDP_YoY'] = macro_data['GDP'].pct_change(12) * 100  # YoY growth rate for GDP (%)\nmacro_data['GDP_MoM'] = macro_data['GDP'].pct_change(1) * 100   # MoM growth rate for GDP (%)\nmacro_data['CPI_YoY'] = macro_data['CPI'].pct_change(12) * 100  # YoY growth rate for CPI (%)\n\n# Create lagged features\nmacro_data['GDP_Lag1'] = macro_data['GDP'].shift(1)  # GDP lagged by one month\nmacro_data['CPI_Lag1'] = macro_data['CPI'].shift(1)  # CPI lagged by one month\n\n# Load asset data (daily data)\nasset_data['date'] = pd.to_datetime(asset_data['index'])  # Convert `index` to datetime\n\n# Extract year and month from the asset data\nasset_data['year_month'] = asset_data['date'].dt.to_period('M').astype(str)  # Format: YYYY-MM\n\n# Downsample asset data (daily frequency -&gt; monthly frequency)\n# Group by `year_month` and take the last available data for each month\nasset_data_monthly = asset_data.groupby('year_month').last().reset_index()\n\n# 3. Merge macroeconomic data with asset data\n# Merge both datasets based on the `year_month` column\nmacro_data = pd.merge(macro_data, asset_data_monthly, on='year_month', how='left')\n\n# Handle missing values in the merged dataset\nmacro_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\nmacro_data.fillna(method='bfill', inplace=True)  # Backward fill missing values (if needed)\n\n# 4. Standardize the data\n# Select the columns to be standardized\ncolumns_to_scale = ['GDP', 'CPI', 'Unemployment', 'FedFundsRate', 'M2',\n                    'Umscent', 'real_estate', 'Exports', 'Imports',\n                    'GDP_YoY', 'GDP_MoM', 'CPI_YoY', 'GDP_Lag1', 'CPI_Lag1']\n\nscaler = StandardScaler()\n# Apply standard scaling (mean = 0, std dev = 1) to the selected columns\nmacro_data[columns_to_scale] = scaler.fit_transform(macro_data[columns_to_scale])\n\n# 5. Inspect the processed data\nprint(macro_data.head())\n\n# Save the processed dataset to a CSV file\nmacro_data.to_csv(\"../../data/processed-data/macro_data.csv\", index=False)\n\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78437/4291704790.py:12: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  macro_data.fillna(method='ffill', inplace=True)  # Forward fill any missing values\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78437/4291704790.py:39: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  macro_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n/var/folders/1z/n1k_hhl168s2rqld4kqxxr880000gn/T/ipykernel_78437/4291704790.py:40: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  macro_data.fillna(method='bfill', inplace=True)  # Backward fill missing values (if needed)\n\n\n      date_x       GDP       CPI  Unemployment  FedFundsRate        M2  \\\n0 2000-01-01 -1.419333 -1.594756     -0.867414      1.754717 -1.259868   \n1 2000-02-01 -1.371109 -1.576251     -0.816379      1.893549 -1.257353   \n2 2000-03-01 -1.371109 -1.549815     -0.867414      1.953048 -1.251527   \n3 2000-04-01 -1.371735 -1.552459     -0.969482      2.037339 -1.240917   \n4 2000-05-01 -1.371109 -1.544528     -0.867414      2.161296 -1.243205   \n\n    Umscent  real_estate   Exports   Imports  ...  MACD_Lag2_dow_jones  \\\n0  2.199732    -1.446673 -1.458048 -1.614811  ...           -65.554617   \n1  2.146820    -1.436692 -1.391110 -1.523412  ...           -65.554617   \n2  1.829348    -1.421047 -1.391110 -1.523412  ...           -65.554617   \n3  1.988084    -1.402256 -1.395372 -1.552202  ...           -65.554617   \n4  2.101467    -1.381961 -1.391110 -1.523412  ...           -65.554617   \n\n   Close_Lag2_dow_jones  Volume_Lag2_dow_jones  RSI_Lag3_dow_jones  \\\n0          10090.900391            160980000.0           46.925369   \n1          10090.900391            160980000.0           46.925369   \n2          10090.900391            160980000.0           46.925369   \n3          10090.900391            160980000.0           46.925369   \n4          10090.900391            160980000.0           46.925369   \n\n   MACD_Lag3_dow_jones  Close_Lag3_dow_jones Volume_Lag3_dow_jones  \\\n0           -50.963179          10222.030273           172820000.0   \n1           -50.963179          10222.030273           172820000.0   \n2           -50.963179          10222.030273           172820000.0   \n3           -50.963179          10222.030273           172820000.0   \n4           -50.963179          10222.030273           172820000.0   \n\n  Target_dow_jones  Future_Return_5D_dow_jones     date_y  \n0              0.0                   -0.034598 2001-08-31  \n1              0.0                   -0.034598 2001-08-31  \n2              0.0                   -0.034598 2001-08-31  \n3              0.0                   -0.034598 2001-08-31  \n4              0.0                   -0.034598 2001-08-31  \n\n[5 rows x 189 columns]"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#select-key-features",
    "href": "technical-details/unsupervised-learning/main.html#select-key-features",
    "title": "Unsupervised Learning",
    "section": "Select key features",
    "text": "Select key features\n\n\n# Select key features for analysis\nselected_features = [\n    'GDP', 'CPI', 'Unemployment', 'FedFundsRate', 'M2', 'Umscent',  # Macroeconomic data\n    'MA_50_oil', 'MA_50_gold', 'MA_50_nasdaq', 'MA_50_dow_jones',   # Technical indicators\n    'RSI_oil', 'RSI_gold', 'RSI_nasdaq', 'RSI_dow_jones',           # RSI indicators\n    'MACD_oil', 'MACD_gold', 'MACD_nasdaq', 'MACD_dow_jones',       # MACD indicators\n    'Future_Return_5D_oil', 'Future_Return_5D_gold',                # Target variables\n    'Future_Return_5D_nasdaq', 'Future_Return_5D_dow_jones',\n    'MA_50_sp500', 'RSI_sp500', 'MACD_sp500', 'Future_Return_5D_sp500',\n    'Close_oil', 'Close_gold', 'Close_nasdaq', 'Close_dow_jones', 'Close_sp500'\n]\n\nselected_features_2 = [\n    'Close_oil', 'Close_gold', 'Close_nasdaq', 'Close_dow_jones', 'Close_sp500',\n    'MA_50_oil', 'MA_50_gold', 'MA_50_nasdaq', 'MA_50_dow_jones',   # Technical indicators\n    'RSI_oil', 'RSI_gold', 'RSI_nasdaq', 'RSI_dow_jones',           # RSI indicators\n    'MACD_oil', 'MACD_gold', 'MACD_nasdaq', 'MACD_dow_jones',       # MACD indicators\n    'Future_Return_5D_oil', 'Future_Return_5D_gold',                # Target variables\n    'Future_Return_5D_nasdaq', 'Future_Return_5D_dow_jones',\n    'MA_50_sp500', 'RSI_sp500', 'MACD_sp500', 'Future_Return_5D_sp500'\n]\n\n# Filter the dataset to include only the selected features\nmacro_data = macro_data[selected_features]\n\n# Drop rows with missing values (if any remain after filling)\nmacro_data.dropna(inplace=True)\n\n\n# Filter the dataset to include only the selected features\nasset_data = asset_data[selected_features_2]\n\n# Drop rows with missing values (if any remain after filling)\nasset_data.dropna(inplace=True)"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#multi-market-linkage-analysis-correlation-and-pca",
    "href": "technical-details/unsupervised-learning/main.html#multi-market-linkage-analysis-correlation-and-pca",
    "title": "Unsupervised Learning",
    "section": "Multi-market linkage analysis (correlation and PCA)",
    "text": "Multi-market linkage analysis (correlation and PCA)\n\n# Questions\n# Are there any connections between the behavioral patterns of various markets (such as S&P 500, gold, crude oil, Nasdaq, etc.)?\n# Is it possible to divide the market into different categories (such as high-risk vs. safe-haven assets) based on market characteristics (such as RSI, MACD, rainbows, etc.)?\n\nsave_path = \"../../data/visualized-data\"\nimport os\nos.makedirs(save_path, exist_ok=True)\n\n# -------------------------------\n# Step 1: Calculate Daily Returns\n# -------------------------------\n\n# Calculate daily returns from closing prices\nreturns = asset_data.filter(like='Close').pct_change().dropna()\n\n# Compute the correlation matrix\ncorrelation_matrix = returns.corr()\nprint(asset_data.columns)\n\n# Plot the heatmap of the correlation matrix\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\nplt.title(\"Market Correlation Matrix\")\nplt.savefig(os.path.join(save_path, 'market_correlation_matrix.png'))\nplt.show()\nplt.close()\n\n# -------------------------------\n# Step 2: Remove Outliers\n# -------------------------------\n\n# Calculate upper and lower limits (3 standard deviations from the mean)\nupper_limit = returns.mean() + 3 * returns.std()\nlower_limit = returns.mean() - 3 * returns.std()\n\n# Remove outliers based on the calculated limits\nfiltered_returns = returns[(returns &lt; upper_limit) & (returns &gt; lower_limit)].dropna()\n\n# -------------------------------\n# Step 3: Standardize the Data\n# -------------------------------\n\n# Standardize the filtered returns\nscaler = StandardScaler()\nfiltered_returns_scaled = scaler.fit_transform(filtered_returns)\n\n# -------------------------------\n# Step 4: PCA for Dimensionality Reduction\n# -------------------------------\n\n# Perform PCA and reduce to 2 components\npca = PCA(n_components=2)\npca_data = pca.fit_transform(filtered_returns_scaled)\n\n# Print the explained variance ratio of the first two principal components\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n\n# -------------------------------\n# Step 5: Visualize PCA Results\n# -------------------------------\n\n# Scatter plot of the PCA results\nplt.figure(figsize=(8, 6))\nplt.scatter(pca_data[:, 0], pca_data[:, 1], alpha=0.5)\nplt.title(\"PCA of Market Returns\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.savefig(os.path.join(save_path, 'market_pca_results.png'))\nplt.show()\nplt.close()\n\nIndex(['Close_oil', 'Close_gold', 'Close_nasdaq', 'Close_dow_jones',\n       'Close_sp500', 'MA_50_oil', 'MA_50_gold', 'MA_50_nasdaq',\n       'MA_50_dow_jones', 'RSI_oil', 'RSI_gold', 'RSI_nasdaq', 'RSI_dow_jones',\n       'MACD_oil', 'MACD_gold', 'MACD_nasdaq', 'MACD_dow_jones',\n       'Future_Return_5D_oil', 'Future_Return_5D_gold',\n       'Future_Return_5D_nasdaq', 'Future_Return_5D_dow_jones', 'MA_50_sp500',\n       'RSI_sp500', 'MACD_sp500', 'Future_Return_5D_sp500'],\n      dtype='object')\n\n\n\n\n\n\n\n\n\nExplained variance ratio: [0.56997664 0.2343595 ]"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#multi-market-linkage-analysis-kmeans",
    "href": "technical-details/unsupervised-learning/main.html#multi-market-linkage-analysis-kmeans",
    "title": "Unsupervised Learning",
    "section": "Multi-market linkage analysis (kmeans)",
    "text": "Multi-market linkage analysis (kmeans)\n\n# -------------------------------\n# Step 6: Kmeans\n# -------------------------------\n\n# -------------------------------\n#   Step 1: Elbow Method to Determine Optimal k\n# -------------------------------\ninertia = []\nrange_k = range(2, 10)\nfor k in range_k:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(pca_data)\n    inertia.append(kmeans.inertia_)\n\n# Plot the Elbow Curve\nplt.figure(figsize=(8, 6))\nplt.plot(range_k, inertia, marker='o')\nplt.title(\"Elbow Method for K-Means\")\nplt.xlabel(\"Number of Clusters (k)\")\nplt.ylabel(\"Inertia\")\nplt.savefig(os.path.join(save_path, 'kmeans_elbow_curve.png'))\nplt.show()\nplt.close()\n\n# -------------------------------\n#   Step 2: Fit K-Means with Optimal k\n# -------------------------------\noptimal_k = 3\nkmeans = KMeans(n_clusters=optimal_k, random_state=42)\nkmeans_labels = kmeans.fit_predict(pca_data)\n\n# Evaluate clustering with Silhouette Score\nsilhouette_avg = silhouette_score(pca_data, kmeans_labels)\nprint(\"Silhouette Score:\", silhouette_avg)\n\n# -------------------------------\n#   Step 3: Visualize K-Means Clustering Results\n# -------------------------------\nplt.figure(figsize=(8, 6))\nplt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.7)\nplt.title(\"K-Means Clustering\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.colorbar(label='Cluster')\nplt.savefig(os.path.join(save_path, 'kmeans_clustering_results.png'))\nplt.show()\nplt.close()\n\n\n\n\n\n\n\n\nSilhouette Score: 0.3465435410464012"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca-clustered-feature-distributions",
    "href": "technical-details/unsupervised-learning/main.html#pca-clustered-feature-distributions",
    "title": "Unsupervised Learning",
    "section": "PCA Clustered Feature Distributions",
    "text": "PCA Clustered Feature Distributions\n\n# -------------------------------\n#   Step 7: PCA Dimensionality Reduction and Clustering\n# -------------------------------\n\n# Create a DataFrame for PCA-transformed data\nclustered_data = pd.DataFrame(pca_data, columns=[\"PC1\", \"PC2\"])  # Principal Component Data\nclustered_data[\"Cluster\"] = kmeans_labels  # Add cluster labels\n\n# Calculate the mean value of each principal component for each cluster\ncluster_means = clustered_data.groupby(\"Cluster\").mean()\nprint(\"Cluster Means:\\n\", cluster_means)\n\n# Visualize the distribution of principal components by cluster\ncluster_means.T.plot(kind='bar', figsize=(12, 6))\nplt.title(\"Principal Component Distribution by Cluster\")\nplt.ylabel(\"Mean Value\")\nplt.xlabel(\"Principal Component\")\nplt.legend(title=\"Cluster\")\nplt.savefig(os.path.join(save_path, 'pc_distribution_by_cluster.png'))\nplt.show()\nplt.close()\n\n# Display the PCA loading matrix (components)\nprint(\"PCA Components:\\n\", pca.components_)\n\n# -------------------------------\n#   Step 8: Combine Cluster Labels with Original Feature Data\n# -------------------------------\nasset_data = asset_data.loc[filtered_returns.index] \nprint(\"Length of asset_data after alignment:\", len(asset_data))\nprint(\"Length of kmeans_labels:\", len(kmeans_labels))\n\nif len(asset_data) != len(kmeans_labels):\n    asset_data = asset_data.iloc[:len(kmeans_labels)]\n\noriginal_clustered_data = asset_data.copy()\noriginal_clustered_data[\"Cluster\"] = kmeans_labels\n\n# -------------------------------\n#   Step 9: Standardize the Feature Data\n# -------------------------------\n\n# Extract features (excluding the cluster labels)\nfeatures = original_clustered_data.drop(columns=[\"Cluster\"])\n\n# Use StandardScaler to standardize the feature data\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features)\n\n# Convert standardized data back to a DataFrame\nfeatures_scaled_df = pd.DataFrame(features_scaled, columns=features.columns)\nfeatures_scaled_df[\"Cluster\"] = original_clustered_data[\"Cluster\"]  # Add cluster labels\n\n# -------------------------------\n#   Step 10: Calculate Mean of Standardized Features by Cluster\n# -------------------------------\n\n# Group by cluster and calculate the mean of each feature\nscaled_cluster_means = features_scaled_df.groupby(\"Cluster\").mean()\n\n# -------------------------------\n#   Step 11: Visualize Clustered Feature Distributions\n# -------------------------------\n\n# Plot the mean values of standardized features for each cluster\nplt.figure(figsize=(14, 8))\nscaled_cluster_means.T.plot(kind=\"bar\", figsize=(14, 8))\nplt.title(\"Standardized Feature Distribution by Cluster\")\nplt.ylabel(\"Mean Value (Standardized)\")\nplt.xlabel(\"Feature\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'feature_distribution_by_cluster.png'))\nplt.show()\nplt.close()\n\n# -------------------------------\n#   Step 12: Filter Features for Specific Analysis (e.g., RSI and MACD)\n# -------------------------------\n\n# Select only RSI and MACD-related features for focused analysis\nselected_features = [col for col in features.columns if \"RSI\" in col or \"MACD\" in col]\n\n# Calculate the mean values of the selected features by cluster\nfiltered_cluster_means = scaled_cluster_means[selected_features]\n\n# Plot the distribution of the selected features by cluster\nplt.figure(figsize=(14, 8))\nfiltered_cluster_means.T.plot(kind=\"bar\", figsize=(14, 8))\nplt.title(\"Standardized RSI and MACD Distribution by Cluster\")\nplt.ylabel(\"Mean Value (Standardized)\")\nplt.xlabel(\"Feature\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'rsi_macd_distribution_by_cluster.png'))\nplt.show()\nplt.close()\n\nCluster Means:\n               PC1       PC2\nCluster                    \n0        2.061786  0.017502\n1       -0.091093 -0.008499\n2       -2.607793  0.003936\n\n\n\n\n\n\n\n\n\nPCA Components:\n [[ 1.40296075e-01 -4.31707754e-05  5.59111904e-01  5.68705657e-01\n   5.86757841e-01]\n [ 6.65914113e-01  7.39689815e-01 -7.00063894e-02 -5.36870662e-02\n  -4.04249546e-02]]\nLength of asset_data after alignment: 5650\nLength of kmeans_labels: 5650\n\n\n&lt;Figure size 1400x800 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1400x800 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# -------------------------------\n#   Step 13: Group and Filter Features by Categories\n# -------------------------------\n\n# Group features based on their categories\n# Price-related features: Columns containing \"Close\" or \"MA_50\"\nprice_features = [col for col in features.columns if \"Close\" in col or \"MA_50\" in col]\n\n# Technical indicators: Columns containing \"RSI\" or \"MACD\"\nindicator_features = [col for col in features.columns if \"RSI\" in col or \"MACD\" in col]\n\n# Target variables: Columns containing \"Future_Return\"\ntarget_features = [col for col in features.columns if \"Future_Return\" in col]\n\n# -------------------------------\n#   Step 14: Visualize Price Features by Cluster\n# -------------------------------\n\n# Plot the standardized mean distribution of price-related features by cluster\nscaled_cluster_means[price_features].T.plot(kind=\"bar\", figsize=(14, 8))\nplt.title(\"Standardized Price Features Distribution by Cluster\")\nplt.ylabel(\"Mean Value (Standardized)\")\nplt.xlabel(\"Feature\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'price_features_by_cluster.png'))\nplt.show()\nplt.close()\n\n# -------------------------------\n#   Step 15: Visualize Technical Indicators by Cluster\n# -------------------------------\n\n# Plot the standardized mean distribution of technical indicators (RSI and MACD) by cluster\nscaled_cluster_means[indicator_features].T.plot(kind=\"bar\", figsize=(14, 8))\nplt.title(\"Standardized Technical Indicators Distribution by Cluster\")\nplt.ylabel(\"Mean Value (Standardized)\")\nplt.xlabel(\"Feature\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'target_variables_by_cluster.png'))\nplt.show()\nplt.close()\n\n# -------------------------------\n#   Step 16: Visualize Target Variables by Cluster\n# -------------------------------\n\n# Plot the standardized mean distribution of target variables (Future_Return features) by cluster\nscaled_cluster_means[target_features].T.plot(kind=\"bar\", figsize=(14, 8))\nplt.title(\"Standardized Target Variables Distribution by Cluster\")\nplt.ylabel(\"Mean Value (Standardized)\")\nplt.xlabel(\"Feature\")\nplt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'technical_indicators_by_cluster.png'))\nplt.show()\nplt.close()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#the-relationship-between-macroeconomics-and-market-performance",
    "href": "technical-details/unsupervised-learning/main.html#the-relationship-between-macroeconomics-and-market-performance",
    "title": "Unsupervised Learning",
    "section": "The relationship between macroeconomics and market performance",
    "text": "The relationship between macroeconomics and market performance\n\n\n# question:\n# How do macroeconomic data (such as GDP growth, CPI, unemployment rate, federal funds rate, etc.) affect the performance of different markets?\n# Is the behavior of different markets driven by the same macroeconomic indicators?\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\n# -------------------------------\n#   Step 1: Define Macro Features and Asset Returns\n# -------------------------------\n# Define macroeconomic features\nmacro_features = ['GDP', 'CPI', 'Unemployment', 'FedFundsRate']\n\n# Define asset return columns (e.g., S&P 500, oil, gold, etc.)\nasset_features = ['Close_sp500', 'Close_oil', 'Close_gold', 'Close_nasdaq', 'Close_dow_jones']\n\n# -------------------------------\n#   Step 2: Calculate Correlations Between Macro Variables and Asset Returns\n# -------------------------------\n# Create an empty DataFrame to store correlations\ncorrelations = pd.DataFrame(index=macro_features, columns=asset_features)\n\n# Loop through each asset and calculate correlations with macro features\nfor asset in asset_features:\n    correlations[asset] = macro_data[macro_features].corrwith(returns[asset])\n\n# Visualize the correlations\ncorrelations.plot(kind='bar', figsize=(12, 8), colormap='viridis')\nplt.title(\"Correlation Between Macro Variables and Asset Returns\")\nplt.ylabel(\"Correlation\")\nplt.xlabel(\"Macro Variables\")\nplt.xticks(rotation=45)\nplt.legend(title=\"Assets\")\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'macro_asset_correlations.png'))\nplt.show()\nplt.close()\n\n# -------------------------------\n#   Step 3: Perform Granger Causality Tests for Each Asset\n# -------------------------------\n# Define max lag for Granger causality tests\nmax_lag = 4\n\n# Loop through each asset and test causality with macro variables\ngranger_results = {}\nfor asset in asset_features:\n    for macro in macro_features:\n        # Align data and drop missing values\n        data = macro_data[[macro, asset]].dropna()\n        # Perform Granger causality test\n        print(f\"Granger Causality Test: {macro} -&gt; {asset}\")\n        result = grangercausalitytests(data, maxlag=max_lag, verbose=False)\n        granger_results[(macro, asset)] = result\n        # Display p-values for each lag\n        for lag, test_result in result.items():\n            p_value = test_result[0]['ssr_chi2test'][1]\n            print(f\"Lag {lag}: p-value = {p_value:.4f}\")\n        print(\"-\" * 50)\n\n\n\n\n\n\n\n\nGranger Causality Test: GDP -&gt; Close_sp500\nLag 1: p-value = 0.0137\nLag 2: p-value = 0.0206\nLag 3: p-value = 0.0013\nLag 4: p-value = 0.0000\n--------------------------------------------------\nGranger Causality Test: CPI -&gt; Close_sp500\n\n\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\nLag 1: p-value = 0.0000\nLag 2: p-value = 0.0001\nLag 3: p-value = 0.0001\nLag 4: p-value = 0.0001\n--------------------------------------------------\nGranger Causality Test: Unemployment -&gt; Close_sp500\nLag 1: p-value = 0.0229\nLag 2: p-value = 0.0000\nLag 3: p-value = 0.0000\nLag 4: p-value = 0.0000\n--------------------------------------------------\nGranger Causality Test: FedFundsRate -&gt; Close_sp500\nLag 1: p-value = 0.0014\nLag 2: p-value = 0.0780\nLag 3: p-value = 0.0050\nLag 4: p-value = 0.0812\n--------------------------------------------------\nGranger Causality Test: GDP -&gt; Close_oil\nLag 1: p-value = 0.6948\nLag 2: p-value = 0.0066\nLag 3: p-value = 0.0030\nLag 4: p-value = 0.0012\n--------------------------------------------------\nGranger Causality Test: CPI -&gt; Close_oil\nLag 1: p-value = 0.1169\nLag 2: p-value = 0.0000\nLag 3: p-value = 0.0000\nLag 4: p-value = 0.0000\n--------------------------------------------------\nGranger Causality Test: Unemployment -&gt; Close_oil\nLag 1: p-value = 0.5079\nLag 2: p-value = 0.0001\nLag 3: p-value = 0.0003\nLag 4: p-value = 0.0002\n--------------------------------------------------\nGranger Causality Test: FedFundsRate -&gt; Close_oil\n\n\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\nLag 1: p-value = 0.0587\nLag 2: p-value = 0.1552\nLag 3: p-value = 0.0478\nLag 4: p-value = 0.0884\n--------------------------------------------------\nGranger Causality Test: GDP -&gt; Close_gold\n\n\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\nLag 1: p-value = 0.5160\nLag 2: p-value = 0.5347\nLag 3: p-value = 0.1150\nLag 4: p-value = 0.2512\n--------------------------------------------------\nGranger Causality Test: CPI -&gt; Close_gold\nLag 1: p-value = 0.9421\nLag 2: p-value = 0.0844\nLag 3: p-value = 0.2157\nLag 4: p-value = 0.2093\n--------------------------------------------------\nGranger Causality Test: Unemployment -&gt; Close_gold\nLag 1: p-value = 0.5553\nLag 2: p-value = 0.8625\nLag 3: p-value = 0.8373\nLag 4: p-value = 0.8699\n--------------------------------------------------\nGranger Causality Test: FedFundsRate -&gt; Close_gold\nLag 1: p-value = 0.0388\nLag 2: p-value = 0.8806\nLag 3: p-value = 0.6924\nLag 4: p-value = 0.9076\n--------------------------------------------------\nGranger Causality Test: GDP -&gt; Close_nasdaq\nLag 1: p-value = 0.0118\nLag 2: p-value = 0.0069\nLag 3: p-value = 0.0009\nLag 4: p-value = 0.0006\n--------------------------------------------------\nGranger Causality Test: CPI -&gt; Close_nasdaq\nLag 1: p-value = 0.0000\nLag 2: p-value = 0.0004\nLag 3: p-value = 0.0001\nLag 4: p-value = 0.0002\n--------------------------------------------------\nGranger Causality Test: Unemployment -&gt; Close_nasdaq\n\n\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n\n\nLag 1: p-value = 0.0375\nLag 2: p-value = 0.0008\nLag 3: p-value = 0.0001\nLag 4: p-value = 0.0004\n--------------------------------------------------\nGranger Causality Test: FedFundsRate -&gt; Close_nasdaq\nLag 1: p-value = 0.0034\nLag 2: p-value = 0.2720\nLag 3: p-value = 0.0258\nLag 4: p-value = 0.2066\n--------------------------------------------------\nGranger Causality Test: GDP -&gt; Close_dow_jones\nLag 1: p-value = 0.0044\nLag 2: p-value = 0.0135\nLag 3: p-value = 0.0008\nLag 4: p-value = 0.0000\n--------------------------------------------------\nGranger Causality Test: CPI -&gt; Close_dow_jones\nLag 1: p-value = 0.0000\nLag 2: p-value = 0.0000\nLag 3: p-value = 0.0001\nLag 4: p-value = 0.0000\n--------------------------------------------------\nGranger Causality Test: Unemployment -&gt; Close_dow_jones\nLag 1: p-value = 0.0211\nLag 2: p-value = 0.0000\nLag 3: p-value = 0.0000\nLag 4: p-value = 0.0000\n--------------------------------------------------\nGranger Causality Test: FedFundsRate -&gt; Close_dow_jones\nLag 1: p-value = 0.0010\nLag 2: p-value = 0.0155\nLag 3: p-value = 0.0010\nLag 4: p-value = 0.0260\n--------------------------------------------------\n\n\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn(\n/Users/qqmian/opt/anaconda3/envs/dsan5400/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1556: FutureWarning: verbose is deprecated since functions should not print results\n  warnings.warn("
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#detection-of-abnormal-market-behavior",
    "href": "technical-details/unsupervised-learning/main.html#detection-of-abnormal-market-behavior",
    "title": "Unsupervised Learning",
    "section": "Detection of abnormal market behavior",
    "text": "Detection of abnormal market behavior\n\n# question:\n# Are there ways to detect unusual behavior in markets (such as extreme volatility or abnormal returns)?\n# What technical indicators or macro factors might these abnormal behaviors be related to?\n\n# -------------------------------------\n# Step 1: Define a threshold for extreme volatility\n# -------------------------------------\n# Calculate the threshold for extreme volatility (3 standard deviations)\nthreshold = 3 * returns.std()\n\n# Identify rows where any asset's return exceeds the threshold\nextreme_volatility = returns[(returns &gt; threshold).any(axis=1)]\n\n# -------------------------------------\n# Step 2: Plot extreme volatility for multiple assets\n# -------------------------------------\n# List of assets to visualize\nassets_to_plot = ['Close_sp500', 'Close_oil', 'Close_gold', 'Close_nasdaq', 'Close_dow_jones']\n\n# Initialize the figure and axes\nfig, axes = plt.subplots(len(assets_to_plot), 1, figsize=(12, 18), sharex=True)\n\nfor i, asset in enumerate(assets_to_plot):\n    ax = axes[i]\n    # Plot returns for each asset\n    ax.plot(returns.index, returns[asset], label=f'{asset} Returns', color='blue')\n    # Highlight extreme volatility\n    ax.scatter(\n        extreme_volatility.index, \n        extreme_volatility[asset], \n        color='red', \n        label='Extreme Volatility', \n        s=10\n    )\n    # Customization\n    ax.set_title(f\"Extreme Volatility Detection - {asset}\")\n    ax.set_ylabel(\"Returns\")\n    ax.legend(loc='upper left')\n\n# Add a common x-label\nplt.xlabel(\"Time\")\nplt.tight_layout()\nplt.savefig(os.path.join(save_path, 'extreme_volatility_detection.png'))\nplt.show()\nplt.close()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#asset-grouping-and-portfolio-analysis",
    "href": "technical-details/unsupervised-learning/main.html#asset-grouping-and-portfolio-analysis",
    "title": "Unsupervised Learning",
    "section": "Asset Grouping and Portfolio Analysis",
    "text": "Asset Grouping and Portfolio Analysis\n\n# question\n# Can different markets be divided into stable groups (e.g. risky assets vs. safe-haven assets)?\n# Do these groupings help investors with asset allocation (e.g., diversification of investment risk)?\n\nimport cvxpy as cp\n\n# ----------------------------------------------\n# Step 1: Check for NaN or Inf in Returns Data\n# ----------------------------------------------\n# Check for NaN or Inf values in returns\nprint(\"Checking for NaN or Inf values in data...\")\nprint(\"NaN values:\", np.isnan(returns).sum().sum())\nprint(\"Inf values:\", np.isinf(returns).sum().sum())\n\n# Replace Inf values with NaN for data cleaning\nreturns.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n# Remove any rows containing NaN values\nreturns.dropna(inplace=True)\n\n# Ensure using correct asset names\nreturns.columns = ['SP500', 'Oil', 'Gold', 'Nasdaq', 'Dow_Jones']\n\n# ----------------------------------------------\n# Step 2: Compute Returns Statistics\n# ----------------------------------------------\n# Calculate mean returns and volatility for each asset\nasset_stats = pd.DataFrame({\n   'Mean Return': returns.mean(),\n   'Volatility': returns.std()\n})\n\n# Handle any remaining extreme or missing values\nasset_stats.replace([np.inf, -np.inf], np.nan, inplace=True)\nasset_stats.fillna(0, inplace=True)  # Replace NaN with 0 (or use mean/median if needed)\n\n# ----------------------------------------------\n# Step 3: K-Means Clustering for Asset Grouping\n# ----------------------------------------------\n# Standardize features for clustering\nscaler = StandardScaler()\nscaled_stats = scaler.fit_transform(asset_stats)\n\n# Perform K-Means clustering with 3 clusters\nkmeans = KMeans(n_clusters=3, random_state=42)\nasset_stats['Cluster'] = kmeans.fit_predict(scaled_stats)\n\n# Visualize clustering results with asset labels\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(asset_stats['Volatility'], \n                    asset_stats['Mean Return'], \n                    c=asset_stats['Cluster'], \n                    cmap='viridis', \n                    s=50)\n\n# Add asset labels to the plot\nfor idx, row in asset_stats.iterrows():\n   plt.annotate(idx, \n               (row['Volatility'], row['Mean Return']),\n               xytext=(5, 5), \n               textcoords='offset points')\n\nplt.title(\"Asset Clustering Based on Return and Volatility\")\nplt.xlabel(\"Volatility\")\nplt.ylabel(\"Mean Return\")\nplt.colorbar(label='Cluster')\nplt.savefig(os.path.join(save_path, 'asset_clustering.png'))\nplt.show()\nplt.close()\n\n# ----------------------------------------------\n# Step 4: Clip Extreme Values for Stability\n# ----------------------------------------------\n# Limit extreme values for more stable analysis\nreturns = returns.clip(lower=-0.5, upper=0.5)\nasset_stats['Volatility'] = asset_stats['Volatility'].clip(upper=50)\n\n# Recalculate statistics after clipping\nasset_stats = pd.DataFrame({\n   'Mean Return': returns.mean(),\n   'Volatility': returns.std()\n})\n\n# ----------------------------------------------\n# Step 5: Adjust Target Return\n# ----------------------------------------------\n# Calculate target return as mean across all assets\ntarget_return = asset_stats['Mean Return'].mean()\nprint(\"Target Return:\", target_return)\n\n# ----------------------------------------------\n# Step 6: Portfolio Optimization\n# ----------------------------------------------\ndef optimize_portfolio(returns, target_return):\n   \"\"\"\n   Optimize portfolio weights to minimize risk while meeting target return\n   \n   Parameters:\n   returns: DataFrame of asset returns\n   target_return: float, minimum required portfolio return\n   \n   Returns:\n   array of optimal weights\n   \"\"\"\n   n_assets = returns.shape[1]\n   mu = returns.mean().values  # Expected returns\n   cov_matrix = returns.cov().values  # Covariance matrix\n\n   # Define optimization variables\n   weights = cp.Variable(n_assets)\n   portfolio_return = mu.T @ weights\n   portfolio_risk = cp.quad_form(weights, cov_matrix)\n\n   # Set optimization constraints\n   constraints = [cp.sum(weights) == 1, weights &gt;= 0, portfolio_return &gt;= target_return]\n\n   # Minimize portfolio variance subject to constraints\n   problem = cp.Problem(cp.Minimize(portfolio_risk), constraints)\n   problem.solve()\n\n   return weights.value\n\n# Re-optimize portfolio with target return\noptimal_weights = optimize_portfolio(returns, target_return)\n\n# Display optimization results\nportfolio = pd.DataFrame({\n   'Asset': returns.columns,\n   'Optimal Weight': optimal_weights\n})\nprint(portfolio)\n\n# ----------------------------------------------\n# Step 7: Visualize Portfolio Weights\n# ----------------------------------------------\n# Create bar plot of optimal weights\nplt.figure(figsize=(8, 6))\nplt.bar(portfolio['Asset'], portfolio['Optimal Weight'], color='skyblue')\nplt.title(\"Improved Optimal Portfolio Weights\")\nplt.xlabel(\"Assets\")\nplt.ylabel(\"Weight\")\nplt.xticks(rotation=45)\nplt.savefig(os.path.join(save_path, 'portfolio_weights.png'))\nplt.show()\nplt.close()\n\nChecking for NaN or Inf values in data...\nNaN values: 0\nInf values: 0\n\n\n\n\n\n\n\n\n\nTarget Return: 0.00040986463585241246\n       Asset  Optimal Weight\n0      SP500    3.892382e-03\n1        Oil    5.545658e-01\n2       Gold    1.053118e-01\n3     Nasdaq    3.362301e-01\n4  Dow_Jones    6.304507e-19"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#introduction-and-motivation",
    "href": "technical-details/supervised-learning/instructions.html#introduction-and-motivation",
    "title": "Instructions",
    "section": "",
    "text": "This section is to predict financial outcomes and market behavior. The primary focus is on predicting: 1. The future direction of the Dow Jones Index (binary classification). 2. The 5-day future return of the Dow Jones Index (regression)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#overview-of-methods",
    "href": "technical-details/supervised-learning/instructions.html#overview-of-methods",
    "title": "Instructions",
    "section": "",
    "text": "Normalization: Features were standardized using StandardScaler to ensure consistent scaling.\nFeature Selection: Selected key financial indicators (e.g., Daily_Return_sp500, Volatility_sp500, MA_50_sp500) and lagged features (e.g., Lag1_Return, Lag2_Return).\nHandling Missing Data: Missing values were imputed using column means or removed when necessary.\nTrain-Test Split: Data was split into 80% training and 20% testing subsets.\n\n\n\n\n\n\n\n\nAlgorithm: Logistic Regression\nObjective: Predict the direction of the Dow Jones Index (Target_dow_jones).\nEvaluation Metrics: Accuracy, Precision, Recall, F1 Score, and Confusion Matrix.\n\n\n\n\n\nAlgorithms: Linear Regression, Random Forest, XGBoost, and LightGBM.\nObjective: Predict the 5-day future return (Future_Return_5D_dow_jones).\nEvaluation Metrics: Mean Squared Error (MSE) and R² Score.\n\n\n\n\n\n\n\nCross-Validation: 5-fold cross-validation to ensure robust model evaluation.\nParameter Tuning: Hyperparameters for Random Forest, XGBoost, and LightGBM were optimized to improve performance."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#significance",
    "href": "technical-details/data-collection/overview.html#significance",
    "title": "Overview",
    "section": "",
    "text": "Bridge the gap between diverse sources and meaningful economic analysis by transforming raw economic data into valuable research resources. We lay the foundation for subsequent analysis by collecting data, managing data quality, research potential, and monitoring economic dynamics."
  },
  {
    "objectID": "technical-details/data-collection/overview.html#key-deliverables",
    "href": "technical-details/data-collection/overview.html#key-deliverables",
    "title": "Overview",
    "section": "",
    "text": "Comprehensive CSV datasets covering multiple economic indicators\nWell-documented data collection methodology\nClean, standardized economic and market performance data"
  },
  {
    "objectID": "index.html#about-us",
    "href": "index.html#about-us",
    "title": "Landing page",
    "section": "",
    "text": "Hello! My name is Shengmian Wang. I am a first-year student in the Data Science and Analytics(DSAN) masters program at Georgetown University. I completed my undergraduate studies in Taiwan with a major of Information Management.\n\n\n\nWith two years of full-time experience as a data analyst and school learning, I have developed a robust skill set in transforming complex data into actionable intelligence. My professional background has equipped me with:\nData Analysis & Visualization\n\nLeveraged Python (Pandas, NumPy) and R to analyze large-scale datasets, including geospatial data and user reviews, ensuring high data quality and accuracy\nCreated interactive dashboards using Power BI and SPSS that significantly improved decision-making efficiency and delivered actionable insights to diverse clients\nConducted comprehensive exploratory data analysis (EDA) using SQL and BigQuery, driving substantial increases in user engagement and identifying key business patterns\nGenerated extensive analytical reports using SPSS and Excel, translating complex data into clear insights for stakeholders\n\nMachine Learning & Statistical Analysis\n\nImplemented various machine learning models (Decision Tree, K-means, KNN) achieving high accuracy in predicting consumer behavior patterns\nApplied advanced statistical methods including Chi-Square tests, Logistic Regression, and Bayes’ Theorem to analyze adoption patterns and consumer behavior\nDeveloped NLP pipelines utilizing BioBERT embeddings and cosine similarity for sentiment analysis and classification\nOptimized model performance through feature engineering and parameter tuning, significantly improving data accuracy\n\nData Collection & Processing\n\nAutomated data collection processes using Selenium, successfully gathering extensive data entries and user reviews from multiple sources\nDeveloped a VBA-based format check tool that substantially increased data validation efficiency\nCreated efficient data processing pipelines to handle various data formats (CSV, JSON, API responses)\nOptimized SQL queries for Google Analytics transitions, enhancing implementation efficiency\n\n\n\n\n\nLinkedIn - Professional Profile\nGitHub - Project Repository\nPortfolio - Project Showcase\nEmail: wsm19990820@gmail.com\nPhone: (571) 276-6276\n\n\n\n\n\n\n\nHello! My name is Hailing Liao. I am also a first-year student in the Data Science and Analytics(DSAN) masters program at Georgetown University. I completed my undergraduate studies in Zhongnan University of Economics and Law, majored in Information management and Information system.\n\n\n\nDuring my undergraduate study, I found myself a strong interest in data analytics through relevant coursework called “Commercial Data Analysis”. I developed a strong interest in business analytics and data visualization, which led me to work on several related projects. This experience deepened my interest in data science. So I chose my graduate study at Georgetown University. In the DSAN program, I hope to build solid programming skills and apply them to my future work. Although I feel like my current programming abilities need a lot of work, I’m eager to learn and make the most of this program. After graduation, my goal is to join a leading firm as a data analyst.\n\n\n\n\nI haven’t worked full time, and after my undergrad graduation gap for a year to prepare for various exams and materials for grad school. I previously had a two-month stint as a business analytics intern at Johnson & Johnson. While there, I - using data analysis tools, including Tableau and Python - analyzed sales data from over 10 retail stores. I then conducted detailed interviews and analyzed customer traffic to develop tailored promotional strategies for contact lens brands, contributing to increased sales conversions.\n\n\n\nLinkedIn - Professional Profile\nGitHub - Project Repository\nPortfolio - Project Showcase\nEmail: haileyyy.liao@gmail.com\nPhone: (202) 975-9385"
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Landing page",
    "section": "Research Questions",
    "text": "Research Questions\n\nCan the short-term returns of the stock market be predicted?\n\nImplementing machine learning models (XGBoost, LightGBM)\nAnalyzing technical indicators’ predictive power\nEvaluating prediction accuracy across different market conditions\n\nHow do GDP, CPI, unemployment rate and federal funds rate affect the performance of different markets (Dow Jones, S&P 500, Nasdaq, gold, crude oil)?\n\nStudying impacts of GDP, CPI, unemployment rate, and federal funds rate\nAnalyzing cross-market effects\nIdentifying key economic drivers for different assets\n\nHow to identify abnormal market behavior through technical indicators and volatility analysis, and is there a correlation between these abnormalities and macroeconomic indicators?\n\nDeveloping volatility analysis frameworks\nCorrelating market anomalies with technical indicators\nExamining relationships with macroeconomic factors\n\nWhat characteristic patterns do different market assets (stocks, gold, crude oil) show in different clusters? What implications do these patterns have for portfolio allocation?\n\nImplementing cluster analysis for market segmentation\nAnalyzing asset behavior patterns\nDeveloping portfolio allocation strategies\n\nHow do each market perform in terms of future return (Future_Return)? What is the correlation between this performance and its technical indicator characteristics?\n\nExamining future return patterns\nAnalyzing technical indicator relationships\nDeveloping predictive frameworks"
  },
  {
    "objectID": "index.html#professional-networks",
    "href": "index.html#professional-networks",
    "title": "Landing page",
    "section": "Professional Networks",
    "text": "Professional Networks"
  },
  {
    "objectID": "index.html#contact-information",
    "href": "index.html#contact-information",
    "title": "Landing page",
    "section": "Contact Information",
    "text": "Contact Information\n📧 wsm19990820@gmail.com"
  },
  {
    "objectID": "technical-details/data-collection/main.html#goals",
    "href": "technical-details/data-collection/main.html#goals",
    "title": "Data Collection",
    "section": "Goals",
    "text": "Goals\nThe primary goal of this project is to develop a framework for collecting, integrating, and preprocessing economic and financial data from multiple authoritative sources. By creating a systematic approach to data aggregation, we aim to enable advanced economic research, predictive analysis, and data-driven decision-making."
  },
  {
    "objectID": "technical-details/data-collection/main.html#motivation",
    "href": "technical-details/data-collection/main.html#motivation",
    "title": "Data Collection",
    "section": "Motivation",
    "text": "Motivation\nThe complexity of economic analysis is fundamentally constrained by consistency, and integration. Existing approaches to economic data collection often suffer from:\n\nFragmented data sources\nInconsistent reporting frequencies\nLimited cross-source compatibility"
  },
  {
    "objectID": "technical-details/data-collection/main.html#objectives",
    "href": "technical-details/data-collection/main.html#objectives",
    "title": "Data Collection",
    "section": "Objectives",
    "text": "Objectives\nOur specific objectives are focused on creating a framework for economic data collection, emphasizing comprehensive aggregation, data quality, research potential, and methodological transparency:\n\n1. Comprehensive Data Aggregation\n\nCollect diverse economic indicators from multiple authoritative sources\nIntegrate data from key economic and financial platforms:\n\nFederal Reserve Economic Data (FRED)\nBureau of Economic Analysis (BEA)\nYahoo Finance\n\nGather a wide range of economic and market performance metrics, including:\n\nMacroeconomic indicators (GDP, CPI, Unemployment)\nMarket indices (S&P 500, Nasdaq, Dow Jones)\nCommodity prices (Gold, Crude Oil)\nTreasury yield data\n\n\n\n\n2. Data Quality and Consistency\n\nImplement data processing techniques to ensure data reliability\nAddress challenges of data integration across different sources\nDevelop methods to:\n\nHandle missing or invalid data points\nConvert and standardize data types\nMerge datasets from multiple sources\n\nEnsure clean, consistent data preparation for further analysis\n\n\n\n3. Research Enablement\n\nPrepare a structured, accessible dataset for economic research\nSave raw collected data in easily manageable CSV format\nCreate a foundation for potential future analyses, including:\n\nEconomic trend identification\nMarket performance studies\nMacroeconomic indicator exploration\n\nProvide a flexible dataset that supports various research approaches\n\n\n\n4. Methodological Transparency\n\nDocument the entire data collection process comprehensively\nProvide clear, reproducible code for data retrieval\nExplain the rationale behind data source selection\nEnsure other researchers can understand and potentially replicate the data collection methodology"
  },
  {
    "objectID": "technical-details/data-collection/main.html#significance",
    "href": "technical-details/data-collection/main.html#significance",
    "title": "Data Collection",
    "section": "Significance",
    "text": "Significance\nBridge the gap between diverse sources and meaningful economic analysis by transforming raw economic data into valuable research resources. We lay the foundation for subsequent analysis by collecting data, managing data quality, research potential, and monitoring economic dynamics."
  },
  {
    "objectID": "technical-details/data-collection/main.html#key-deliverables",
    "href": "technical-details/data-collection/main.html#key-deliverables",
    "title": "Data Collection",
    "section": "Key Deliverables",
    "text": "Key Deliverables\n\nComprehensive CSV datasets covering multiple economic indicators\nWell-documented data collection methodology\nClean, standardized economic and market performance data"
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-collection-strategies",
    "href": "technical-details/data-collection/main.html#data-collection-strategies",
    "title": "Data Collection",
    "section": "1. Data Collection Strategies",
    "text": "1. Data Collection Strategies\n\na) FRED (Federal Reserve Economic Data) API\n\nPurpose: Retrieve comprehensive macroeconomic indicators\nSeries Collected:\n\nGross Domestic Product (GDP)\nConsumer Price Index (CPI)\nUnemployment Rate\nFederal Funds Rate\nMoney Supply (M2)\nConsumer Sentiment Index\nReal Estate Price Index\nExports and Imports Data\n\nTechnique:\n\nUtilized requests library for API communication\nConverted raw JSON responses into pandas DataFrames\nStandardized date formats and numeric conversions\nMerged multiple economic series into a single comprehensive dataset\n\n\n\n\nb) BEA (Bureau of Economic Analysis) API\n\nPurpose: Fetch National Income and Product Accounts (NIPA) data\nTables Retrieved:\n\nT20100: Gross Domestic Product and Personal Consumption\nT10105: National Account Savings and Investment Data\n\nApproach:\n\nCreated a generic fetch_bea_data() function\nConfigured API parameters for annual frequency data\nHandled potential API response variations\nConverted successful responses to CSV for further analysis\n\n\n\n\nc) Yahoo Finance API (yfinance)\n\nPurpose: Download market performance data for major indices and commodities\nAssets Retrieved:\n\nS&P 500 Index\nNasdaq Composite\nDow Jones Industrial Average\nGold Futures\nCrude Oil Futures\n\nMethod:\n\nImplemented a flexible fetch_data() function\nSet consistent date ranges for historical data\nDownloaded daily price and volume information\nStandardized data columns and saved as CSV files"
  },
  {
    "objectID": "technical-details/data-collection/main.html#data-processing-techniques",
    "href": "technical-details/data-collection/main.html#data-processing-techniques",
    "title": "Data Collection",
    "section": "2. Data Processing Techniques",
    "text": "2. Data Processing Techniques\n\nFile and Directory Management\n\nUsed os module for robust file path handling\nDynamically created output directories\nImplemented consistent file naming conventions\nEnsured data is saved in a structured, accessible format\n\n\n\nAPI Request Handling\n\nImplemented error handling and logging\nConverted API responses to consistent pandas DataFrame structures\nPerformed data type conversions (datetime, numeric)\nManaged potential missing or invalid data points\n\n\n\nData Transformation\n\nReset DataFrame indexes\nRenamed columns for clarity\nPerformed basic data cleaning\nPrepared data for subsequent analysis stages"
  },
  {
    "objectID": "technical-details/data-collection/main.html#modular-design-principles",
    "href": "technical-details/data-collection/main.html#modular-design-principles",
    "title": "Data Collection",
    "section": "3. Modular Design Principles",
    "text": "3. Modular Design Principles\n\nCreated reusable functions for data fetching\nSeparated concerns between data collection, processing, and storage\nUsed configuration dictionaries for flexible asset and series selection\nImplemented error-tolerant code with try-except blocks"
  },
  {
    "objectID": "technical-details/data-collection/main.html#security-and-configuration",
    "href": "technical-details/data-collection/main.html#security-and-configuration",
    "title": "Data Collection",
    "section": "4. Security and Configuration",
    "text": "4. Security and Configuration\n\nUtilized environment-specific API keys\nConfigured date ranges and data retrieval parameters\nImplemented interval and frequency specifications for precise data collection"
  },
  {
    "objectID": "technical-details/data-collection/main.html#output-and-persistence",
    "href": "technical-details/data-collection/main.html#output-and-persistence",
    "title": "Data Collection",
    "section": "5. Output and Persistence",
    "text": "5. Output and Persistence\n\nSaved raw collected data as CSV files\nMaintained a consistent output folder structure\nEnabled easy data inspection and further processing"
  },
  {
    "objectID": "technical-details/data-collection/main.html#technological-stack",
    "href": "technical-details/data-collection/main.html#technological-stack",
    "title": "Data Collection",
    "section": "Technological Stack",
    "text": "Technological Stack\n\nProgramming Language: Python 3.12\nKey Libraries:\n\nrequests for API communication\npandas for data manipulation\nyfinance for financial data retrieval\nos for file system operations\n\nData Sources:\n\nFRED API\nBEA API\nYahoo Finance"
  },
  {
    "objectID": "technical-details/data-collection/main.html#technical-challenges-and-limitations",
    "href": "technical-details/data-collection/main.html#technical-challenges-and-limitations",
    "title": "Data Collection",
    "section": "Technical Challenges and Limitations",
    "text": "Technical Challenges and Limitations\nNavigating the complex world of economic data APIs comes with many challenges. The financial data landscape is characterized by fragmentation, with each source (FRED, BEA, and Yahoo Finance) that offers unique data formats and reporting mechanisms. So we need a sophisticated approach to data standardization and integration.\nOne of the most important difficulties is the inherent time inconsistency of managing economic indicators. Macroeconomic data often arrive at different frequencies—some monthly, others quarterly or annually—creating data alignment challenges. Our solution is to utilize interpolation and aggregation strategies to transform these disparate data streams into coherent, consistent time series."
  },
  {
    "objectID": "technical-details/data-collection/main.html#unintended-consequences-and-technical-impact",
    "href": "technical-details/data-collection/main.html#unintended-consequences-and-technical-impact",
    "title": "Data Collection",
    "section": "Unintended Consequences and Technical Impact",
    "text": "Unintended Consequences and Technical Impact\n\nObserved Data Anomaly\n\nIrregular Yield Curve:\n\nUnexpected fluctuations in 10-year Treasury yield data detected\nPotential impact on the interpretation of economic indicators\nRecommend further investigation of anomalous data points\n\n\n\n\nComputational Insights\n\nPerformance Bottleneck:\n\nIdentify potential optimization opportunities in the data retrieval and processing pipeline\nEmphasis on the need for a more efficient API request strategy"
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-suggestions",
    "href": "technical-details/data-collection/main.html#conclusion-and-suggestions",
    "title": "Data Collection",
    "section": "Conclusion and Suggestions",
    "text": "Conclusion and Suggestions\n\nKey Technical Achievements\n\nSuccessfully developed a powerful multi-source financial data collection framework\nCreate a flexible modular data processing pipeline\nAbility to effectively process complex, multi-frequency economic data sets\n\n\n\nActionable Suggestions\n\nShort-Term Improvements:\n\nImplement more complex error handling\nEnhanced data verification mechanism\nOptimize API request strategy\n\nLong-Term Research Directions:\n\nExplore machine learning integration for predictive economic analysis\nDevelop a more comprehensive data standardization framework\nEstablish a common economic data collection and processing toolkit\n\n\n\n\nFinal Thoughts\nThis step is an important step in creating a more flexible approach to financial and economic data collection. By solving technical challenges and using solutions, we lay the foundation for subsequent economic analysis techniques."
  },
  {
    "objectID": "report/report.html#eda",
    "href": "report/report.html#eda",
    "title": "Final Report",
    "section": "EDA",
    "text": "EDA\nWe loaded several datasets related to economic indicators, including crude oil prices, gold prices, the Dow Jones, the NASDAQ, government spending, and macroeconomic variables. The data were then examined and cleaned, missing values were addressed, and outliers were removed, and the distributional properties of each variable were analyzed using histograms and density plots.\n\nFrom the distribution of oil, we find that some of the characteristics, such as Volume and Volatility, have significant long-tailed distributions that need to be further analyzed or processed. They may have important implications for market behavior.\n\nThis heat map shows the relationship between the variables in the Dow Jones data. Price-like characteristics (e.g., opening and closing prices) are highly correlated, reflecting synchronized changes in market prices, while volume and volatility are weakly correlated with price. In addition, the target variable exhibits some positive correlation with future returns but a weaker relationship with other variables, suggesting that market performance is influenced by a combination of factors.\n\nThe plot of correlation heatmap for Macro_Series shows the relationship between macroeconomic indicators. There is a highly positive correlation between GDP, CPI, import and export data, and money supply (M2), suggesting that they usually move in tandem; while the unemployment rate shows a negative correlation with these economic indicators, reflecting an inverse relationship between economic growth and employment levels.\nFor better in-depth analysis, we merge macro data and asset data. We made a multi-market linkage analysis.   \nFrom the market correlation matrix, we know that there is a high degree of synchronization between the NASDAQ, Dow Jones and S&P 500, while gold and crude oil prices are weakly linked to the other markets. After performing the PCA analysis, the market return data was successfully downscaled to capture the key market volatility characteristics. K-Means clustering identified three distinct groups in the market data.\n\nMacroeconomic indicators have a different impact on market assets. Gold is more affected by macro turbulence (e.g. changes in unemployment and interest rates). Stock markets have a positive relationship with economic growth and inflation, but the impact is more complex. Gold may offer protection in times of economic instability or rising interest rates. Unemployment and interest rates are important indicators of market volatility to focus on.\n\nWe analyzed yield volatility across five market asset classes, focusing on the magnitude of their daily volatility as well as periods of extreme volatility. S&P 500 Index (SP500) yield volatility remained within ±5% most of the time. During periods of market turbulence, there were significant extremes of volatility. The volatility pattern of Dow Jones is almost identical to that of S&P 500. The crude oil market has been the most volatile, especially in April 2020 when there was a historic extreme decline (close to -300%). The crude oil market shows a high degree of risk and volatility and requires special attention. The overall performance of the gold market has been relatively stable, with low volatility in yields. There are fewer extreme volatility points, reflecting gold’s characteristics as a safe-haven asset and relative safety during market crises. Nasdaq’s volatility pattern is similar to that of the S&P 500, but with slightly higher volatility. The characteristics of technology stocks make them more sensitive to market fluctuations and prone to dramatic changes in times of crisis."
  },
  {
    "objectID": "report/report.html#supervised-learning",
    "href": "report/report.html#supervised-learning",
    "title": "Final Report",
    "section": "Supervised-learning",
    "text": "Supervised-learning\nThe purpose of this section is to forecast financial results and market behavior. The main focus is on predicting the future direction of the Dow Jones and the return of the Dow Jones over the next 5 days, which are classification and regression problems, respectively.\n \nIn order to determine the direction of Dow Jones rise or fall, the logistic regression model performs well with an accuracy of 95% and an area under the ROC curve of 0.96, which indicates the model’s strong differentiation ability.\n \nIn predicting future Dow Jones returns for the next 5 days, I used multiple models for comparison. Linear regression and random forest performed poorly, so LightGBM and XGboost were used. After adding lagged features, rolling statistical features and parameter tuning, the model performed well. Among them, the XGboost model performed the best. After that we used the XGboost model to predict future yields with a downward trend.\nIn terms of determining the direction of market rise or fall, the logistic regression model performs well with an accuracy of 95% and an area under the ROC curve of 0.96, which indicates the model’s strong differentiation ability.\nFrom the results, indicators such as historical returns and market volatility play an important role in predicting short-term market trends. Linear models are effective in dealing with simple trends, but nonlinear models such as XGBoost and LightGBM are able to perform better when facing complex market dynamics. This also reflects the fact that market performance is affected by a combination of factors and it is difficult to capture all the patterns in simple methods."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#evaluation-metrics-guidance",
    "href": "technical-details/llm-usage-log.html#evaluation-metrics-guidance",
    "title": "LLM usage log",
    "section": "** Evaluation Metrics Guidance**",
    "text": "** Evaluation Metrics Guidance**\n\nModel Performance Interpretation:\nProvided intuitive explanations of metrics such as Precision, Recall for classification tasks and R², MSE for regression tasks, ensuring results were understandable.\n\nOptimization Methods:\nSuggested improvements to enhance model performance, such as hyperparameter tuning in XGboost and LightGBM."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#results-interpretation",
    "href": "technical-details/llm-usage-log.html#results-interpretation",
    "title": "LLM usage log",
    "section": "Results Interpretation",
    "text": "Results Interpretation\n\nVisual Analysis Summaries:\nProvided initial observations on visual outputs, such as correlation heatmaps, PCA scatter plots, and K-means clustering results."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Landing page",
    "section": "Literature Review",
    "text": "Literature Review\n\n1. Global Stock Market Prediction with XGBoost and LightGBM Machine Learning Models\nGuennioui, Omaima, Dalila Chiadmi, and Mustapha Amghar explore the application of machine learning techniques LightGBM and XGBoost in stock market trend prediction and propose a hybrid model Global XGBoost-LightGBM that is tested by harmonizing parameter settings over tested on multiple stock series. The experiments are conducted on two different stock markets, the United States and Morocco, and the results show that the hybrid model combining LightGBM and XGBoost is able to effectively capture stock price fluctuations, significantly improve prediction accuracy, and address the challenges of stock market forecasting. Guennioui, Omaima, Dalila Chiadmi, and Mustapha Amghar. “Improving Global Stock Market Prediction with XGBoost and LightGBM Machine Learning Models.”\n\n\n2. Macroeconomic Factors and Stock\nA variety of complex economic factors and scenarios can affect stock returns, including interest rates, exchange rates, company performance, political situations, current events, economic conditions, market psychology, natural disasters and government policies. Theoretically, macroeconomic variables are considered to be the main source of stock volatility. Therefore, these factors are the main indicators of stock returns. Jabeen, Ayesha, et al. “An Empirical Study of Macroeconomic Factors and Stock Returns in the Context of Economic Uncertainty News Sentiment Using Machine Learning.” Complexity 2022.1 (2022): 4646733.\n\n\n3. Benefits of Machine Learning in Financial Forecasting\nMachine learning methods are well suited to the core tasks of financial forecasting, planning, and analysis, primarily because of their focus on predictive performance, their ability to identify generalizable patterns, and their applicability to new data. In addition, many off-the-shelf machine learning algorithms perform well across a wide range of tasks and are technically easy to use, making them highly attractive to real-world applicators.Wasserbacher, Helmut, and Martin Spindler. “Machine learning for financial forecasting, planning and analysis: recent developments and pitfalls.” Digital Finance 4.1 (2022): 63-88.\n\n\n4. Macroeconomic Factors and Stock Returns\nFlannery and Protopapadakis (2002) proposed a comprehensive GARCH model to analyze the relationship between macroeconomic factors and stock returns. Their study identified six major macroeconomic factors that influence total stock returns—three nominal factors (Consumer Price Index, Producer Price Index, and Monetary Aggregates) and three real factors (trade balance, employment, and housing construction ). However, traditional indicators of economic activity have limited impact, showing that market dynamics are non-linear and complex. Andersen, T. (1996). Return Volatility and Trading Volume: An Information Flow Interpretation of Stochastic Volatility. Journal of Finance, 51(1), 169-204.\n\n\n5. Analyze Return Volatility and Volume\nAndersen (1996) developed an empirical model based on a microstructural framework to examine the relationship between return volatility and trading volume. This study uses stochastic volatility procedures to improve forecast accuracy by focusing on information asymmetry and liquidity needs in trading patterns. The results demonstrate the superiority of the modified mixture distribution hypothesis (MDH) in dynamic modeling of information flow. Boyd, J. H., Hu, J., & Jagannathan, R. (2005). The Stock Market’s Reaction to Unemployment News: Why Bad News Is Usually Good for Stocks. Journal of Finance, 60(2), 649-672.\n\n\n6. The Impact of Economic News on the Stock Market\nBoyd, Jagannathan, and Hu (NBER working paper) provide an original study that examines how unemployment news affects the stock market differently during different business cycles. Their research shows that news of rising unemployment is generally good for stocks during periods of economic expansion but has a negative impact on stocks during periods of economic contraction, and highlights how economic news brings together multiple types of information (interest rate expectations and future corporate earnings) , thus having different impacts on stock values ​​depending on economic conditions.Flannery, M., & Protopapadakis, A. (2002). Macroeconomic Factors Do Influence Aggregate Stock Returns. The Review of Financial Studies, 15(3), 751-782."
  }
]